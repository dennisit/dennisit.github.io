<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[乐理理论基础]]></title>
    <url>%2F2018%2F02%2F11%2F%E7%94%9F%E6%B4%BB%E5%A8%B1%E4%B9%90%2F%E4%B9%90%E7%90%86%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[五线谱大谱表: 高音谱表和低音谱表合并起来就是大谱表. 音符音符是用来记录不同长短的音的进行符号. 音符包括三个组成部分: 符头和符干和符尾. 常见的音符有: 全音符、二分音符、四分音符、八分音符、十六分音符、三十二分音符等. 自然音调一般所说的音调指自然大小调, 一共有24个; 从钢琴上小字一组的C(中央C)向上数一个八度,到小字二组的C黑白键加起来一共12个键(1、#1、 2、 #2、 3、 4、 #4、 5、 #5、 6、 #6、 7 )就是所谓12平均律; 以每个音为启始都可以有一对大小调.下图为大谱表和钢琴键盘对照表(箭头所示为中央C的位置) 调性类型大调调性 小调调性 如下图所示 五声音阶五声音阶是在自然大调的基础上,省去第4级音和第7级音. 音阶规律上面介绍了自然大调、自然小调以及衍生出的和声、旋律调, 以及五声音阶. 这里介绍音阶规律, 因为对应的调都是基于自然大调衍生出来的, 所以我们掌握了自然大调后, 其它的调根据规律变换即可. 大调规律升号调(口诀: 4-1-5-2-6-3-7) 说明: 如果有n个#号,调号则比第n个#所对应的音高一个全音。 降号调(口诀: 7-3-6-2-5-1-4) 如下图: 五度圈 流行钢琴教程 本教程适用于流行钢琴兴趣爱好者,古典钢琴是以技术练习为重点，是将谱子上定好的音符弹出来，类似于照本朗读。它有大量练习曲，目的并不是好听，而是以高强度训练来习得非常复杂的技术，因此一般都枯燥无味。作为成年人学习钢琴，不要从古典练习曲拜厄、加农等开始，要直接学习现代即兴伴奏的体系，从喜欢的歌曲入手。即兴伴奏体系非常简单灵活，强调创新灵感，类似于随性聊天。学了以后立即可以进行弹唱，自娱娱人都十分有用。 1. 优酷《数字灵感钢琴课程——精华十二课》 这个教程是我看到的最简易高效、最生动的钢琴速成课程。它不仅告诉我们如何弹奏，还教会我们如何零基础作曲。我觉得作者小帅是一个学习的大师，有非常深刻的思想，让人脑洞大开。 2.《蓝调小生司琴即兴伴奏一点通》 微信和百度一搜都有，这是最系统但不失通俗的一套教程。比起数字灵感钢琴课程要更全面和细致些。每集不到10分钟；掌握一集水平就高一截。 3. 优酷《教你快速识简谱》 共九集，共1.5小时长度。可以跳着看，大概半小时可以搞定简谱。简谱一搞定，弹任何流行歌曲都没有问题了。不一定要学五线谱。 4. 优酷《教会键盘速成教材（基础）之快速入门基本的指法（右手和左手基本指法）》 共12集，每集10到20分钟。非常通俗易懂。作者佟显生. 5. 优酷《林文信流行键盘12小时》和《宋老师教音乐》 两个都是系统而实用的教程。林文信的东西在掌握了基本乐理后再看，会发现讲得很透彻通俗。其中第3集的第39分钟讲基本指法可以先看。不过这两个视频教程一集有接近一个小时，零基础的看了会觉得太长。虽然是速成课程，但讲得还是比较复杂。可以在达到一定水平再看。 6. 优酷《主内钢琴学校的自频道》 有丰富内容，让你对其他视频看不懂的术语加深了解。 7. 优酷《钢琴弹唱的秘密》 了解了基本的乐理知识后可以看。虽然讲话很小声还带台湾腔，不过可以迅速明白弹唱的原理，。而且可以直观看到大量歌曲是如何伴奏的。作者是台湾的陈俊宇。 8. 优酷《 琴之缘电子琴和弦指法》 其中有对常用指法的通俗易懂的介绍。3分钟可学会顺指法、穿指法、跨指法、缩指法和扩指法等基本指法。 9. 优酷《大卫音乐》 他有独门记忆和弦的方法。首先看他的《大卫音乐:钢琴即兴伴奏之菜鸟6分钟学会7个大和弦》，6分钟用几个口诀记住钢琴键盘基本结构和大和弦，是超快入门的好素材。他的《大卫音乐:2分钟学会弹唱《征服》》超清晰帮助我们2分钟明白无伴奏和弦弹奏的本质。《大卫音乐:实用乐理》3分钟明白一张谱子的全部 推荐文章 https://www.jianshu.com/p/1bd753f06587]]></content>
      <categories>
        <category>生活娱乐</category>
      </categories>
      <tags>
        <tag>Music</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic实战自动补全]]></title>
    <url>%2F2017%2F11%2F01%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FElastic%2FElastic%E5%AE%9E%E6%88%98%E8%87%AA%E5%8A%A8%E8%A1%A5%E5%85%A8%2F</url>
    <content type="text"><![CDATA[自动补全(中文)定义索引1234567[root@icloud-store ~]# curl -XPUT "http://192.168.0.103:9200/index_test?pretty=true" -H 'Content-Type: application/json' -d'&#123; "index": &#123; "number_of_shards": "1", "number_of_replicas": "1" &#125;&#125;' 定义类型1234567891011121314151617181920[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/product/_mapping?pretty=true" -H 'Content-Type: application/json' -d'&#123; "properties": &#123; "name": &#123; "type": "text", "analyzer": "ik_smart", "search_analyzer": "ik_smart" &#125;, "tag": &#123; "type": "text", "analyzer": "ik_smart", "search_analyzer": "ik_smart" &#125;, "tag_suggest": &#123; "type": "completion", "analyzer": "ik_smart", "search_analyzer": "ik_smart" &#125; &#125;&#125;' 查看索引1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768[root@icloud-store ~]# curl -XGET "http://192.168.0.103:9200/index_test?pretty=true"&#123; "index_test" : &#123; "aliases" : &#123; &#125;, "mappings" : &#123; "product" : &#123; "properties" : &#123; "name" : &#123; "type" : "text", "analyzer" : "ik_smart" &#125;, "tag" : &#123; "type" : "text", "analyzer" : "ik_smart" &#125;, "tag_suggest" : &#123; "type" : "completion", "analyzer" : "ik_smart", "preserve_separators" : true, "preserve_position_increments" : true, "max_input_length" : 50 &#125;, "tagsuggest" : &#123; "properties" : &#123; "completion" : &#123; "properties" : &#123; "field" : &#123; "type" : "text", "fields" : &#123; "keyword" : &#123; "type" : "keyword", "ignore_above" : 256 &#125; &#125; &#125;, "size" : &#123; "type" : "long" &#125; &#125; &#125;, "text" : &#123; "type" : "text", "fields" : &#123; "keyword" : &#123; "type" : "keyword", "ignore_above" : 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125; &#125;, "settings" : &#123; "index" : &#123; "creation_date" : "1513327157579", "number_of_shards" : "1", "number_of_replicas" : "1", "uuid" : "QRHKXGpRSPen2l9u1zWJyw", "version" : &#123; "created" : "6000099" &#125;, "provided_name" : "index_test" &#125; &#125; &#125;&#125; 索引数据1234567891011121314151617[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/product/1?pretty=true" -H 'Content-Type: application/json' -d'&#123; "name" : "花花公子男士修身冬季青年潮流羽绒服", "tag": "羽绒服, 青年, 潮流", "tag_suggest": "羽绒服, 青年, 潮流"&#125;'[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/product/2?pretty=true" -H 'Content-Type: application/json' -d'&#123; "name" : "波司登中老年爸爸加厚保暖男士羽绒服", "tag": "羽绒服, 保暖, 加厚, 中老年", "tag_suggest": "羽绒服, 保暖, 加厚, 中老年"&#125;'[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/product/3?pretty=true" -H 'Content-Type: application/json' -d'&#123; "name" : "真皮进口头层牛皮立领修身中年皮夹克", "tag": "羽毛, 中年, 绒, 修身, 羽绒", "tag_suggest": "羽毛, 中年, 绒, 修身, 羽绒"&#125;' 搜索示例简单自动补全示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/_search?pretty" -H 'Content-Type: application/json' -d'&#123; "suggest": &#123; "product-suggest" : &#123; "prefix" : "羽绒", "completion" : &#123; "field" : "tag_suggest" &#125; &#125; &#125;&#125;'&#123; "took" : 6, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 0, "max_score" : 0.0, "hits" : [ ] &#125;, "suggest" : &#123; "product-suggest" : [ &#123; "text" : "羽绒", "offset" : 0, "length" : 2, "options" : [ &#123; "text" : "羽绒服, 保暖, 加厚, 中老年", "_index" : "index_test", "_type" : "product", "_id" : "2", "_score" : 1.0, "_source" : &#123; "name" : "波司登中老年爸爸加厚保暖男士羽绒服", "tag" : "羽绒服, 保暖, 加厚, 中老年", "tag_suggest" : "羽绒服, 保暖, 加厚, 中老年" &#125; &#125;, &#123; "text" : "羽绒服, 青年, 潮流", "_index" : "index_test", "_type" : "product", "_id" : "1", "_score" : 1.0, "_source" : &#123; "name" : "花花公子男士修身冬季青年潮流羽绒服", "tag" : "羽绒服, 青年, 潮流", "tag_suggest" : "羽绒服, 青年, 潮流" &#125; &#125; ] &#125; ] &#125;&#125; 指定返回特定列12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/_search?pretty" -H 'Content-Type: application/json' -d'&#123; "_source": "tag_suggest", "suggest": &#123; "product-suggest" : &#123; "prefix" : "羽绒", "completion" : &#123; "field" : "tag_suggest" &#125; &#125; &#125;&#125;'&#123; "took" : 6, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 0, "max_score" : 0.0, "hits" : [ ] &#125;, "suggest" : &#123; "product-suggest" : [ &#123; "text" : "羽绒", "offset" : 0, "length" : 2, "options" : [ &#123; "text" : "羽绒服, 保暖, 加厚, 中老年", "_index" : "index_test", "_type" : "product", "_id" : "2", "_score" : 1.0, "_source" : &#123; "tag_suggest" : "羽绒服, 保暖, 加厚, 中老年" &#125; &#125;, &#123; "text" : "羽绒服, 青年, 潮流", "_index" : "index_test", "_type" : "product", "_id" : "1", "_score" : 1.0, "_source" : &#123; "tag_suggest" : "羽绒服, 青年, 潮流" &#125; &#125; ] &#125; ] &#125;&#125; 模糊搜索12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/_search?pretty" -H 'Content-Type: application/json' -d'&#123; "suggest": &#123; "product-suggest" : &#123; "text" : "羽类", "completion" : &#123; "field" : "tag_suggest", "fuzzy" : &#123; "fuzziness" : 2 &#125; &#125; &#125; &#125;&#125;'&#123; "took" : 9, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 0, "max_score" : 0.0, "hits" : [ ] &#125;, "suggest" : &#123; "product-suggest" : [ &#123; "text" : "羽类", "offset" : 0, "length" : 2, "options" : [ &#123; "text" : "羽绒服, 保暖, 加厚, 中老年", "_index" : "index_test", "_type" : "product", "_id" : "2", "_score" : 3.0, "_source" : &#123; "name" : "波司登中老年爸爸加厚保暖男士羽绒服", "tag" : "羽绒服, 保暖, 加厚, 中老年", "tag_suggest" : "羽绒服, 保暖, 加厚, 中老年" &#125; &#125;, &#123; "text" : "羽绒服, 青年, 潮流", "_index" : "index_test", "_type" : "product", "_id" : "1", "_score" : 3.0, "_source" : &#123; "name" : "花花公子男士修身冬季青年潮流羽绒服", "tag" : "羽绒服, 青年, 潮流", "tag_suggest" : "羽绒服, 青年, 潮流" &#125; &#125; ] &#125; ] &#125;&#125; 指定推荐数量123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/_search?pretty" -H 'Content-Type: application/json' -d'&#123; "_source": "tag_suggest", "suggest": &#123; "product-suggest" : &#123; "text" : "羽", "completion" : &#123; "field" : "tag_suggest" &#125; &#125; &#125;&#125;'&#123; "took" : 5, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 0, "max_score" : 0.0, "hits" : [ ] &#125;, "suggest" : &#123; "product-suggest" : [ &#123; "text" : "羽", "offset" : 0, "length" : 1, "options" : [ &#123; "text" : "羽毛, 中年, 绒, 修身, 羽绒", "_index" : "index_test", "_type" : "product", "_id" : "3", "_score" : 1.0, "_source" : &#123; "tag_suggest" : "羽毛, 中年, 绒, 修身, 羽绒" &#125; &#125;, &#123; "text" : "羽绒服, 保暖, 加厚, 中老年", "_index" : "index_test", "_type" : "product", "_id" : "2", "_score" : 1.0, "_source" : &#123; "tag_suggest" : "羽绒服, 保暖, 加厚, 中老年" &#125; &#125;, &#123; "text" : "羽绒服, 青年, 潮流", "_index" : "index_test", "_type" : "product", "_id" : "1", "_score" : 1.0, "_source" : &#123; "tag_suggest" : "羽绒服, 青年, 潮流" &#125; &#125; ] &#125; ] &#125;&#125;[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/_search?pretty" -H 'Content-Type: application/json' -d'&#123; "_source": "tag_suggest", "suggest": &#123; "product-suggest" : &#123; "text" : "羽", "completion" : &#123; "field" : "tag_suggest", "size": 2 &#125; &#125; &#125;&#125;'&#123; "took" : 3, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 0, "max_score" : 0.0, "hits" : [ ] &#125;, "suggest" : &#123; "product-suggest" : [ &#123; "text" : "羽", "offset" : 0, "length" : 1, "options" : [ &#123; "text" : "羽毛, 中年, 绒, 修身, 羽绒", "_index" : "index_test", "_type" : "product", "_id" : "3", "_score" : 1.0, "_source" : &#123; "tag_suggest" : "羽毛, 中年, 绒, 修身, 羽绒" &#125; &#125;, &#123; "text" : "羽绒服, 保暖, 加厚, 中老年", "_index" : "index_test", "_type" : "product", "_id" : "2", "_score" : 1.0, "_source" : &#123; "tag_suggest" : "羽绒服, 保暖, 加厚, 中老年" &#125; &#125; ] &#125; ] &#125;&#125; 拼音搜索推荐1234567891011121314151617181920212223242526272829303132333435363738[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/_search?pretty" -H 'Content-Type: application/json' -d'&#123; "_source": "tag_suggest", "suggest": &#123; "product-suggest" : &#123; "text" : "yu", "completion" : &#123; "field" : "tag_suggest", "size": 2 &#125; &#125; &#125;&#125;' &#123; "took" : 2, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 0, "max_score" : 0.0, "hits" : [ ] &#125;, "suggest" : &#123; "product-suggest" : [ &#123; "text" : "yu", "offset" : 0, "length" : 2, "options" : [ ] &#125; ] &#125;&#125; 可以看到拼音搜索并不能得到友好的推荐词,所以需要结合拼音分词进行再一次优化. 自动补全(中文+拼音)删除索引1[root@icloud-store ~]# curl -XDELETE "http://192.168.0.103:9200/index_test*?pretty=true" 定义索引123456789101112131415161718192021222324252627282930[elon@icloud-store elasticsearch-6.0.0]$ curl -XPUT "http://192.168.0.103:9200/index_test?pretty=true" -H 'Content-Type: application/json' -d'&#123; "index" : &#123; "analysis": &#123; "analyzer": &#123; "default" : &#123; "tokenizer" : "ik_smart" &#125;, "ik_pinyin_analyzer": &#123; "tokenizer": "ik_smart", "filter": [ "mix_pinyin", "word_delimiter" ] &#125; &#125;, "filter": &#123; "mix_pinyin": &#123; "type": "pinyin", "keep_full_pinyin" : true, "lowercase" : true, "first_letter": "prefix", "padding_char": " " &#125; &#125; &#125; &#125;, "number_of_shards": "3", "number_of_replicas": "1"&#125;' 定义类型123456789101112131415161718192021[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/product/_mapping?pretty=true" -H 'Content-Type: application/json' -d'&#123; "properties": &#123; "name": &#123; "type": "text", "analyzer": "ik_smart", "search_analyzer": "ik_smart" &#125;, "tag": &#123; "type": "text", "analyzer": "ik_smart", "search_analyzer": "ik_smart" &#125;, "tag_suggest": &#123; "type": "completion", "analyzer": "ik_pinyin_analyzer", "search_analyzer": "ik_pinyin_analyzer", "preserve_separators": false &#125; &#125;&#125;' 创建索引1234567891011121314151617[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/product/1?pretty=true" -H 'Content-Type: application/json' -d'&#123; "name" : "花花公子男士修身冬季青年潮流羽绒服", "tag": "羽绒服, 青年, 潮流", "tag_suggest": "羽绒服, 青年, 潮流"&#125;'[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/product/2?pretty=true" -H 'Content-Type: application/json' -d'&#123; "name" : "波司登中老年爸爸加厚保暖男士羽绒服", "tag": "羽绒服, 保暖, 加厚, 中老年", "tag_suggest": "羽绒服, 保暖, 加厚, 中老年"&#125;'[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/product/3?pretty=true" -H 'Content-Type: application/json' -d'&#123; "name" : "真皮进口头层牛皮立领修身中年皮夹克", "tag": "羽毛, 中年, 绒, 修身, 羽绒", "tag_suggest": "羽毛, 中年, 绒, 修身, 羽绒"&#125;' 补全搜索(拼音)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/_search?pretty" -H 'Content-Type: application/json' -d'&#123; "_source": "tag_suggest", "suggest": &#123; "product-suggest" : &#123; "text" : "yumao", "completion" : &#123; "field" : "tag_suggest", "size": 5 &#125; &#125; &#125;&#125;' &#123; "took" : 17, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 0, "max_score" : 0.0, "hits" : [ ] &#125;, "suggest" : &#123; "product-suggest" : [ &#123; "text" : "yumao", "offset" : 0, "length" : 5, "options" : [ &#123; "text" : "羽毛, 中年, 绒, 修身, 羽绒", "_index" : "index_test", "_type" : "product", "_id" : "3", "_score" : 1.0, "_source" : &#123; "tag_suggest" : "羽毛, 中年, 绒, 修身, 羽绒" &#125; &#125; ] &#125; ] &#125;&#125; 模糊搜索(拼音)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/_search?pretty" -H 'Content-Type: application/json' -d'&#123; "_source": "tag_suggest", "suggest": &#123; "product-suggest" : &#123; "text" : "yumao", "completion" : &#123; "field" : "tag_suggest", "fuzzy" : &#123; "fuzziness" : 2 &#125;, "size": 5 &#125; &#125; &#125;&#125;' &#123; "took" : 18, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 0, "max_score" : 0.0, "hits" : [ ] &#125;, "suggest" : &#123; "product-suggest" : [ &#123; "text" : "yumao", "offset" : 0, "length" : 5, "options" : [ &#123; "text" : "羽毛, 中年, 绒, 修身, 羽绒", "_index" : "index_test", "_type" : "product", "_id" : "3", "_score" : 3.0, "_source" : &#123; "tag_suggest" : "羽毛, 中年, 绒, 修身, 羽绒" &#125; &#125;, &#123; "text" : "羽绒服, 保暖, 加厚, 中老年", "_index" : "index_test", "_type" : "product", "_id" : "2", "_score" : 2.0, "_source" : &#123; "tag_suggest" : "羽绒服, 保暖, 加厚, 中老年" &#125; &#125;, &#123; "text" : "羽绒服, 青年, 潮流", "_index" : "index_test", "_type" : "product", "_id" : "1", "_score" : 2.0, "_source" : &#123; "tag_suggest" : "羽绒服, 青年, 潮流" &#125; &#125; ] &#125; ] &#125;&#125;[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/_search?pretty" -H 'Content-Type: application/json' -d'&#123; "_source": "tag_suggest", "suggest": &#123; "product-suggest" : &#123; "text" : "yu毛", "completion" : &#123; "field" : "tag_suggest", "fuzzy" : &#123; "fuzziness" : 2 &#125;, "size": 5 &#125; &#125; &#125;&#125;' &#123; "took" : 11, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 0, "max_score" : 0.0, "hits" : [ ] &#125;, "suggest" : &#123; "product-suggest" : [ &#123; "text" : "yu毛", "offset" : 0, "length" : 3, "options" : [ &#123; "text" : "羽毛, 中年, 绒, 修身, 羽绒", "_index" : "index_test", "_type" : "product", "_id" : "3", "_score" : 1.0, "_source" : &#123; "tag_suggest" : "羽毛, 中年, 绒, 修身, 羽绒" &#125; &#125;, &#123; "text" : "羽绒服, 保暖, 加厚, 中老年", "_index" : "index_test", "_type" : "product", "_id" : "2", "_score" : 1.0, "_source" : &#123; "tag_suggest" : "羽绒服, 保暖, 加厚, 中老年" &#125; &#125;, &#123; "text" : "羽绒服, 青年, 潮流", "_index" : "index_test", "_type" : "product", "_id" : "1", "_score" : 1.0, "_source" : &#123; "tag_suggest" : "羽绒服, 青年, 潮流" &#125; &#125; ] &#125; ] &#125;&#125;[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/_search?pretty" -H 'Content-Type: application/json' -d'&#123; "_source": "tag_suggest", "suggest": &#123; "product-suggest" : &#123; "text" : "羽毛", "completion" : &#123; "field" : "tag_suggest", "fuzzy" : &#123; "fuzziness" : 2 &#125;, "size": 5 &#125; &#125; &#125;&#125;' &#123; "took" : 13, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 0, "max_score" : 0.0, "hits" : [ ] &#125;, "suggest" : &#123; "product-suggest" : [ &#123; "text" : "羽毛", "offset" : 0, "length" : 2, "options" : [ &#123; "text" : "羽毛, 中年, 绒, 修身, 羽绒", "_index" : "index_test", "_type" : "product", "_id" : "3", "_score" : 3.0, "_source" : &#123; "tag_suggest" : "羽毛, 中年, 绒, 修身, 羽绒" &#125; &#125;, &#123; "text" : "羽绒服, 保暖, 加厚, 中老年", "_index" : "index_test", "_type" : "product", "_id" : "2", "_score" : 2.0, "_source" : &#123; "tag_suggest" : "羽绒服, 保暖, 加厚, 中老年" &#125; &#125;, &#123; "text" : "羽绒服, 青年, 潮流", "_index" : "index_test", "_type" : "product", "_id" : "1", "_score" : 2.0, "_source" : &#123; "tag_suggest" : "羽绒服, 青年, 潮流" &#125; &#125; ] &#125; ] &#125;&#125;[root@icloud-store ~]# curl -XPOST "http://192.168.0.103:9200/index_test/_search?pretty" -H 'Content-Type: application/json' -d'&#123; "suggest": &#123; "product-suggest" : &#123; "text" : "yu毛", "completion": &#123; "field": "tag_suggest", "highlight": &#123; "pre_tag": "&lt;em&gt;", "post_tag": "&lt;/em&gt;" &#125;, "size": 5 &#125; &#125; &#125;&#125;']]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Elastic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic索引定义应用]]></title>
    <url>%2F2017%2F10%2F11%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FElastic%2FElastic%E7%B4%A2%E5%BC%95%E5%AE%9A%E4%B9%89%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[数据索引字段类型Elastic数据类型和数据库字段类型映射关系 数据库类型 Elastic类型 说明 String/Varchar keyword 该field不做分词 String/Varchar/Text text 该field会经过分词处理 Integer integer int类型(32bit) Long long long类型(64bit) float float 浮点类型(32bit) double double 浮点类型(64bit) boolean boolean 布尔类型: true或者false date/datetime date 日期类型: 2015-10-11, 2015-10-11T22:21:10 bytes/binary binary 二进制类型, 用于存储文件或者字节流 创建索引123456789101112131415161718192021222324252627282930313233[root@localhost ~]# curl -XPUT "http://127.0.0.1:9200/db_test?pretty=true" -H 'Content-Type: application/json' -d'&#123; "index": &#123; "analysis": &#123; "filter": &#123; "mix_pinyin": &#123; "lowercase": "true", "padding_char": " ", "first_letter": "prefix", "keep_original": "true", "remove_duplicated_term": "true", "type": "pinyin", "keep_full_pinyin": "true" &#125; &#125;, "analyzer": &#123; "ik_pinyin_analyzer": &#123; "filter": [ "mix_pinyin", "word_delimiter" ], "type": "custom", "tokenizer": "ik_max_word" &#125;, "default": &#123; "tokenizer": "ik_max_word" &#125; &#125; &#125;, "number_of_shards": "1", "number_of_replicas": "1" &#125;&#125;' 定义类型123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146[root@localhost ~]# curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_mapping?pretty=true" -H 'Content-Type: application/json' -d'&#123; "properties": &#123; "aliasName": &#123; "type": "text", "fields": &#123; "origin": &#123; "type": "keyword" &#125;, "pinyin": &#123; "type": "text", "term_vector": "with_positions_offsets", "analyzer": "ik_pinyin_analyzer" &#125;, "suggest": &#123; "type": "completion", "analyzer": "ik_max_word", "preserve_separators": true, "preserve_position_increments": true, "max_input_length": 50 &#125; &#125;, "analyzer": "ik_max_word" &#125;, "appId": &#123; "type": "long" &#125;, "author": &#123; "type": "text", "fields": &#123; "origin": &#123; "type": "keyword" &#125;, "pinyin": &#123; "type": "text", "term_vector": "with_positions_offsets", "analyzer": "ik_pinyin_analyzer" &#125;, "suggest": &#123; "type": "completion", "analyzer": "ik_max_word", "preserve_separators": true, "preserve_position_increments": true, "max_input_length": 50 &#125; &#125;, "analyzer": "ik_smart" &#125;, "createTime": &#123; "type": "date" &#125;, "enabled": &#123; "type": "integer" &#125;, "favoriteTotal": &#123; "type": "long" &#125;, "fingerprint": &#123; "type": "keyword" &#125;, "id": &#123; "type": "long" &#125;, "images": &#123; "type": "keyword" &#125;, "introduce": &#123; "type": "text", "fields": &#123; "pinyin": &#123; "type": "text", "boost": 10.0, "term_vector": "with_positions_offsets", "analyzer": "ik_pinyin_analyzer" &#125; &#125;, "analyzer": "ik_smart" &#125;, "issueTime": &#123; "type": "long" &#125;, "opusName": &#123; "type": "text", "fields": &#123; "origin": &#123; "type": "keyword" &#125;, "pinyin": &#123; "type": "text", "term_vector": "with_positions_offsets", "analyzer": "ik_pinyin_analyzer" &#125;, "suggest": &#123; "type": "completion", "analyzer": "ik_max_word", "preserve_separators": true, "preserve_position_increments": true, "max_input_length": 50 &#125; &#125;, "analyzer": "ik_max_word" &#125;, "sort": &#123; "type": "long" &#125;, "tags": &#123; "type": "text", "fields": &#123; "origin": &#123; "type": "keyword" &#125;, "pinyin": &#123; "type": "text", "term_vector": "with_positions_offsets", "analyzer": "ik_pinyin_analyzer" &#125;, "suggest": &#123; "type": "completion", "analyzer": "ik_max_word", "preserve_separators": true, "preserve_position_increments": true, "max_input_length": 50 &#125; &#125;, "analyzer": "ik_max_word" &#125;, "type": &#123; "type": "integer" &#125;, "score": &#123; "type": "double" &#125;, "updateTime": &#123; "type": "date" &#125;, "userId": &#123; "type": "long" &#125;, "uuid": &#123; "type": "keyword" &#125;, "viewTotal": &#123; "type": "long" &#125; &#125;&#125;' 查看索引信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191[root@localhost elasticsearch-6.0.0 ]$ curl -XGET "http://127.0.0.1:9200/db_test?pretty=true"&#123; "db_test" : &#123; "aliases" : &#123; &#125;, "mappings" : &#123; "tb_opus" : &#123; "properties" : &#123; "aliasName" : &#123; "type" : "text", "fields" : &#123; "origin" : &#123; "type" : "keyword" &#125;, "pinyin" : &#123; "type" : "text", "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125;, "suggest" : &#123; "type" : "completion", "analyzer" : "ik_max_word", "preserve_separators" : true, "preserve_position_increments" : true, "max_input_length" : 50 &#125; &#125;, "analyzer" : "ik_max_word" &#125;, "appId" : &#123; "type" : "long" &#125;, "author" : &#123; "type" : "text", "fields" : &#123; "origin" : &#123; "type" : "keyword" &#125;, "pinyin" : &#123; "type" : "text", "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125;, "suggest" : &#123; "type" : "completion", "analyzer" : "ik_max_word", "preserve_separators" : true, "preserve_position_increments" : true, "max_input_length" : 50 &#125; &#125;, "analyzer" : "ik_smart" &#125;, "createTime" : &#123; "type" : "date" &#125;, "enabled" : &#123; "type" : "integer" &#125;, "favoriteTotal" : &#123; "type" : "long" &#125;, "fingerprint" : &#123; "type" : "keyword" &#125;, "id" : &#123; "type" : "long" &#125;, "images" : &#123; "type" : "keyword" &#125;, "introduce" : &#123; "type" : "text", "fields" : &#123; "pinyin" : &#123; "type" : "text", "boost" : 10.0, "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125; &#125;, "analyzer" : "ik_smart" &#125;, "issueTime" : &#123; "type" : "long" &#125;, "opusName" : &#123; "type" : "text", "fields" : &#123; "origin" : &#123; "type" : "keyword" &#125;, "pinyin" : &#123; "type" : "text", "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125;, "suggest" : &#123; "type" : "completion", "analyzer" : "ik_max_word", "preserve_separators" : true, "preserve_position_increments" : true, "max_input_length" : 50 &#125; &#125;, "analyzer" : "ik_max_word" &#125;, "score" : &#123; "type" : "double" &#125;, "sort" : &#123; "type" : "long" &#125;, "tags" : &#123; "type" : "text", "fields" : &#123; "origin" : &#123; "type" : "keyword" &#125;, "pinyin" : &#123; "type" : "text", "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125;, "suggest" : &#123; "type" : "completion", "analyzer" : "ik_max_word", "preserve_separators" : true, "preserve_position_increments" : true, "max_input_length" : 50 &#125; &#125;, "analyzer" : "ik_max_word" &#125;, "type" : &#123; "type" : "integer" &#125;, "updateTime" : &#123; "type" : "date" &#125;, "userId" : &#123; "type" : "long" &#125;, "uuid" : &#123; "type" : "keyword" &#125;, "viewTotal" : &#123; "type" : "long" &#125; &#125; &#125; &#125;, "settings" : &#123; "index" : &#123; "number_of_shards" : "1", "provided_name" : "db_test", "creation_date" : "1514353694378", "analysis" : &#123; "filter" : &#123; "mix_pinyin" : &#123; "lowercase" : "true", "padding_char" : " ", "first_letter" : "prefix", "keep_original" : "true", "remove_duplicated_term" : "true", "type" : "pinyin", "keep_full_pinyin" : "true" &#125; &#125;, "analyzer" : &#123; "ik_pinyin_analyzer" : &#123; "filter" : [ "mix_pinyin", "word_delimiter" ], "type" : "custom", "tokenizer" : "ik_max_word" &#125;, "default" : &#123; "tokenizer" : "ik_max_word" &#125; &#125; &#125;, "number_of_replicas" : "1", "uuid" : "seg7yr55R121l8tRW3r7uA", "version" : &#123; "created" : "6000099" &#125; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Elastic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic分词插件介绍]]></title>
    <url>%2F2017%2F10%2F11%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FElastic%2FElastic%E5%88%86%E8%AF%8D%E6%8F%92%E4%BB%B6%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[IK分词插件插件主页https://github.com/medcl/elasticsearch-analysis-ik 查看插件1234[elon@icloud-store elasticsearch-6.0.0]$ bin/elasticsearch-plugin listanalysis-ikanalysis-pinyinanalysis-stconvert 分词测试创建测试索引123456[elon@icloud-store logs]# curl -XPUT "http://192.168.0.103:9200/index_test?pretty=true"&#123; "acknowledged" : true, "shards_acknowledged" : true, "index" : "index_test"&#125; 测试分词效果12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667[elon@icloud-store logs]# curl 'http://192.168.0.103:9200/index_test/_analyze?pretty=true' -H 'Content-Type: application/json' -d '&#123; "analyzer":"ik_max_word", "text":"元旦火车票开售."&#125;'&#123; "tokens" : [ &#123; "token" : "元旦", "start_offset" : 0, "end_offset" : 2, "type" : "CN_WORD", "position" : 0 &#125;, &#123; "token" : "火车票", "start_offset" : 2, "end_offset" : 5, "type" : "CN_WORD", "position" : 1 &#125;, &#123; "token" : "火车", "start_offset" : 2, "end_offset" : 4, "type" : "CN_WORD", "position" : 2 &#125;, &#123; "token" : "车票", "start_offset" : 3, "end_offset" : 5, "type" : "CN_WORD", "position" : 3 &#125;, &#123; "token" : "开售", "start_offset" : 5, "end_offset" : 7, "type" : "CN_WORD", "position" : 4 &#125; ]&#125;[elon@icloud-store logs]# curl 'http://192.168.0.103:9200/index_test/_analyze?pretty=true' -H 'Content-Type: application/json' -d '&#123; "analyzer":"ik_smart", "text":"元旦火车 票开售."&#125;'&#123; "tokens" : [ &#123; "token" : "元旦", "start_offset" : 0, "end_offset" : 2, "type" : "CN_WORD", "position" : 0 &#125;, &#123; "token" : "火车票", "start_offset" : 2, "end_offset" : 5, "type" : "CN_WORD", "position" : 1 &#125;, &#123; "token" : "开售", "start_offset" : 5, "end_offset" : 7, "type" : "CN_WORD", "position" : 2 &#125; ]&#125; 通过对中文串的分词结果可以明显看到, ik_smart和ik_max_word两种模式的分词结果上的差异. ik_max_word: 会将文本做最细粒度的拆分，比如会将“元旦火车票开售”拆分为“元旦,火车票,火车,车票,开售”，会穷尽各种可能的组合； ik_smart: 会做最粗粒度的拆分，比如会将“元旦火车票开售”拆分为“元旦,火车票,开售”; 应用示例删除索引1234[wuyu@icloud-store elasticsearch-6.0.0]$ curl -XDELETE "http://192.168.0.103:9200/index_test*?pretty=true"&#123; "acknowledged" : true&#125; 创建索引12345678910111213141516171819202122232425262728293031[elon@icloud-store logs]# curl -XPUT "http://192.168.0.103:9200/index_test?pretty=true" -H 'Content-Type: application/json' -d'&#123; "index": &#123; "number_of_shards": "1", "number_of_replicas": "1" &#125;&#125;'&#123; "acknowledged" : true, "shards_acknowledged" : true, "index" : "index_test"&#125;[elon@icloud-store elasticsearch-6.0.0]$ curl -XGET "http://192.168.0.103:9200/index_test?pretty=true"&#123; "index_test" : &#123; "aliases" : &#123; &#125;, "mappings" : &#123; &#125;, "settings" : &#123; "index" : &#123; "creation_date" : "1512990681578", "number_of_shards" : "1", "number_of_replicas" : "1", "uuid" : "tXNLl-JERiuIxn-g4Ip3Og", "version" : &#123; "created" : "6000099" &#125;, "provided_name" : "index_test" &#125; &#125; &#125;&#125; 创建类型1234567891011121314151617181920212223242526272829303132333435363738394041[[elon@icloud-store elasticsearch-6.0.0]$ curl -XPOST "http://192.168.0.103:9200/index_test/person/_mapping?pretty=true" -H 'Content-Type: application/json' -d'&#123; "properties": &#123; "name": &#123; "type": "text", "analyzer": "ik_max_word", "search_analyzer": "ik_max_word" &#125; &#125;&#125;'&#123; "acknowledged" : true&#125;[elon@icloud-store elasticsearch-6.0.0]$ curl -XGET "http://192.168.0.103:9200/index_test?pretty=true"&#123; "index_test" : &#123; "aliases" : &#123; &#125;, "mappings" : &#123; "person" : &#123; "properties" : &#123; "name" : &#123; "type" : "text", "analyzer" : "ik_max_word" &#125; &#125; &#125; &#125;, "settings" : &#123; "index" : &#123; "creation_date" : "1512990681578", "number_of_shards" : "1", "number_of_replicas" : "1", "uuid" : "tXNLl-JERiuIxn-g4Ip3Og", "version" : &#123; "created" : "6000099" &#125;, "provided_name" : "index_test" &#125; &#125; &#125;&#125; 索引数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[elon@icloud-store elasticsearch-6.0.0]$ curl -XPOST "http://192.168.0.103:9200/index_test/person/1?pretty=true" -H 'Content-Type: application/json' -d'&#123;"name":"元旦火车票开售."&#125;'&#123; "_index" : "index_test", "_type" : "person", "_id" : "1", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 0, "_primary_term" : 1&#125;[elon@icloud-store elasticsearch-6.0.0]$ curl -XPOST "http://192.168.0.103:9200/index_test/person/2?pretty=true" -H 'Content-Type: application/json' -d'&#123;"name":"元旦节是公历新一年的第一天."&#125;'&#123; "_index" : "index_test", "_type" : "person", "_id" : "2", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 1, "_primary_term" : 1&#125;[elon@icloud-store elasticsearch-6.0.0]$ curl -XPOST "http://192.168.0.103:9200/index_test/person?pretty=true" -H 'Content-Type: application/json' -d'&#123;"name":"使用ES默认ID机制创建唯一键"&#125;'&#123; "_index" : "index_test", "_type" : "person", "_id" : "psZNRWABuJTmkY1Rwcoe", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 2, "_primary_term" : 1&#125;[elon@icloud-store elasticsearch-6.0.0]$ curl -XPOST "http://192.168.0.103:9200/index_test/person/2?pretty=true" -H 'Content-Type: application/json' -d'&#123;"name":"元旦节是每年1月1号"&#125;'&#123; "_index" : "index_test", "_type" : "person", "_id" : "2", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 1, "_primary_term" : 1&#125; 查询数据(主键)12345678910111213141516171819202122[elon@icloud-store elasticsearch-6.0.0]$ curl -XGET "http://192.168.0.103:9200/index_test/person/2?pretty=true"&#123; "_index" : "index_test", "_type" : "person", "_id" : "1", "_version" : 1, "found" : true, "_source" : &#123; "name" : "元旦节是公历新一年的第一天." &#125;&#125;[elon@icloud-store elasticsearch-6.0.0]$ curl -XGET "http://192.168.0.103:9200/index_test/person/psZNRWABuJTmkY1Rwcoe?pretty=true"&#123; "_index" : "index_test", "_type" : "person", "_id" : "psZNRWABuJTmkY1Rwcoe", "_version" : 1, "found" : true, "_source" : &#123; "name" : "使用ES默认ID机制创建唯一键" &#125;&#125; 更新数据1234567891011121314151617181920212223242526[elon@icloud-store elasticsearch-6.0.0]$ curl -XPOST "http://192.168.0.103:9200/index_test/person/2?pretty=true" -H 'Content-Type: application/json' -d'&#123;"name":"元旦节是每年1月1号"&#125;'&#123; "_index" : "index_test", "_type" : "person", "_id" : "2", "_version" : 2, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 4, "_primary_term" : 1&#125;[elon@icloud-store elasticsearch-6.0.0]$ curl -XGET "http://192.168.0.103:9200/index_test/person/2?pretty=true"&#123; "_index" : "index_test", "_type" : "person", "_id" : "2", "_version" : 2, "found" : true, "_source" : &#123; "name" : "元旦节是每年1月1号" &#125;&#125; 查询数据(关键词、分页、高量)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[elon@icloud-store elasticsearch-6.0.0]$ curl -XPOST "192.168.0.103:9200/index_test/person/_search?pretty" -H 'Content-Type: application/json' -d'&#123; "from" : 0, "size" : 10, "query": &#123; "match": &#123; "name": "元旦" &#125; &#125;, "highlight": &#123; "fields": &#123; "name": &#123;&#125; &#125; &#125;&#125;'&#123; "took" : 14, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 2, "max_score" : 0.5551721, "hits" : [ &#123; "_index" : "index_test", "_type" : "person", "_id" : "1", "_score" : 0.5551721, "_source" : &#123; "name" : "元旦火车票开售." &#125;, "highlight" : &#123; "name" : [ "&lt;em&gt;元旦&lt;/em&gt;火车票开售." ] &#125; &#125;, &#123; "_index" : "index_test", "_type" : "person", "_id" : "2", "_score" : 0.4471386, "_source" : &#123; "name" : "元旦节是每年1月1号" &#125;, "highlight" : &#123; "name" : [ "&lt;em&gt;元旦&lt;/em&gt;节是每年1月1号" ] &#125; &#125; ] &#125;&#125; Pinyin分词插件插件主页https://github.com/medcl/elasticsearch-analysis-pinyin 查看插件1234[elon@icloud-store elasticsearch-6.0.0]$ bin/elasticsearch-plugin listanalysis-ikanalysis-pinyinanalysis-stconvert 分词测试创建测试索引1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768[elon@icloud-store elasticsearch-6.0.0]$ curl -XPUT "http://192.168.0.103:9200/index_test?pretty=true" -H 'Content-Type: application/json' -d'&#123; "index" : &#123; "analysis" : &#123; "analyzer" : &#123; "pinyin_analyzer" : &#123; "tokenizer" : "mix_pinyin" &#125; &#125;, "tokenizer" : &#123; "mix_pinyin" : &#123; "type" : "pinyin", "keep_separate_first_letter" : false, "keep_full_pinyin" : true, "keep_original" : true, "limit_first_letter_length" : 16, "lowercase" : true, "remove_duplicated_term" : true &#125; &#125; &#125; &#125;, "number_of_shards": "3", "number_of_replicas": "1"&#125;'&#123; "acknowledged" : true, "shards_acknowledged" : true, "index" : "index_test"&#125;[elon@icloud-store elasticsearch-6.0.0]$ curl -XGET "http://192.168.0.103:9200/index_test?pretty=true" &#123; "index_test" : &#123; "aliases" : &#123; &#125;, "mappings" : &#123; &#125;, "settings" : &#123; "index" : &#123; "number_of_shards" : "3", "provided_name" : "index_test", "creation_date" : "1513148861791", "analysis" : &#123; "analyzer" : &#123; "pinyin_analyzer" : &#123; "tokenizer" : "mix_pinyin" &#125; &#125;, "tokenizer" : &#123; "mix_pinyin" : &#123; "lowercase" : "true", "keep_original" : "true", "remove_duplicated_term" : "true", "keep_separate_first_letter" : "false", "type" : "pinyin", "limit_first_letter_length" : "16", "keep_full_pinyin" : "true" &#125; &#125; &#125;, "number_of_replicas" : "1", "uuid" : "uIW8o_bGTkefLEVL5mTw7w", "version" : &#123; "created" : "6000099" &#125; &#125; &#125; &#125;&#125; 测试分词效果1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768[elon@icloud-store elasticsearch-6.0.0]$ curl "http://192.168.0.103:9200/index_test/_analyze?pretty=true" -H 'Content-Type: application/json' -d'&#123; "analyzer":"pinyin_analyzer", "text":"元旦火车票开售."&#125;'&#123; "tokens" : [ &#123; "token" : "yuan", "start_offset" : 0, "end_offset" : 1, "type" : "word", "position" : 0 &#125;, &#123; "token" : "元旦火车票开售.", "start_offset" : 0, "end_offset" : 8, "type" : "word", "position" : 0 &#125;, &#123; "token" : "ydhcpks", "start_offset" : 0, "end_offset" : 7, "type" : "word", "position" : 0 &#125;, &#123; "token" : "dan", "start_offset" : 1, "end_offset" : 2, "type" : "word", "position" : 1 &#125;, &#123; "token" : "huo", "start_offset" : 2, "end_offset" : 3, "type" : "word", "position" : 2 &#125;, &#123; "token" : "che", "start_offset" : 3, "end_offset" : 4, "type" : "word", "position" : 3 &#125;, &#123; "token" : "piao", "start_offset" : 4, "end_offset" : 5, "type" : "word", "position" : 4 &#125;, &#123; "token" : "kai", "start_offset" : 5, "end_offset" : 6, "type" : "word", "position" : 5 &#125;, &#123; "token" : "shou", "start_offset" : 6, "end_offset" : 7, "type" : "word", "position" : 6 &#125; ]&#125; 应用示例(Ik+Pinyin)应用示例展示IK+Pinyin集成的索引分词效果 删除索引1234[elon@icloud-store elasticsearch-6.0.0]$ curl -XDELETE "http://192.168.0.103:9200/index_test*?pretty=true"&#123; "acknowledged" : true&#125; 创建索引12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576[elon@icloud-store elasticsearch-6.0.0]$ curl -XPUT "http://192.168.0.103:9200/index_test?pretty=true" -H 'Content-Type: application/json' -d'&#123; "index" : &#123; "analysis": &#123; "analyzer": &#123; "default" : &#123; "tokenizer" : "ik_smart" &#125;, "ik_pinyin_analyzer": &#123; "type": "custom", "tokenizer": "ik_smart", "filter": [ "mix_pinyin", "word_delimiter" ] &#125; &#125;, "filter": &#123; "mix_pinyin": &#123; "type": "pinyin", "first_letter": "prefix", "padding_char": " " &#125; &#125; &#125; &#125;, "number_of_shards": "3", "number_of_replicas": "1"&#125;'&#123; "acknowledged" : true, "shards_acknowledged" : true, "index" : "index_test"&#125;[elon@icloud-store elasticsearch-6.0.0]$ curl -XGET "http://192.168.0.103:9200/index_test?pretty=true" &#123; "index_test" : &#123; "aliases" : &#123; &#125;, "mappings" : &#123; &#125;, "settings" : &#123; "index" : &#123; "number_of_shards" : "3", "provided_name" : "index_test", "creation_date" : "1513155447722", "analysis" : &#123; "filter" : &#123; "mix_pinyin" : &#123; "padding_char" : " ", "type" : "pinyin", "first_letter" : "prefix" &#125; &#125;, "analyzer" : &#123; "ik_pinyin_analyzer" : &#123; "filter" : [ "mix_pinyin", "word_delimiter" ], "type" : "custom", "tokenizer" : "ik_smart" &#125;, "default" : &#123; "tokenizer" : "ik_smart" &#125; &#125; &#125;, "number_of_replicas" : "1", "uuid" : "JiyIMwnlS7mpMB1T-3MmOQ", "version" : &#123; "created" : "6000099" &#125; &#125; &#125; &#125;&#125; 测试分词效果123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145[elon@icloud-store elasticsearch-6.0.0]$ curl "http://192.168.0.103:9200/index_test/_analyze?pretty=true" -H 'Content-Type: application/json' -d'&#123; "analyzer":"ik_pinyin_analyzer", "text":"元旦节是公历新一年的第一天."&#125;'&#123; "tokens" : [ &#123; "token" : "yuan", "start_offset" : 0, "end_offset" : 3, "type" : "CN_WORD", "position" : 0 &#125;, &#123; "token" : "dan", "start_offset" : 0, "end_offset" : 3, "type" : "CN_WORD", "position" : 1 &#125;, &#123; "token" : "jie", "start_offset" : 0, "end_offset" : 3, "type" : "CN_WORD", "position" : 2 &#125;, &#123; "token" : "ydj", "start_offset" : 0, "end_offset" : 3, "type" : "CN_WORD", "position" : 2 &#125;, &#123; "token" : "shi", "start_offset" : 3, "end_offset" : 4, "type" : "CN_CHAR", "position" : 3 &#125;, &#123; "token" : "s", "start_offset" : 3, "end_offset" : 4, "type" : "CN_CHAR", "position" : 3 &#125;, &#123; "token" : "gong", "start_offset" : 4, "end_offset" : 6, "type" : "CN_WORD", "position" : 4 &#125;, &#123; "token" : "li", "start_offset" : 4, "end_offset" : 6, "type" : "CN_WORD", "position" : 5 &#125;, &#123; "token" : "gl", "start_offset" : 4, "end_offset" : 6, "type" : "CN_WORD", "position" : 5 &#125;, &#123; "token" : "xin", "start_offset" : 6, "end_offset" : 7, "type" : "CN_CHAR", "position" : 6 &#125;, &#123; "token" : "x", "start_offset" : 6, "end_offset" : 7, "type" : "CN_CHAR", "position" : 6 &#125;, &#123; "token" : "yi", "start_offset" : 7, "end_offset" : 9, "type" : "CN_WORD", "position" : 7 &#125;, &#123; "token" : "nian", "start_offset" : 7, "end_offset" : 9, "type" : "CN_WORD", "position" : 8 &#125;, &#123; "token" : "yn", "start_offset" : 7, "end_offset" : 9, "type" : "CN_WORD", "position" : 8 &#125;, &#123; "token" : "de", "start_offset" : 9, "end_offset" : 10, "type" : "CN_CHAR", "position" : 9 &#125;, &#123; "token" : "d", "start_offset" : 9, "end_offset" : 10, "type" : "CN_CHAR", "position" : 9 &#125;, &#123; "token" : "di", "start_offset" : 10, "end_offset" : 13, "type" : "CN_WORD", "position" : 10 &#125;, &#123; "token" : "yi", "start_offset" : 10, "end_offset" : 13, "type" : "CN_WORD", "position" : 11 &#125;, &#123; "token" : "tian", "start_offset" : 10, "end_offset" : 13, "type" : "CN_WORD", "position" : 12 &#125;, &#123; "token" : "dyt", "start_offset" : 10, "end_offset" : 13, "type" : "CN_WORD", "position" : 12 &#125; ]&#125; 创建类型123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778[elon@icloud-store elasticsearch-6.0.0]$ curl "http://192.168.0.103:9200/index_test/article/_mapping?pretty=true" -H 'Content-Type: application/json' -d'&#123; "properties": &#123; "name": &#123; "type" : "text", "analyzer" : "ik_smart", "fields" : &#123; "pinyin" : &#123; "type" : "text", "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer", "boost" : 10.0 &#125; &#125; &#125; &#125;&#125;'&#123; "acknowledged" : true&#125;[elon@icloud-store elasticsearch-6.0.0]$ curl -XGET "http://192.168.0.103:9200/index_test?pretty=true" &#123; "index_test" : &#123; "aliases" : &#123; &#125;, "mappings" : &#123; "article" : &#123; "properties" : &#123; "name" : &#123; "type" : "text", "fields" : &#123; "pinyin" : &#123; "type" : "text", "boost" : 10.0, "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125; &#125;, "analyzer" : "ik_smart" &#125; &#125; &#125; &#125;, "settings" : &#123; "index" : &#123; "number_of_shards" : "3", "provided_name" : "index_test", "creation_date" : "1513155447722", "analysis" : &#123; "filter" : &#123; "mix_pinyin" : &#123; "padding_char" : " ", "type" : "pinyin", "first_letter" : "prefix" &#125; &#125;, "analyzer" : &#123; "ik_pinyin_analyzer" : &#123; "filter" : [ "mix_pinyin", "word_delimiter" ], "type" : "custom", "tokenizer" : "ik_smart" &#125;, "default" : &#123; "tokenizer" : "ik_smart" &#125; &#125; &#125;, "number_of_replicas" : "1", "uuid" : "JiyIMwnlS7mpMB1T-3MmOQ", "version" : &#123; "created" : "6000099" &#125; &#125; &#125; &#125;&#125; 索引数据123456789101112131415161718192021222324252627282930313233343536373839404142434445[elon@icloud-store elasticsearch-6.0.0]$ curl -XPOST "http://192.168.0.103:9200/index_test/article/1?pretty=true" -H 'Content-Type: application/json' -d'&#123;"name":"元旦火车票 开售."&#125;'&#123; "_index" : "index_test", "_type" : "article", "_id" : "1", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 0, "_primary_term" : 1&#125;[elon@icloud-store elasticsearch-6.0.0]$ curl -XPOST "http://192.168.0.103:9200/index_test/article/2?pretty=true" -H 'Content-Type: application/json' -d'&#123;"name":"元旦节是公历新一年的第一天."&#125;'&#123; "_index" : "index_test", "_type" : "article", "_id" : "2", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 0, "_primary_term" : 1&#125;[elon@icloud-store elasticsearch-6.0.0]$ curl -XPOST "http://192.168.0.103:9200/index_test/article/3?pretty=true" -H 'Content-Type: application/json' -d'&#123;"name":"元旦节是每年1月1号"&#125;'&#123; "_index" : "index_test", "_type" : "article", "_id" : "3", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 1, "_primary_term" : 1&#125; 查询数据(主键)1234567891011[elon@icloud-store elasticsearch-6.0.0]$ curl -XGET "http://192.168.0.103:9200/index_test/article/2?pretty=true"&#123; "_index" : "index_test", "_type" : "article", "_id" : "2", "_version" : 1, "found" : true, "_source" : &#123; "name" : "元旦节是公历新一年的第一天." &#125;&#125; 查询数据(中文)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[elon@icloud-store elasticsearch-6.0.0]$ curl -XPOST "http://192.168.0.103:9200/index_test/article/_search?pretty" -H 'Content-Type: application/json' -d'&#123; "from" : 0, "size" : 10, "query": &#123; "match": &#123; "name": "元旦节" &#125; &#125;, "highlight": &#123; "fields": &#123; "name": &#123;&#125; &#125; &#125;&#125;'&#123; "took" : 9, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 2, "max_score" : 0.6288345, "hits" : [ &#123; "_index" : "index_test", "_type" : "article", "_id" : "3", "_score" : 0.6288345, "_source" : &#123; "name" : "元旦节是每年1月1号" &#125;, "highlight" : &#123; "name" : [ "&lt;em&gt;元旦节&lt;/em&gt;是每年1月1号" ] &#125; &#125;, &#123; "_index" : "index_test", "_type" : "article", "_id" : "2", "_score" : 0.2876821, "_source" : &#123; "name" : "元旦节是公历新一年的第一天." &#125;, "highlight" : &#123; "name" : [ "&lt;em&gt;元旦节&lt;/em&gt;是公历新一年的第一天." ] &#125; &#125; ] &#125;&#125; 查询数据(拼音)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134[elon@icloud-store elasticsearch-6.0.0]$ curl -XPOST "http://192.168.0.103:9200/index_test/article/_search?pretty" -H 'Content-Type: application/json' -d'&#123; "from" : 0, "size" : 10, "query": &#123; "match": &#123; "name.pinyin": "ydj" &#125; &#125;, "highlight": &#123; "fields": &#123; "name.pinyin": &#123;&#125; &#125; &#125;&#125;'&#123; "took" : 7, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 2, "max_score" : 7.5108185, "hits" : [ &#123; "_index" : "index_test", "_type" : "article", "_id" : "3", "_score" : 7.5108185, "_source" : &#123; "name" : "元旦节是每年1月1号" &#125;, "highlight" : &#123; "name.pinyin" : [ "&lt;em&gt;元旦节&lt;/em&gt;是每年1&lt;em&gt;月&lt;/em&gt;1号" ] &#125; &#125;, &#123; "_index" : "index_test", "_type" : "article", "_id" : "2", "_score" : 3.693319, "_source" : &#123; "name" : "元旦节是公历新一年的第一天." &#125;, "highlight" : &#123; "name.pinyin" : [ "&lt;em&gt;元旦节&lt;/em&gt;是公历新一年&lt;em&gt;的&lt;/em&gt;第一天." ] &#125; &#125; ] &#125;&#125;[elon@icloud-store elasticsearch-6.0.0]$ curl -XPOST "http://192.168.0.103:9200/index_test/article/_search?pretty" -H 'Content-Type: application/json' -d'&#123; "from" : 0, "size" : 10, "query": &#123; "match": &#123; "name.pinyin": "元旦j" &#125; &#125;, "highlight": &#123; "fields": &#123; "name.pinyin": &#123;&#125; &#125; &#125;&#125;'&#123; "took" : 16, "timed_out" : false, "_shards" : &#123; "total" : 3, "successful" : 3, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 3, "max_score" : 3.693319, "hits" : [ &#123; "_index" : "index_test", "_type" : "article", "_id" : "2", "_score" : 3.693319, "_source" : &#123; "name" : "元旦节是公历新一年的第一天." &#125;, "highlight" : &#123; "name.pinyin" : [ "&lt;em&gt;元旦节&lt;/em&gt;是公历新一年的第一天." ] &#125; &#125;, &#123; "_index" : "index_test", "_type" : "article", "_id" : "1", "_score" : 2.560377, "_source" : &#123; "name" : "元旦火车票 开售." &#125;, "highlight" : &#123; "name.pinyin" : [ "&lt;em&gt;元旦&lt;/em&gt;火车票 开售." ] &#125; &#125;, &#123; "_index" : "index_test", "_type" : "article", "_id" : "3", "_score" : 1.9756038, "_source" : &#123; "name" : "元旦节是每年1月1号" &#125;, "highlight" : &#123; "name.pinyin" : [ "&lt;em&gt;元旦节&lt;/em&gt;是每年1月1号" ] &#125; &#125; ] &#125;&#125;`]]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Elastic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic安装配置说明]]></title>
    <url>%2F2017%2F10%2F10%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FElastic%2FElastic%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[Elastic下载安装下载Elastic123[root@elonsu cloud]# wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.0.tar.gz[root@elonsu cloud]# tar -zxvf elasticsearch-5.6.0.tar.gz [root@elonsu cloud]# mv elasticsearch-5.6.0 elastic 创建用户Elastic默认不支持root用户启动, 这里创建用户dennisit 123456789101112[root@elonsu cloud]# pwd/export/cloud[root@elonsu ~]# adduser dennisit[root@elonsu ~]# passwd dennisitChanging password for user dennisit.New password: Retype new password: passwd: all authentication tokens updated successfully.[root@elonsu cloud]# chown dennisit:dennisit elastic -R[root@elonsu cloud]# su dennisit[dennisit@elonsu cloud]$ cd elastic[dennisit@elonsu elastic]$ bin/elasticsearch &amp; Elastic分词插件中文分词插件1[dennisit@elonsu elastic]$ bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.0.0/elasticsearch-analysis-ik-6.0.0.zip 拼音分词插件1[dennisit@elonsu elastic]$ bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-pinyin/releases/download/v6.0.0/elasticsearch-analysis-pinyin-6.0.0.zip 简繁转换插件1[dennisit@elonsu elastic]$ bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-stconvert/releases/download/v6.0.0/elasticsearch-analysis-stconvert-6.0.0.zip 安装插件查看1234[dennisit@elonsu elastic]$ bin/elasticsearch-plugin listanalysis-ikanalysis-pinyinanalysis-stconvert Elastic插件-Head插件插件地址: https://github.com/mobz/elasticsearch-head/推荐安装: Chrome插件方式(简单省事) Elastic插件-IK分词器插件主页https://github.com/medcl/elasticsearch-analysis-ik 安装IK分词器1234567[dennisit@elonsu cloud]$ cd elastic[dennisit@elonsu elastic]$ bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.6.0/elasticsearch-analysis-ik-5.6.0.zip-&gt; Downloading https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v5.6.0/elasticsearch-analysis-ik-5.6.0.zip[=================================================] 100% -&gt; Installed analysis-ik[dennisit@elonsu elastic]$ bin/elasticsearch &amp;[1] 33893 查看安装插件1234[elon@icloud-store elasticsearch-6.0.0]$ bin/elasticsearch-plugin listanalysis-ikanalysis-pinyinanalysis-stconvert 验证IK分词器创建index123[dennisit@elonsu elastic]$ curl -XPUT http://localhost:9200/index[2017-09-26T14:08:12,782][INFO ][o.w.a.d.Monitor ] try load config from /export/cloud/elastic/config/analysis-ik/IKAnalyzer.cfg.xml[2017-09-26T14:08:13,243][INFO ][o.e.c.m.MetaDataCreateIndexService] [OWIX3y4] [index] creating index, cause [api], templates [], shards [5]/[1], mappings [] 创建mapping12345678910111213[dennisit@elonsu elastic]$ curl -XPOST http://localhost:9200/index/fulltext/_mapping -d'&gt; &#123;&gt; "properties": &#123;&gt; "content": &#123;&gt; "type": "text",&gt; "analyzer": "ik_max_word",&gt; "search_analyzer": "ik_max_word"&gt; &#125;&gt; &#125;&gt; &gt; &#125;'[2017-09-26T14:08:58,805][INFO ][o.e.c.m.MetaDataMappingService] [OWIX3y4] [index/ZNJu9Nz_R-qJKTDUnoIlxQ] create_mapping [fulltext]&#123;"acknowledged":true&#125; 索引Document1234567891011121314151617181920212223242526272829303132333435363738394041424344[dennisit@elonsu elastic]$ curl -XPOST http://localhost:9200/index/fulltext/3?pretty -d '&#123;"content":"中韩渔警冲突调查：韩警平均每天扣1艘中国渔船"&#125;'&#123; "_index" : "index", "_type" : "fulltext", "_id" : "3", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "created" : false&#125;[dennisit@elonsu elastic]$ curl -XPOST http://localhost:9200/index/fulltext/4?pretty -d '&#123;"content":"中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首"&#125;'&#123; "_index" : "index", "_type" : "fulltext", "_id" : "4", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "created" : true&#125;[dennisit@elonsu elastic]$ curl -XPOST http://localhost:9200/index/fulltext/5?pretty -d '&#123;"content":"10月1日是中国国庆节"&#125;'&#123; "_index" : "index", "_type" : "fulltext", "_id" : "5", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "created" : true&#125; 查询Document1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071[dennisit@elonsu elastic]$ curl -XPOST http://localhost:9200/index/fulltext/_search?pretty -d' &#123; "query" : &#123; "match" : &#123; "content" : "中国" &#125;&#125;, "highlight" : &#123; "pre_tags" : ["&lt;tag1&gt;", "&lt;tag2&gt;"], "post_tags" : ["&lt;/tag1&gt;", "&lt;/tag2&gt;"], "fields" : &#123; "content" : &#123;&#125; &#125; &#125; &#125;'&#123; "took" : 6, "timed_out" : false, "_shards" : &#123; "total" : 5, "successful" : 5, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 3, "max_score" : 0.27233246, "hits" : [ &#123; "_index" : "index", "_type" : "fulltext", "_id" : "5", "_score" : 0.27233246, "_source" : &#123; "content" : "10月1日是中国国庆节" &#125;, "highlight" : &#123; "content" : [ "10月1日是&lt;tag1&gt;中国&lt;/tag1&gt;国庆节" ] &#125; &#125;, &#123; "_index" : "index", "_type" : "fulltext", "_id" : "4", "_score" : 0.27179778, "_source" : &#123; "content" : "中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首" &#125;, "highlight" : &#123; "content" : [ "&lt;tag1&gt;中国&lt;/tag1&gt;驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首" ] &#125; &#125;, &#123; "_index" : "index", "_type" : "fulltext", "_id" : "3", "_score" : 0.27179778, "_source" : &#123; "content" : "中韩渔警冲突调查：韩警平均每天扣1艘中国渔船" &#125;, "highlight" : &#123; "content" : [ "中韩渔警冲突调查：韩警平均每天扣1艘&lt;tag1&gt;中国&lt;/tag1&gt;渔船" ] &#125; &#125; ] &#125;&#125;[dennisit@elonsu elastic]$ 删除Document123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384[dennisit@elonsu elastic]$ curl -XDELETE http://localhost:9200/index/fulltext/3?pretty&#123; "found" : true, "_index" : "index", "_type" : "fulltext", "_id" : "3", "_version" : 2, "result" : "deleted", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;&#125;[dennisit@elonsu elastic]$ curl -XDELETE http://localhost:9200/index/fulltext/5?pretty&#123; "found" : true, "_index" : "index", "_type" : "fulltext", "_id" : "5", "_version" : 2, "result" : "deleted", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;&#125;[dennisit@elonsu elastic]$ curl -XDELETE http://localhost:9200/index/fulltext/9?pretty&#123; "found" : false, "_index" : "index", "_type" : "fulltext", "_id" : "9", "_version" : 1, "result" : "not_found", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;&#125;[dennisit@elonsu elastic]$ curl -XPOST http://localhost:9200/index/fulltext/_search?pretty -d' &#123; "query" : &#123; "match" : &#123; "content" : "中国" &#125;&#125;, "highlight" : &#123; "pre_tags" : ["&lt;tag1&gt;", "&lt;tag2&gt;"], "post_tags" : ["&lt;/tag1&gt;", "&lt;/tag2&gt;"], "fields" : &#123; "content" : &#123;&#125; &#125; &#125; &#125;'&#123; "took" : 7, "timed_out" : false, "_shards" : &#123; "total" : 5, "successful" : 5, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 1, "max_score" : 0.27179778, "hits" : [ &#123; "_index" : "index", "_type" : "fulltext", "_id" : "4", "_score" : 0.27179778, "_source" : &#123; "content" : "中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首" &#125;, "highlight" : &#123; "content" : [ "&lt;tag1&gt;中国&lt;/tag1&gt;驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首" ] &#125; &#125; ] &#125;&#125; 修改索引查询最大窗口123456789101112131415161718192021222324252627282930313233343536373839[root@localhost ~ ]$ curl http://localhost:9200/idx_index/_settings/?pretty&#123; "idx_index" : &#123; "settings" : &#123; "index" : &#123; "creation_date" : "1529829607133", "number_of_shards" : "5", "number_of_replicas" : "1", "uuid" : "XtX1yMORQWqH2jihDqLomw", "version" : &#123; "created" : "6020499" &#125;, "provided_name" : "idx_index" &#125; &#125; &#125;&#125;[root@localhost ~ ]$ curl -H "Content-Type: application/json" -XPUT http://localhost:9200/idx_index/_settings\?pretty -d '&#123; "index" : &#123; "max_result_window" : 100000000&#125;&#125;'&#123; "acknowledged" : true&#125;[root@localhost ~ ]$ curl http://localhost:9200/idx_index/_settings/\?pretty&#123; "idx_index" : &#123; "settings" : &#123; "index" : &#123; "number_of_shards" : "5", "provided_name" : "idx_index", "max_result_window" : "100000000", "creation_date" : "1529829607133", "number_of_replicas" : "1", "uuid" : "XtX1yMORQWqH2jihDqLomw", "version" : &#123; "created" : "6020499" &#125; &#125; &#125; &#125;&#125; 安装Nginx12345678910111213141516171819202122232425[root@elonsu cloud]# wget http://nginx.org/download/nginx-1.12.1.tar.gz[root@elonsu cloud]# tar -zxvf nginx-1.12.1.tar.gz [root@elonsu cloud]# cd nginx-1.12.1[root@elonsu nginx-1.12.1]# ./configure \ --prefix=/usr/local/nginx \ --sbin-path=/usr/local/nginx/sbin/nginx \ --conf-path=/usr/local/nginx/conf/nginx.conf \ --http-log-path=/usr/local/nginx/logs/access.log \ --error-log-path=/usr/local/nginx/logs/error.log \ --pid-path=/usr/local/nginx/logs/nginx.pid \ --lock-path=/usr/local/nginx/lock/nginx.lock \ --http-client-body-temp-path=/usr/local/nginx/client_body_temp \ --http-proxy-temp-path=/usr/local/nginx/proxy_temp \ --http-fastcgi-temp-path=/usr/local/nginx/fastcgi-temp \ --http-uwsgi-temp-path=/usr/local/nginx/uwsgi-temp \ --http-scgi-temp-path=/usr/local/nginx/scgi-temp \ --user=root \ --group=root \ --with-http_ssl_module \ --with-http_flv_module \ --with-http_mp4_module \ --with-http_gzip_static_module \ --with-http_stub_status_module[root@elonsu nginx-1.12.1]# make [root@elonsu nginx-1.12.1]# make install 配置nginx上面我们安装了nginx在目录/usr/local/nginx/,接下来配置upstream Nginx主配置文件修改123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@elonsu conf]# pwd/usr/local/nginx/conf[root@elonsu conf]# cat nginx.conf#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; access_log logs/access.log main; sendfile on; keepalive_timeout 65; gzip on; server_names_hash_bucket_size 64; server_names_hash_max_size 512; include domains/*; server &#123; listen 80; server_name localhost; location / &#123; root html; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 该配置文件我们做了最简单的配置, 在nginx默认的配置上做了如下修改: 打开log_format 增加server_names_hash相关配置 使用include domains/*; 加载我们自定义upstream配置 自定义Upstream配置这里我们对elastic进行代理 123456789101112131415161718192021222324[root@elonsu domains]# pwd/usr/local/nginx/conf/domains[root@elonsu domains]# cat elastic.icloud.com.conf upstream domain.elastic.local &#123; server 127.0.0.1:9200 weight=10 max_fails=2 fail_timeout=300s;&#125;server &#123; listen 80; server_name elastic.icloud.com; root /export/cloud/elastic/; access_log /export/cloud/elastic/logs/elastic.icloud.com_access.log main; error_log /export/cloud/elastic/logs/elastic.icloud.com_error.log warn; error_page 403 404 /40x.html; location / &#123; index index.html index.htm; proxy_next_upstream http_500 http_502 http_503 http_504 error timeout invalid_header; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://domain.elastic.local; expires 0d; &#125;&#125; 配置完启动nginxNginx的启停脚本 启动Nginx并访问 1234567891011121314[root@elonsu domains]# service nginx restartStopping Nginx: [ OK ]Starting Nginx: [ OK ][root@elonsu domains]# curl -XGET http://127.0.0.1:9200/index/fulltext/4\?pretty&#123; "_index" : "index", "_type" : "fulltext", "_id" : "4", "_version" : 1, "found" : true, "_source" : &#123; "content" : "中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首" &#125;&#125; 本机使用外网域名访问上面服务器上Elastic的反向代理已经配置好, 本机使用域名访问(因为域名非公网域名,所以本机需要配置host) 1234567891011[root@localhost ~ ]$ curl http://elastic.icloud.com/index/fulltext/4\?pretty&#123; "_index" : "index", "_type" : "fulltext", "_id" : "4", "_version" : 1, "found" : true, "_source" : &#123; "content" : "中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首" &#125;&#125;]]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Elastic</tag>
        <tag>CentOs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[持续集成 - 发布系统]]></title>
    <url>%2F2017%2F09%2F22%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2F%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90-%E5%8F%91%E5%B8%83%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[发布系统在技术团队中有着重要作用, 承担我们每天的服务部署工作, 企业发布系统根据企业技术能力来决定, 小公司不像那些公司一样,有充足的人力去开发一套自己的发布系统. 所以尽可能的去寻找一些适合企业应用的部署系统, 以提高人力成本. 这里推荐两款开源的发布系统: jekins: https://jenkins.io/ (Java语言开发)[普遍] walle: http://www.walle-web.io/ (PHP语言开发)[小众] jekins可能大家都很熟悉,也有好多文章, walle是国人写的一个轻量的发布系统, 只所以推荐它 是因为相对其它开源的发布系统来说, 其UI界面至少看着比较舒服, 基本功能也都有. 支持国产开源. Walle安装部署Walle是基于PHP写的, 所以安装前需要安装PHP环境. PHP环境安装这里选用php最新稳定版7.1.10 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@localhost ~]# wget http://cn2.php.net/distributions/php-7.1.10.tar.gz[root@localhost ~]# tar -zxvf php-7.1.10.tar.gz [root@localhost ~]# cd php-7.1.10/[root@localhost php-7.1.10]# ./configure --help[root@localhost php-7.1.10]# yum -y install libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel mysql pcre-devel[root@localhost php-7.1.10]# ./configure --prefix=/usr/local/php \ --with-curl \ --with-freetype-dir \ --with-gd \ --with-gettext \ --with-iconv-dir \ --with-kerberos \ --with-libdir=lib64 \ --with-libxml-dir \ --with-mysqli \ --with-openssl \ --with-pcre-regex \ --with-pdo-mysql \ --with-pdo-sqlite \ --with-pear \ --with-png-dir \ --with-xmlrpc \ --with-zlib \ --enable-fpm \ --enable-bcmath \ --enable-libxml \ --enable-inline-optimization \ --enable-gd-native-ttf \ --enable-mbregex \ --enable-mbstring \ --enable-pcntl \ --enable-shmop \ --enable-soap \ --enable-sockets \ --enable-sysvsem \ --enable-xml \ --enable-zip [root@localhost php-7.1.10]# make &amp; make install [root@localhost php-7.1.10]# cp php.ini-development /usr/local/php/lib/php.ini[root@localhost php-7.1.10]# cp /usr/local/php/etc/php-fpm.conf.default /usr/local/php/etc/php-fpm.conf[root@localhost php-7.1.10]# cp /usr/local/php/etc/php-fpm.d/www.conf.default /usr/local/php/etc/php-fpm.d/www.conf[root@localhost php-7.1.10]# cp -R ./sapi/fpm/php-fpm /etc/init.d/php-fpm[root@localhost php-7.1.10]# /etc/init.d/php-fpm 修改php进程归属用户(/usr/local/php/etc/php-fpm.d/www.conf), 修改user和group值.笔者示例中值均为 elonsu. 123456789[elonsu@localhost ~]$ grep -C 3 'elonsu' /usr/local/php/etc/php-fpm.d/www.conf -n20-; Unix user/group of processes21-; Note: The user is mandatory. If the group is not set, the default user's group22-; will be used.23:user = elonsu24:group = elonsu25-26-; The address on which to accept FastCGI requests.27-; Valid syntaxes are: 配置php环境变量在/etc/profile文件末尾追加环境变量,如下: 1234567891011# 这是PHP环境变量PHP_HOME=/usr/local/php# 设置Meven环境变量M2_HOME=/usr/local/apache-maven-3.5.0# 设置Java环境变量JAVA_HOME=/usr/local/jdk1.8.0_121JRE_HOME=/usr/local/jdk1.8.0_121/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$M2_HOME/bin:$PHP_HOME/bin:export PHP_HOME M2_HOME JAVA_HOME JRE_HOME CLASS_PATH PATH 使配置生效 12345[root@localhost ~]$ source /etc/profile[elonsu@localhost ~]$ php -vPHP 7.1.10 (cli) (built: Oct 22 2017 14:23:52) ( NTS )Copyright (c) 1997-2017 The PHP GroupZend Engine v3.1.0, Copyright (c) 1998-2017 Zend Technologies 下载WalleGithub: https://github.com/meolu/walle-web 12345elonsu@localhost cloud]$ pwd/export/cloudelonsu@localhost cloud]$ git clone git@github.com:meolu/walle-web.gitelonsu@localhost cloud]$ cd walle-webelonsu@localhost cloud]$ vi config/local.php 设置数据库链接和用户注册邮件通知服务配置 安装Walle12345[elonsu@localhost walle-web]$ curl -sS https://getcomposer.org/installer | php[elonsu@localhost walle-web]$ php composer.phar require guzzlehttp/promises[elonsu@localhost walle-web]$ php composer.phar require guzzlehttp/guzzle[elonsu@localhost walle-web]$ php composer.phar install --prefer-dist --no-dev --optimize-autoloader -vvvv[elonsu@localhost walle-web]$ ./yii walle/setup Nginx反向代理12345678910111213141516171819202122[elonsu@localhost walle-web]$ cat /usr/local/nginx/conf/domains/walle.wuyu.com server &#123; listen 80; server_name walle.wuyu.com; root /export/cloud/walle-web/web; index index.php; # 建议放内网 # allow 192.168.0.0/24; # deny all; location / &#123; try_files $uri $uri/ /index.php$is_args$args; &#125; location ~ \.php$ &#123; try_files $uri = 404; fastcgi_pass 127.0.0.1:9000; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125;&#125; 访问登录访问: http://walle.wuyu.com/首次登录默认管理员用户admin/admin. 配置项目发布系统部署完后登录后效果图如下(苏若年账户为笔者自己创建): 添加项目配置【项目配置】- 【新建项目】: 上图为个人技术博客发布项配置. 检测项目配置【项目配置】-【配置列表】-【检测】, 检测配置项是否配置成功. 创建上线任务【我的上线单】-【创建上线单】-【选择上线的模板】, 填写如下项: 上线单标题 选取分之 版本选取 全量/增量 填写完之后会在【我的上线单】, 中出现刚才添加的上线单项目. 如下图: 执行上线操作【我的上线单】-【选择上线单项】-【上线】-【部署】: 远程GIT仓库授权配置远程私有仓库ssh 授权123456789101112131415161718192021222324[elonsu@localhost ~]# ssh-keygen -t rsa -C "dennisit@163.com"Generating public/private rsa key pair.Enter file in which to save the key (/elonsu/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /elonsu/.ssh/id_rsa.Your public key has been saved in /elonsu/.ssh/id_rsa.pub.The key fingerprint is:8b:8a:a0:b3:a5:a1:5f:96:8a:3c:5a:c1:e0:b3:f4:f3 dennisit@163.comThe key's randomart image is:+--[ RSA 2048]----+| || ||. ||.o || +o S ||. +. . . . ||o.oo+ . . ||=O =o. ||O== .E |+-----------------+[elonsu@localhost ~]# ls [elonsu@localhost ~]$ ls ~/.ssh/id_rsa id_rsa.pub 这里的id_rsa为私钥, id_rsa_pub为公钥 Git服务器配置SSH公钥以开源中国为例, 【个人】-【设置】-【SSH公钥】 开源中国官方配置说明:http://git.mydoc.io/?t=154712 检测配置12345[elonsu@localhost ~]$ ssh -T git@gitee.com...Welcome to Gitee.com, -苏若年-! 部署目标机器授权我们以root用户身份, 将项目发布到192.178.0.107服务上时,需要给目标机器授权.如下: 1[root@localhost ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.0.107 此时会提示让输入目标服务器密码. 正确输入密码后,配置成功.]]></content>
      <categories>
        <category>运维部署</category>
      </categories>
      <tags>
        <tag>发布系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic实战Rest统计]]></title>
    <url>%2F2017%2F09%2F03%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FElastic%2FElastic%E5%AE%9E%E6%88%98Rest%E7%BB%9F%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[索引信息查看索引信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191[root@localhost elasticsearch-6.0.0 ]$ curl -XGET "http://127.0.0.1:9200/db_test?pretty=true"&#123; "db_test" : &#123; "aliases" : &#123; &#125;, "mappings" : &#123; "tb_opus" : &#123; "properties" : &#123; "aliasName" : &#123; "type" : "text", "fields" : &#123; "origin" : &#123; "type" : "keyword" &#125;, "pinyin" : &#123; "type" : "text", "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125;, "suggest" : &#123; "type" : "completion", "analyzer" : "ik_max_word", "preserve_separators" : true, "preserve_position_increments" : true, "max_input_length" : 50 &#125; &#125;, "analyzer" : "ik_max_word" &#125;, "appId" : &#123; "type" : "long" &#125;, "author" : &#123; "type" : "text", "fields" : &#123; "origin" : &#123; "type" : "keyword" &#125;, "pinyin" : &#123; "type" : "text", "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125;, "suggest" : &#123; "type" : "completion", "analyzer" : "ik_max_word", "preserve_separators" : true, "preserve_position_increments" : true, "max_input_length" : 50 &#125; &#125;, "analyzer" : "ik_smart" &#125;, "createTime" : &#123; "type" : "date" &#125;, "enabled" : &#123; "type" : "integer" &#125;, "favoriteTotal" : &#123; "type" : "long" &#125;, "fingerprint" : &#123; "type" : "keyword" &#125;, "id" : &#123; "type" : "long" &#125;, "images" : &#123; "type" : "keyword" &#125;, "introduce" : &#123; "type" : "text", "fields" : &#123; "pinyin" : &#123; "type" : "text", "boost" : 10.0, "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125; &#125;, "analyzer" : "ik_smart" &#125;, "issueTime" : &#123; "type" : "long" &#125;, "opusName" : &#123; "type" : "text", "fields" : &#123; "origin" : &#123; "type" : "keyword" &#125;, "pinyin" : &#123; "type" : "text", "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125;, "suggest" : &#123; "type" : "completion", "analyzer" : "ik_max_word", "preserve_separators" : true, "preserve_position_increments" : true, "max_input_length" : 50 &#125; &#125;, "analyzer" : "ik_max_word" &#125;, "score" : &#123; "type" : "double" &#125;, "sort" : &#123; "type" : "long" &#125;, "tags" : &#123; "type" : "text", "fields" : &#123; "origin" : &#123; "type" : "keyword" &#125;, "pinyin" : &#123; "type" : "text", "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125;, "suggest" : &#123; "type" : "completion", "analyzer" : "ik_max_word", "preserve_separators" : true, "preserve_position_increments" : true, "max_input_length" : 50 &#125; &#125;, "analyzer" : "ik_max_word" &#125;, "type" : &#123; "type" : "integer" &#125;, "updateTime" : &#123; "type" : "date" &#125;, "userId" : &#123; "type" : "long" &#125;, "uuid" : &#123; "type" : "keyword" &#125;, "viewTotal" : &#123; "type" : "long" &#125; &#125; &#125; &#125;, "settings" : &#123; "index" : &#123; "number_of_shards" : "1", "provided_name" : "db_test", "creation_date" : "1514353694378", "analysis" : &#123; "filter" : &#123; "mix_pinyin" : &#123; "lowercase" : "true", "padding_char" : " ", "first_letter" : "prefix", "keep_original" : "true", "remove_duplicated_term" : "true", "type" : "pinyin", "keep_full_pinyin" : "true" &#125; &#125;, "analyzer" : &#123; "ik_pinyin_analyzer" : &#123; "filter" : [ "mix_pinyin", "word_delimiter" ], "type" : "custom", "tokenizer" : "ik_max_word" &#125;, "default" : &#123; "tokenizer" : "ik_max_word" &#125; &#125; &#125;, "number_of_replicas" : "1", "uuid" : "seg7yr55R121l8tRW3r7uA", "version" : &#123; "created" : "6000099" &#125; &#125; &#125; &#125;&#125; 统计应用分类统计查询查询关键词为的文档, 并按照type分组(group by) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match": &#123; "introduce": "青春" &#125; &#125;, "size": 0, "aggs": &#123; "type_count": &#123; "terms": &#123; "field": "type" &#125; &#125; &#125;&#125;'&#123; "took" : 1, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 84, "max_score" : 0.0, "hits" : [ ] &#125;, "aggregations" : &#123; "type_count" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ &#123; "key" : 30, "doc_count" : 76 &#125;, &#123; "key" : 10, "doc_count" : 4 &#125;, &#123; "key" : 20, "doc_count" : 4 &#125; ] &#125; &#125;&#125; 基础函数统计查询查询评分最高评分和最低评分的文档(max|min|avg|sum) 12345678910111213141516171819202122232425262728293031323334353637383940[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "size": 0, "aggs": &#123; "max_score": &#123; "max": &#123; "field": "score" &#125; &#125;, "min_score": &#123; "min": &#123; "field": "score" &#125; &#125; &#125;&#125;'&#123; "took" : 3, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 650, "max_score" : 0.0, "hits" : [ ] &#125;, "aggregations" : &#123; "max_score" : &#123; "value" : 99.0 &#125;, "min_score" : &#123; "value" : 0.0 &#125; &#125;&#125; 统计信息查询统计评分Top5的文档数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_all": &#123; &#125; &#125;, "size": 0, "aggs": &#123; "score": &#123; "terms": &#123; "field": "score", "size": 5 &#125; &#125; &#125;&#125;'&#123; "took" : 3, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 651, "max_score" : 0.0, "hits" : [ ] &#125;, "aggregations" : &#123; "score" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 590, "buckets" : [ &#123; "key" : 38.0, "doc_count" : 13 &#125;, &#123; "key" : 85.0, "doc_count" : 13 &#125;, &#123; "key" : 15.0, "doc_count" : 12 &#125;, &#123; "key" : 30.0, "doc_count" : 11 &#125;, &#123; "key" : 92.0, "doc_count" : 11 &#125; ] &#125; &#125;&#125; 按照给定维度数据进行统计 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "terms": &#123; "type": ["10", "20"] &#125; &#125;, "size": 0, "aggs": &#123; "significant_type": &#123; "significant_terms": &#123; "field": "type" &#125; &#125; &#125;&#125;'&#123; "took" : 13, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 141, "max_score" : 0.0, "hits" : [ ] &#125;, "aggregations" : &#123; "significant_type" : &#123; "doc_count" : 141, "bg_count" : 675, "buckets" : [ &#123; "key" : 20, "doc_count" : 91, "score" : 2.4442432473215634, "bg_count" : 91 &#125;, &#123; "key" : 10, "doc_count" : 50, "score" : 1.3429907952316285, "bg_count" : 50 &#125; ] &#125; &#125;&#125; 查询评分的基本统计信息(count|min|max|avg|sum) 123456789101112131415161718192021222324252627282930313233343536[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "size": 0, "aggs": &#123; "score_stats": &#123; "stats": &#123; "field": "score" &#125; &#125; &#125;&#125;'&#123; "took" : 3, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 650, "max_score" : 0.0, "hits" : [ ] &#125;, "aggregations" : &#123; "score_stats" : &#123; "count" : 650, "min" : 0.0, "max" : 99.0, "avg" : 48.643076923076926, "sum" : 31618.0 &#125; &#125;&#125; 查询评分的高级统计信息 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "size": 0, "aggs": &#123; "score_stats": &#123; "extended_stats": &#123; "field": "score" &#125; &#125; &#125;&#125;'&#123; "took" : 2, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 650, "max_score" : 0.0, "hits" : [ ] &#125;, "aggregations" : &#123; "score_stats" : &#123; "count" : 650, "min" : 0.0, "max" : 99.0, "avg" : 48.643076923076926, "sum" : 31618.0, "sum_of_squares" : 2069492.0, "variance" : 817.6849136094673, "std_deviation" : 28.59519039295712, "std_deviation_bounds" : &#123; "upper" : 105.83345770899118, "lower" : -8.547303862837317 &#125; &#125; &#125;&#125; 分段区间统计查询分段统计指定区间内的评分和类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "size": 0, "aggs": &#123; "score_range": &#123; "range": &#123; "field": "score", "ranges": [ &#123; "to": 50 &#125;, &#123; "from": 50, "to": 80 &#125;, &#123; "from": 80 &#125; ] &#125; &#125;, "type_range": &#123; "range": &#123; "field": "type", "ranges": [ &#123; "to": 30 &#125;, &#123; "from": 30, "to": 50 &#125;, &#123; "from": 50, "to": 80 &#125;, &#123; "from": 80 &#125; ] &#125; &#125; &#125;&#125;'&#123; "took" : 2, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 651, "max_score" : 0.0, "hits" : [ ] &#125;, "aggregations" : &#123; "type_range" : &#123; "buckets" : [ &#123; "key" : "*-30.0", "to" : 30.0, "doc_count" : 141 &#125;, &#123; "key" : "30.0-50.0", "from" : 30.0, "to" : 50.0, "doc_count" : 509 &#125;, &#123; "key" : "50.0-80.0", "from" : 50.0, "to" : 80.0, "doc_count" : 0 &#125;, &#123; "key" : "80.0-*", "from" : 80.0, "doc_count" : 0 &#125; ] &#125;, "score_range" : &#123; "buckets" : [ &#123; "key" : "*-50.0", "to" : 50.0, "doc_count" : 338 &#125;, &#123; "key" : "50.0-80.0", "from" : 50.0, "to" : 80.0, "doc_count" : 192 &#125;, &#123; "key" : "80.0-*", "from" : 80.0, "doc_count" : 120 &#125; ] &#125; &#125;&#125; 直方图统计, 按照指定维度和步长值统计 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_all": &#123; &#125; &#125;, "size": 0, "aggs": &#123; "score": &#123; "histogram": &#123; "field": "score", "interval": 10 &#125; &#125;, "type": &#123; "histogram": &#123; "field": "type", "interval": 5 &#125; &#125; &#125;&#125;'&#123; "took" : 5, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 651, "max_score" : 0.0, "hits" : [ ] &#125;, "aggregations" : &#123; "score" : &#123; "buckets" : [ &#123; "key" : 0.0, "doc_count" : 70 &#125;, &#123; "key" : 10.0, "doc_count" : 59 &#125;, &#123; "key" : 20.0, "doc_count" : 64 &#125;, &#123; "key" : 30.0, "doc_count" : 81 &#125;, &#123; "key" : 40.0, "doc_count" : 64 &#125;, &#123; "key" : 50.0, "doc_count" : 64 &#125;, &#123; "key" : 60.0, "doc_count" : 60 &#125;, &#123; "key" : 70.0, "doc_count" : 68 &#125;, &#123; "key" : 80.0, "doc_count" : 55 &#125;, &#123; "key" : 90.0, "doc_count" : 65 &#125; ] &#125;, "type" : &#123; "buckets" : [ &#123; "key" : 10.0, "doc_count" : 50 &#125;, &#123; "key" : 15.0, "doc_count" : 0 &#125;, &#123; "key" : 20.0, "doc_count" : 91 &#125;, &#123; "key" : 25.0, "doc_count" : 0 &#125;, &#123; "key" : 30.0, "doc_count" : 509 &#125; ] &#125; &#125;&#125; 过滤统计score=90,type=20的文档 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_all": &#123; &#125; &#125;, "size": 0, "aggs": &#123; "score80": &#123; "filter": &#123; "term": &#123;"score": 80&#125; &#125; &#125;, "type20": &#123; "filter": &#123; "term": &#123;"type": 20&#125; &#125; &#125; &#125;&#125;'&#123; "took" : 5, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 651, "max_score" : 0.0, "hits" : [ ] &#125;, "aggregations" : &#123; "type20" : &#123; "doc_count" : 91 &#125;, "score80" : &#123; "doc_count" : 3 &#125; &#125;&#125; 矩阵统计 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_all": &#123; &#125; &#125;, "size": 0, "aggs": &#123; "matrixstats": &#123; "matrix_stats": &#123; "fields": ["score", "type"] &#125; &#125; &#125;&#125;'&#123; "took" : 2, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 651, "max_score" : 0.0, "hits" : [ ] &#125;, "aggregations" : &#123; "matrixstats" : &#123; "doc_count" : 650, "fields" : [ &#123; "name" : "score", "count" : 650, "mean" : 48.6430769230769, "variance" : 818.9448287305912, "skewness" : 0.06430023639098066, "kurtosis" : 1.827011562407035, "covariance" : &#123; "score" : 818.9448287305912, "type" : -6.150574848879931 &#125;, "correlation" : &#123; "score" : 1.0, "type" : -0.03572662420282527 &#125; &#125;, &#123; "name" : "type", "count" : 650, "mean" : 27.061538461538458, "variance" : 36.19035202086057, "skewness" : -1.894319693928367, "kurtosis" : 5.30339872762004, "covariance" : &#123; "score" : -6.150574848879931, "type" : 36.19035202086057 &#125;, "correlation" : &#123; "score" : -0.03572662420282527, "type" : 1.0 &#125; &#125; ] &#125; &#125;&#125; 参考文档 http://blog.csdn.net/napoay/article/details/56279658(分类统计) http://blog.csdn.net/napoay/article/details/53910646(高亮)]]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Elastic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic实战Rest查询]]></title>
    <url>%2F2017%2F09%2F01%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FElastic%2FElastic%E5%AE%9E%E6%88%98Rest%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[索引信息查看索引信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191[root@localhost elasticsearch-6.0.0 ]$ curl -XGET "http://127.0.0.1:9200/db_test?pretty=true"&#123; "db_test" : &#123; "aliases" : &#123; &#125;, "mappings" : &#123; "tb_opus" : &#123; "properties" : &#123; "aliasName" : &#123; "type" : "text", "fields" : &#123; "origin" : &#123; "type" : "keyword" &#125;, "pinyin" : &#123; "type" : "text", "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125;, "suggest" : &#123; "type" : "completion", "analyzer" : "ik_max_word", "preserve_separators" : true, "preserve_position_increments" : true, "max_input_length" : 50 &#125; &#125;, "analyzer" : "ik_max_word" &#125;, "appId" : &#123; "type" : "long" &#125;, "author" : &#123; "type" : "text", "fields" : &#123; "origin" : &#123; "type" : "keyword" &#125;, "pinyin" : &#123; "type" : "text", "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125;, "suggest" : &#123; "type" : "completion", "analyzer" : "ik_max_word", "preserve_separators" : true, "preserve_position_increments" : true, "max_input_length" : 50 &#125; &#125;, "analyzer" : "ik_smart" &#125;, "createTime" : &#123; "type" : "date" &#125;, "enabled" : &#123; "type" : "integer" &#125;, "favoriteTotal" : &#123; "type" : "long" &#125;, "fingerprint" : &#123; "type" : "keyword" &#125;, "id" : &#123; "type" : "long" &#125;, "images" : &#123; "type" : "keyword" &#125;, "introduce" : &#123; "type" : "text", "fields" : &#123; "pinyin" : &#123; "type" : "text", "boost" : 10.0, "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125; &#125;, "analyzer" : "ik_smart" &#125;, "issueTime" : &#123; "type" : "long" &#125;, "opusName" : &#123; "type" : "text", "fields" : &#123; "origin" : &#123; "type" : "keyword" &#125;, "pinyin" : &#123; "type" : "text", "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125;, "suggest" : &#123; "type" : "completion", "analyzer" : "ik_max_word", "preserve_separators" : true, "preserve_position_increments" : true, "max_input_length" : 50 &#125; &#125;, "analyzer" : "ik_max_word" &#125;, "score" : &#123; "type" : "double" &#125;, "sort" : &#123; "type" : "long" &#125;, "tags" : &#123; "type" : "text", "fields" : &#123; "origin" : &#123; "type" : "keyword" &#125;, "pinyin" : &#123; "type" : "text", "term_vector" : "with_positions_offsets", "analyzer" : "ik_pinyin_analyzer" &#125;, "suggest" : &#123; "type" : "completion", "analyzer" : "ik_max_word", "preserve_separators" : true, "preserve_position_increments" : true, "max_input_length" : 50 &#125; &#125;, "analyzer" : "ik_max_word" &#125;, "type" : &#123; "type" : "integer" &#125;, "updateTime" : &#123; "type" : "date" &#125;, "userId" : &#123; "type" : "long" &#125;, "uuid" : &#123; "type" : "keyword" &#125;, "viewTotal" : &#123; "type" : "long" &#125; &#125; &#125; &#125;, "settings" : &#123; "index" : &#123; "number_of_shards" : "1", "provided_name" : "db_test", "creation_date" : "1514353694378", "analysis" : &#123; "filter" : &#123; "mix_pinyin" : &#123; "lowercase" : "true", "padding_char" : " ", "first_letter" : "prefix", "keep_original" : "true", "remove_duplicated_term" : "true", "type" : "pinyin", "keep_full_pinyin" : "true" &#125; &#125;, "analyzer" : &#123; "ik_pinyin_analyzer" : &#123; "filter" : [ "mix_pinyin", "word_delimiter" ], "type" : "custom", "tokenizer" : "ik_max_word" &#125;, "default" : &#123; "tokenizer" : "ik_max_word" &#125; &#125; &#125;, "number_of_replicas" : "1", "uuid" : "seg7yr55R121l8tRW3r7uA", "version" : &#123; "created" : "6000099" &#125; &#125; &#125; &#125;&#125; 搜索示例多主键查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "_source": &#123; "include": ["id", "opusName"] &#125;, "query": &#123; "ids": &#123; "type": "tb_opus", "values": ["1", "2", "3"] &#125; &#125; &#125;&#125;'&#123; "took" : 7, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 3, "max_score" : 1.0, "hits" : [ &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "1", "_score" : 1.0, "_source" : &#123; "opusName" : "秒速5厘米", "id" : 1 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "2", "_score" : 1.0, "_source" : &#123; "opusName" : "致我们终将逝去的青春", "id" : 2 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "3", "_score" : 1.0, "_source" : &#123; "opusName" : "对不起，我爱你", "id" : 3 &#125; &#125; ] &#125;&#125; term查询未分词的指定列1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "_source": &#123; "include": ["id", "author", "opusName"] &#125;, "query": &#123; "term": &#123; "author.origin": ["八月长安"] &#125; &#125;&#125;'&#123; "took" : 2, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 5, "max_score" : 4.801036, "hits" : [ &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "558", "_score" : 4.801036, "_source" : &#123; "author" : "八月长安", "opusName" : "你好,旧时光：陪你到青春最后", "id" : 558 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "661", "_score" : 4.801036, "_source" : &#123; "author" : "八月长安", "opusName" : "最好的我们", "id" : 661 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "662", "_score" : 4.801036, "_source" : &#123; "author" : "八月长安", "opusName" : "橘生淮南·暗恋", "id" : 662 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "663", "_score" : 4.801036, "_source" : &#123; "author" : "八月长安", "opusName" : "暗恋", "id" : 663 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "664", "_score" : 4.801036, "_source" : &#123; "author" : "八月长安", "opusName" : "被偷走的那五年", "id" : 664 &#125; &#125; ] &#125;&#125; terms查询多个未分词的指定列123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "_source": &#123; "include": ["id", "author", "opusName"] &#125;, "query": &#123; "terms": &#123; "author.origin": ["八月长安", "籽月"] &#125; &#125;&#125;'&#123; "took" : 8, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 8, "max_score" : 1.0, "hits" : [ &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "558", "_score" : 1.0, "_source" : &#123; "author" : "八月长安", "opusName" : "你好,旧时光：陪你到青春最后", "id" : 558 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "661", "_score" : 1.0, "_source" : &#123; "author" : "八月长安", "opusName" : "最好的我们", "id" : 661 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "662", "_score" : 1.0, "_source" : &#123; "author" : "八月长安", "opusName" : "橘生淮南·暗恋", "id" : 662 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "663", "_score" : 1.0, "_source" : &#123; "author" : "八月长安", "opusName" : "暗恋", "id" : 663 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "664", "_score" : 1.0, "_source" : &#123; "author" : "八月长安", "opusName" : "被偷走的那五年", "id" : 664 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "668", "_score" : 1.0, "_source" : &#123; "author" : "籽月", "opusName" : "夏有乔木：雅望天堂", "id" : 668 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "669", "_score" : 1.0, "_source" : &#123; "author" : "籽月", "opusName" : "初晨，是我故意忘记你", "id" : 669 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "670", "_score" : 1.0, "_source" : &#123; "author" : "籽月", "opusName" : "月光满满预见你", "id" : 670 &#125; &#125; ] &#125;&#125; 前缀查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "_source": &#123; "include": ["id", "author", "opusName"] &#125;, "query": &#123; "prefix": &#123; "author.origin": "九" &#125; &#125;&#125;'&#123; "took" : 8, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 13, "max_score" : 1.0, "hits" : [ &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "548", "_score" : 1.0, "_source" : &#123; "author" : "九夜茴", "opusName" : "曾少年", "id" : 548 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "549", "_score" : 1.0, "_source" : &#123; "author" : "九夜茴", "opusName" : "花开半夏", "id" : 549 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "550", "_score" : 1.0, "_source" : &#123; "author" : "九夜茴", "opusName" : "匆匆那年", "id" : 550 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "551", "_score" : 1.0, "_source" : &#123; "author" : "九夜茴,林特特,韩梅梅等", "opusName" : "世界那么大，我想去看看", "id" : 551 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "8", "_score" : 1.0, "_source" : &#123; "author" : "九把刀", "opusName" : "那些年我们一起追的女孩", "id" : 8 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "765", "_score" : 1.0, "_source" : &#123; "author" : "九鹭非香", "opusName" : "百界歌", "id" : 765 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "761", "_score" : 1.0, "_source" : &#123; "author" : "九鹭非香", "opusName" : "苍兰诀", "id" : 761 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "762", "_score" : 1.0, "_source" : &#123; "author" : "九鹭非香", "opusName" : "司命", "id" : 762 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "763", "_score" : 1.0, "_source" : &#123; "author" : "九鹭非香", "opusName" : "你在遥远星空中", "id" : 763 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "764", "_score" : 1.0, "_source" : &#123; "author" : "九鹭非香", "opusName" : "与凤行", "id" : 764 &#125; &#125; ] &#125;&#125; 通配符查询(wildcard) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "_source": &#123; "include": ["id", "author", "opusName"] &#125;, "query": &#123; "wildcard": &#123; "author.origin": "*月*" &#125; &#125;&#125;'&#123; "took" : 11, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 24, "max_score" : 1.0, "hits" : [ &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "558", "_score" : 1.0, "_source" : &#123; "author" : "八月长安", "opusName" : "你好,旧时光：陪你到青春最后", "id" : 558 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "613", "_score" : 1.0, "_source" : &#123; "author" : "四月莲", "opusName" : "玻璃青春", "id" : 613 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "661", "_score" : 1.0, "_source" : &#123; "author" : "八月长安", "opusName" : "最好的我们", "id" : 661 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "161", "_score" : 1.0, "_source" : &#123; "author" : "沧月", "opusName" : "忘川", "id" : 161 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "610", "_score" : 1.0, "_source" : &#123; "author" : "菲雨月", "opusName" : "青春，终究被搁浅", "id" : 610 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "662", "_score" : 1.0, "_source" : &#123; "author" : "八月长安", "opusName" : "橘生淮南·暗恋", "id" : 662 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "663", "_score" : 1.0, "_source" : &#123; "author" : "八月长安", "opusName" : "暗恋", "id" : 663 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "664", "_score" : 1.0, "_source" : &#123; "author" : "八月长安", "opusName" : "被偷走的那五年", "id" : 664 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "668", "_score" : 1.0, "_source" : &#123; "author" : "籽月", "opusName" : "夏有乔木：雅望天堂", "id" : 668 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "669", "_score" : 1.0, "_source" : &#123; "author" : "籽月", "opusName" : "初晨，是我故意忘记你", "id" : 669 &#125; &#125; ] &#125;&#125; 正则查询(Regex) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "_source": &#123; "include": ["id", "author", "opusName"] &#125;, "query": &#123; "regexp": &#123; "author.origin": "[沧|籽]月" &#125; &#125;&#125;'&#123; "took" : 3, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 5, "max_score" : 1.0, "hits" : [ &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "161", "_score" : 1.0, "_source" : &#123; "author" : "沧月", "opusName" : "忘川", "id" : 161 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "668", "_score" : 1.0, "_source" : &#123; "author" : "籽月", "opusName" : "夏有乔木：雅望天堂", "id" : 668 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "669", "_score" : 1.0, "_source" : &#123; "author" : "籽月", "opusName" : "初晨，是我故意忘记你", "id" : 669 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "670", "_score" : 1.0, "_source" : &#123; "author" : "籽月", "opusName" : "月光满满预见你", "id" : 670 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "452", "_score" : 1.0, "_source" : &#123; "author" : "沧月", "opusName" : "羽·苍穹之烬", "id" : 452 &#125; &#125; ] &#125;&#125; span查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "_source": &#123; "include": ["id", "author", "opusName"] &#125;, "query": &#123; "span_or": &#123; "clauses":[ &#123;"span_term" : &#123;"author": "长安"&#125;&#125;, &#123;"span_term" : &#123;"author": "笛"&#125;&#125; ] &#125; &#125;&#125;'&#123; "took" : 4, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 17, "max_score" : 9.539912, "hits" : [ &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "558", "_score" : 9.539912, "_source" : &#123; "author" : "八月长安", "opusName" : "你好,旧时光：陪你到青春最后", "id" : 558 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "661", "_score" : 9.539912, "_source" : &#123; "author" : "八月长安", "opusName" : "最好的我们", "id" : 661 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "662", "_score" : 9.539912, "_source" : &#123; "author" : "八月长安", "opusName" : "橘生淮南·暗恋", "id" : 662 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "663", "_score" : 9.539912, "_source" : &#123; "author" : "八月长安", "opusName" : "暗恋", "id" : 663 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "664", "_score" : 9.539912, "_source" : &#123; "author" : "八月长安", "opusName" : "被偷走的那五年", "id" : 664 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "443", "_score" : 9.539912, "_source" : &#123; "author" : "笛安", "opusName" : "西决", "id" : 443 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "444", "_score" : 9.539912, "_source" : &#123; "author" : "笛安", "opusName" : "姐姐的丛林", "id" : 444 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "445", "_score" : 9.539912, "_source" : &#123; "author" : "笛安", "opusName" : "一个女人的悲欢离合：东霓", "id" : 445 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "446", "_score" : 9.539912, "_source" : &#123; "author" : "笛安", "opusName" : "南方有令秧", "id" : 446 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "447", "_score" : 9.539912, "_source" : &#123; "author" : "笛安", "opusName" : "妩媚航班", "id" : 447 &#125; &#125; ] &#125;&#125; 匹配查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "_source": &#123; "include": ["id", "tags"] &#125;, "query": &#123; "match": &#123; "tags": &#123; "query": "青春 爱情", "operator": "AND" &#125; &#125; &#125;&#125;'&#123; "took" : 3, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 6, "max_score" : 6.3930693, "hits" : [ &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "2", "_score" : 6.3930693, "_source" : &#123; "id" : 2, "tags" : [ "爱情", "青春" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "8", "_score" : 6.3930693, "_source" : &#123; "id" : 8, "tags" : [ "爱情", "青春" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "24", "_score" : 6.3930693, "_source" : &#123; "id" : 24, "tags" : [ "爱情", "青春" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "114", "_score" : 6.3930693, "_source" : &#123; "id" : 114, "tags" : [ "青春", "爱情" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "11", "_score" : 5.4129186, "_source" : &#123; "id" : 11, "tags" : [ "动画", "青春", "爱情" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "1", "_score" : 4.142658, "_source" : &#123; "id" : 1, "tags" : [ "新海诚", "动画", "爱情", "青春" ] &#125; &#125; ] &#125;&#125;[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "_source": &#123; "include": ["id", "tags"] &#125;, "query": &#123; "match": &#123; "tags": &#123; "query": "青春 爱情", "operator": "OR" &#125; &#125; &#125;&#125;'&#123; "took" : 3, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 65, "max_score" : 6.3930693, "hits" : [ &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "2", "_score" : 6.3930693, "_source" : &#123; "id" : 2, "tags" : [ "爱情", "青春" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "8", "_score" : 6.3930693, "_source" : &#123; "id" : 8, "tags" : [ "爱情", "青春" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "24", "_score" : 6.3930693, "_source" : &#123; "id" : 24, "tags" : [ "爱情", "青春" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "114", "_score" : 6.3930693, "_source" : &#123; "id" : 114, "tags" : [ "青春", "爱情" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "11", "_score" : 5.4129186, "_source" : &#123; "id" : 11, "tags" : [ "动画", "青春", "爱情" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "1", "_score" : 4.142658, "_source" : &#123; "id" : 1, "tags" : [ "新海诚", "动画", "爱情", "青春" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "409", "_score" : 3.6897516, "_source" : &#123; "id" : 409, "tags" : [ "励志", "成长", "青春" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "522", "_score" : 2.4852023, "_source" : &#123; "id" : 522, "tags" : [ "爱情" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "12", "_score" : 2.4852023, "_source" : &#123; "id" : 12, "tags" : [ "爱情" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "13", "_score" : 2.4852023, "_source" : &#123; "id" : 13, "tags" : [ "爱情" ] &#125; &#125; ] &#125;&#125; 匹配查询多个field 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "_source": &#123; "include": ["id", "tags", "opusName"] &#125;, "query": &#123; "multi_match": &#123; "fields": ["tags", "opusName"], "query": "青春 爱情", "operator": "AND" &#125; &#125;&#125;'&#123; "took" : 4, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 7, "max_score" : 6.3930693, "hits" : [ &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "2", "_score" : 6.3930693, "_source" : &#123; "opusName" : "致我们终将逝去的青春", "id" : 2, "tags" : [ "爱情", "青春" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "8", "_score" : 6.3930693, "_source" : &#123; "opusName" : "那些年我们一起追的女孩", "id" : 8, "tags" : [ "爱情", "青春" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "24", "_score" : 6.3930693, "_source" : &#123; "opusName" : "带我去远方", "id" : 24, "tags" : [ "爱情", "青春" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "114", "_score" : 6.3930693, "_source" : &#123; "opusName" : "匆匆那年", "id" : 114, "tags" : [ "青春", "爱情" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "11", "_score" : 5.4129186, "_source" : &#123; "opusName" : "言叶之庭", "id" : 11, "tags" : [ "动画", "青春", "爱情" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "568", "_score" : 5.225136, "_source" : &#123; "opusName" : "致青春：我们曾经向往的爱情", "id" : 568, "tags" : [ "纯爱" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "1", "_score" : 4.142658, "_source" : &#123; "opusName" : "秒速5厘米", "id" : 1, "tags" : [ "新海诚", "动画", "爱情", "青春" ] &#125; &#125; ] &#125;&#125; 前缀匹配查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "_source": &#123; "include": ["id", "opusName"] &#125;, "query": &#123; "match_phrase_prefix": &#123; "opusName": "如果" &#125; &#125;&#125;'&#123; "took" : 6, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 5, "max_score" : 5.3439174, "hits" : [ &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "647", "_score" : 5.3439174, "_source" : &#123; "opusName" : "如果还有爱", "id" : 647 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "626", "_score" : 4.80206, "_source" : &#123; "opusName" : "如果蜗牛有爱情", "id" : 626 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "683", "_score" : 4.80206, "_source" : &#123; "opusName" : "如果巴黎不快乐", "id" : 683 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "645", "_score" : 4.3599715, "_source" : &#123; "opusName" : "如果爱犯了错", "id" : 645 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "547", "_score" : 2.3843746, "_source" : &#123; "opusName" : "如果你曾奋不顾身爱上一个人", "id" : 547 &#125; &#125; ] &#125;&#125; query_string查询 查询关键词同时包含”蜗牛”和”爱情” 并且评分不小于80 分类号为30的文档 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "_source": &#123; "include": ["id", "opusName", "tags", "score", "type"] &#125;, "query": &#123; "query_string": &#123; "query": "蜗牛 爱情 -score:&#123;* TO 80&#125; type:30", "fields":[ "opusName^5", "tags" ], "default_operator": "AND" &#125; &#125;&#125;'&#123; "took" : 3, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 1, "max_score" : 52.146034, "hits" : [ &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "626", "_score" : 52.146034, "_source" : &#123; "score" : 91, "opusName" : "如果蜗牛有爱情", "id" : 626, "type" : 30, "tags" : [ "都市", "丁墨" ] &#125; &#125; ] &#125;&#125; 区间查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "_source": &#123; "include": ["id", "score"] &#125;, "from": 0, "size": 5, "query": &#123; "range": &#123; "score": &#123; "from" : 13, "to": 15, "include_lower": true, "include_upper": false &#125; &#125; &#125;&#125;'&#123; "took" : 2, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 11, "max_score" : 1.0, "hits" : [ &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "630", "_score" : 1.0, "_source" : &#123; "score" : 13, "id" : 630 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "647", "_score" : 1.0, "_source" : &#123; "score" : 14, "id" : 647 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "659", "_score" : 1.0, "_source" : &#123; "score" : 13, "id" : 659 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "529", "_score" : 1.0, "_source" : &#123; "score" : 14, "id" : 529 &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "697", "_score" : 1.0, "_source" : &#123; "score" : 14, "id" : 697 &#125; &#125; ] &#125;&#125;``` bool查询``` bash[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "_source": &#123; "include": ["id", "opusName", "tags", "score"] &#125;, "query": &#123; "bool": &#123; "must": [&#123; "term" : &#123;"opusName":"爱情"&#125; &#125;], "must_not": [&#123; "range": &#123; "score": &#123; "from": 80, "to": 100&#125; &#125; &#125;], "should": [&#123; "term" : &#123;"tags":"青春"&#125; &#125;] &#125; &#125;&#125;'&#123; "took" : 4, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 5, "max_score" : 5.156911, "hits" : [ &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "468", "_score" : 5.156911, "_source" : &#123; "score" : 55, "opusName" : "爱情的开关", "id" : 468, "tags" : [ "爱情", "匪我思存" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "504", "_score" : 5.156911, "_source" : &#123; "score" : 39, "opusName" : "因为爱情", "id" : 504, "tags" : [ "家庭" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "275", "_score" : 3.8527093, "_source" : &#123; "score" : 54, "opusName" : "你的味蕾，我的爱情", "id" : 275, "tags" : [ ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "568", "_score" : 3.5531719, "_source" : &#123; "score" : 77, "opusName" : "致青春：我们曾经向往的爱情", "id" : 568, "tags" : [ "纯爱" ] &#125; &#125;, &#123; "_index" : "db_test", "_type" : "tb_opus", "_id" : "688", "_score" : 3.5531719, "_source" : &#123; "score" : 69, "opusName" : "蜗婚：距离爱情一平米", "id" : 688, "tags" : [ "婚恋", "白槿湖" ] &#125; &#125; ] &#125;&#125; 分页排序查询分页查询所有数据,结果以更新时间降序、主键编号升序排列 123456789101112131415161718[root@icloud-store ~]# curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "match_all": &#123; &#125; &#125;, "from": 0, "size": 3, "sort": &#123; "updateTime": &#123; "order": "desc" &#125;, "id": &#123; "order": "asc" &#125; &#125;&#125;' 关键词筛选查询查询与宫崎骏相关的作品 1234567891011121314151617181920212223242526272829[root@icloud-store ~]# curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "query": &#123; "query_string": &#123; "query": "宫崎骏" &#125; &#125;, "highlight": &#123; "pre_tags": [ "&lt;b&gt;" ], "post_tags": [ "&lt;/b&gt;" ], "fields": &#123; "opusName": &#123; "order": "score" &#125;, "tags": &#123; "order": "score" &#125;, "introduce": &#123; "order": "score" &#125; &#125; &#125;, "from": 0, "size": 3&#125;' 搜索推荐查询根据关键词进行搜索推荐(前缀匹配) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@localhost elasticsearch-6.0.0 ]$ curl -XPOST "http://127.0.0.1:9200/db_test/tb_opus/_search?pretty=true" -H 'Content-Type: application/json' -d'&#123; "_source": "opusName.suggest", "suggest": &#123; "product-suggest" : &#123; "text" : "阿狸", "completion" : &#123; "field" : "opusName.suggest", "fuzzy" : &#123; "fuzziness" : 2 &#125;, "size": 5 &#125; &#125; &#125;&#125;'&#123; "took" : 8, "timed_out" : false, "_shards" : &#123; "total" : 1, "successful" : 1, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 0, "max_score" : 0.0, "hits" : [ ] &#125;, "suggest" : &#123; "product-suggest" : [ &#123; "text" : "阿狸", "offset" : 0, "length" : 2, "options" : [ &#123; "text" : "阿狸·梦之城堡", "_index" : "db_test", "_type" : "tb_opus", "_id" : "346", "_score" : 5.0, "_source" : &#123; &#125; &#125;, &#123; "text" : "阿狸·永远站", "_index" : "db_test", "_type" : "tb_opus", "_id" : "353", "_score" : 5.0, "_source" : &#123; &#125; &#125; ] &#125; ] &#125;&#125;]]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Elastic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot项目配置加密]]></title>
    <url>%2F2017%2F07%2F15%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2FSpringBoot%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E5%8A%A0%E5%AF%86%2F</url>
    <content type="text"><![CDATA[SpringBoot配置默认情况下是明文显示，安全性就比较低一些。jasypt该插件可以对配置文件进行加密, 提高配置的安全性 插件引入12345&lt;dependency&gt; &lt;groupId&gt;com.github.ulisesbocchio&lt;/groupId&gt; &lt;artifactId&gt;jasypt-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt;&lt;/dependency&gt; 组件说明使用组件原生API进行加解密，说明: 每次编码生成的编码结果串都会不一样, 均可以逆向解析. 编解码示例123456789101112131415161718192021222324252627/** * 编码 * @param keyt 秘钥 * @param source 原始数据 * @return 这里每次加密后的密文结果都不一样 */public static String encrypt(String keyt, String source)&#123; StandardPBEStringEncryptor encryptor = new StandardPBEStringEncryptor(); encryptor.setPassword(keyt); // 默认算法为"PBEWithMD5AndDES" encryptor.setAlgorithm("PBEWithMD5AndDES"); return encryptor.encrypt(source);&#125;/** * 解码 * @param keyt 秘钥 * @param source ji * @return */public static String decrypt(String keyt, String source)&#123; StandardPBEStringEncryptor encryptor = new StandardPBEStringEncryptor(); encryptor.setPassword(keyt); // 默认算法为"PBEWithMD5AndDES" encryptor.setAlgorithm("PBEWithMD5AndDES"); return encryptor.decrypt(source);&#125; 测试编解码1234567public static void main(String[] args) &#123; // 秘钥 String kept = "123456A"; String source = "配置加解密"; String target = encrypt(kept, source); System.out.println("目标数据:" + source + " \n加密结果:" + target + " \n解密结果:" + decrypt(kept, target));&#125; 输出结果 123目标数据:配置加解密 加密结果:5eZ9Jf/7A75hdoVMhuqcufSFKE/GiVyp 解密结果:配置加解密 SpringBoot中应用引入插件依赖, 然后在配置文件中做如下简易配置. 密钥配置配置秘钥,该步必须配置,该密钥用来对配置文件进行加解密, 对应上述示例中的kept(密钥参数) 12# 加密密码jasypt.encryptor.password=1234ABCD 密文串配置jasypt是通过识别配置文件中的ENC()标签来进行密文解密的,所以我们需要将受保护的配置做如下配置修改. 1234# 使用ENC()标签配置密文串spring.datasource.url=jdbc:mysql://127.0.0.1:3306/ENC(9JXD/P35350dt+R3479d6uZQ==)?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=falsespring.datasource.username=ENC(t+xp3NnBBFmcAePD459==)spring.datasource.password=ENC(A0nQxK1oPaNnBBBFmcgoTI+FFnNnBB+sdEQmUg=)]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>Jasypt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里云25端口封禁解决]]></title>
    <url>%2F2017%2F07%2F15%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2F%E9%98%BF%E9%87%8C%E4%BA%9125%E7%AB%AF%E5%8F%A3%E5%B0%81%E7%A6%81%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[使用JavaMail发送邮件在阿里云服务器邮件发送失败 问题发现问题产生: SpringBoot使用JavaMail发送邮件，本地测试是可以通过的，但项目部署到阿里云服务器后就不行了 问题原因: 阿里云处于安全考虑，TCP25端口出方向默认被封禁. 问题解决对于阿里云线上服务器, 需要将邮箱的配置改为ssl加密465端口发送. application.properties配置文件中增加下面配置 旧版配置1234spring.mail.host=smtp.qq.comspring.mail.port=25spring.mail.username=邮箱账号spring.mail.password=邮箱密码 新版配置12345678910spring.mail.host=smtp.qq.comspring.mail.username=邮箱账号spring.mail.password=邮箱密码spring.mail.default-encoding=UTF-8spring.mail.properties.mail.smtp.auth=truespring.mail.properties.mail.smtp.timeout=25000spring.mail.properties.mail.smtp.ssl.enable=truespring.mail.properties.mail.smtp.socketFactory.class=javax.net.ssl.SSLSocketFactoryspring.mail.properties.mail.smtp.socketFactory.port=465spring.mail.properties.mail.smtp.port=465]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>阿里云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Akka调度器和远程调用]]></title>
    <url>%2F2017%2F02%2F01%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FAkka%2FAkka%E8%B0%83%E5%BA%A6%E5%99%A8%E5%92%8C%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[在没有为Actor作配置的情况下，每一个ActorSystem将有一个缺省的派发器。该缺省派发器可以被配置，默认是使用指定的default-executor的一个Dispatcher Akka Dispatcher应用配置文件application.conf 内容 123456789101112131415161718192021DispatcherExample&#123; defaultDispatcher &#123; type = Dispatcher executor = "fork-join-executor" fork-join-executor &#123; parallelism-min = 2 parallelism-factor = 2.0 parallelism-max = 6 &#125; &#125; defaultDispatcher1 &#123; type = Dispatcher executor = "thread-pool-executor" thread-pool-executor &#123; core-pool-size-min = 1 core-pool-size-factor = 2.0 core-pool-size-max = 6 &#125; throughput = 2 &#125;&#125; 事件处理Actor12345678910111213141516171819public class EchoActor extends AbstractLoggingActor&#123; private static AtomicInteger instance = new AtomicInteger(0); @Override public void preStart() &#123; log().info("实例 #" + instance.incrementAndGet() + ", Hashcode #" + this.hashCode()); &#125; @Override public Receive createReceive() &#123; return receiveBuilder() .match(String.class, x-&gt;&#123; log().info("[接收消息]:&#123;&#125;, Actor:&#123;&#125;, Thread:&#123;&#125;", x, self().path() , Thread.currentThread().getName()); &#125;) .build(); &#125;&#125; 主类测试示例主类示例代码 1234567891011121314151617public class DispatcherMain &#123; public static void main(String[] args) throws Exception &#123; ActorSystem system = ActorSystem.create("dispatcherSystem", ConfigFactory.load().getConfig("DispatcherExample")); Props props = Props.create(EchoActor.class).withDispatcher("defaultDispatcher").withRouter(new RoundRobinPool(3)); ActorRef actor = system.actorOf(props, "echoActor"); for (int i = 0; i &lt; 10; i++) &#123; actor.tell("消息:" + i, ActorRef.noSender()); &#125; Thread.sleep(1000); system.terminate(); &#125;&#125; 测试示例输出 12345678910111213[INFO] [01/29/2018 17:57:12.527] [dispatcherSystem-defaultDispatcher-7] [akka://dispatcherSystem/user/echoActor/$b] 实例 #3, Hashcode #2058102743[INFO] [01/29/2018 17:57:12.527] [dispatcherSystem-defaultDispatcher-6] [akka://dispatcherSystem/user/echoActor/$a] 实例 #2, Hashcode #1700945207[INFO] [01/29/2018 17:57:12.529] [dispatcherSystem-defaultDispatcher-8] [akka://dispatcherSystem/user/echoActor/$c] 实例 #1, Hashcode #1494635147[INFO] [01/29/2018 17:57:12.530] [dispatcherSystem-defaultDispatcher-6] [akka://dispatcherSystem/user/echoActor/$a] [接收消息]:消息:0, Actor:akka://dispatcherSystem/user/echoActor/$a, Thread:dispatcherSystem-defaultDispatcher-6[INFO] [01/29/2018 17:57:12.530] [dispatcherSystem-defaultDispatcher-7] [akka://dispatcherSystem/user/echoActor/$b] [接收消息]:消息:1, Actor:akka://dispatcherSystem/user/echoActor/$b, Thread:dispatcherSystem-defaultDispatcher-7[INFO] [01/29/2018 17:57:12.530] [dispatcherSystem-defaultDispatcher-8] [akka://dispatcherSystem/user/echoActor/$c] [接收消息]:消息:2, Actor:akka://dispatcherSystem/user/echoActor/$c, Thread:dispatcherSystem-defaultDispatcher-8[INFO] [01/29/2018 17:57:12.530] [dispatcherSystem-defaultDispatcher-6] [akka://dispatcherSystem/user/echoActor/$a] [接收消息]:消息:3, Actor:akka://dispatcherSystem/user/echoActor/$a, Thread:dispatcherSystem-defaultDispatcher-6[INFO] [01/29/2018 17:57:12.530] [dispatcherSystem-defaultDispatcher-7] [akka://dispatcherSystem/user/echoActor/$b] [接收消息]:消息:4, Actor:akka://dispatcherSystem/user/echoActor/$b, Thread:dispatcherSystem-defaultDispatcher-7[INFO] [01/29/2018 17:57:12.530] [dispatcherSystem-defaultDispatcher-8] [akka://dispatcherSystem/user/echoActor/$c] [接收消息]:消息:5, Actor:akka://dispatcherSystem/user/echoActor/$c, Thread:dispatcherSystem-defaultDispatcher-8[INFO] [01/29/2018 17:57:12.530] [dispatcherSystem-defaultDispatcher-6] [akka://dispatcherSystem/user/echoActor/$a] [接收消息]:消息:6, Actor:akka://dispatcherSystem/user/echoActor/$a, Thread:dispatcherSystem-defaultDispatcher-6[INFO] [01/29/2018 17:57:12.530] [dispatcherSystem-defaultDispatcher-7] [akka://dispatcherSystem/user/echoActor/$b] [接收消息]:消息:7, Actor:akka://dispatcherSystem/user/echoActor/$b, Thread:dispatcherSystem-defaultDispatcher-7[INFO] [01/29/2018 17:57:12.530] [dispatcherSystem-defaultDispatcher-8] [akka://dispatcherSystem/user/echoActor/$c] [接收消息]:消息:8, Actor:akka://dispatcherSystem/user/echoActor/$c, Thread:dispatcherSystem-defaultDispatcher-8[INFO] [01/29/2018 17:57:12.530] [dispatcherSystem-defaultDispatcher-6] [akka://dispatcherSystem/user/echoActor/$a] [接收消息]:消息:9, Actor:akka://dispatcherSystem/user/echoActor/$a, Thread:dispatcherSystem-defaultDispatcher-6 切换Dispatcher测试主类代码 1234567891011121314151617181920public class DispatcherMain &#123; public static void main(String[] args) throws Exception &#123; ActorSystem system = ActorSystem.create( "default-dispatcher", ConfigFactory.load().getConfig("DispatcherExample")); ActorRef actor = system.actorOf( Props.create(EchoActor.class).withDispatcher("defaultDispatcher1").withRouter(new RoundRobinPool(3)) , "echoActor"); for (int i = 0; i &lt; 10; i++) &#123; actor.tell("消息:" + i, ActorRef.noSender()); &#125; Thread.sleep(1000); system.terminate(); &#125;&#125; 执行输出 1234567891011121314[INFO] [01/29/2018 18:02:44.600] [dispatcherSystem-defaultDispatcher1-5] [akka://dispatcherSystem/user/echoActor/$a] 实例 #3, Hashcode #1590930020[INFO] [01/29/2018 18:02:44.600] [dispatcherSystem-defaultDispatcher1-6] [akka://dispatcherSystem/user/echoActor/$b] 实例 #1, Hashcode #938937247[INFO] [01/29/2018 18:02:44.603] [dispatcherSystem-defaultDispatcher1-7] [akka://dispatcherSystem/user/echoActor/$c] 实例 #2, Hashcode #904652545[INFO] [01/29/2018 18:02:44.603] [dispatcherSystem-defaultDispatcher1-5] [akka://dispatcherSystem/user/echoActor/$a] [接收消息]:消息:0, Actor:akka://dispatcherSystem/user/echoActor/$a, Thread:dispatcherSystem-defaultDispatcher1-5[INFO] [01/29/2018 18:02:44.603] [dispatcherSystem-defaultDispatcher1-6] [akka://dispatcherSystem/user/echoActor/$b] [接收消息]:消息:1, Actor:akka://dispatcherSystem/user/echoActor/$b, Thread:dispatcherSystem-defaultDispatcher1-6[INFO] [01/29/2018 18:02:44.603] [dispatcherSystem-defaultDispatcher1-7] [akka://dispatcherSystem/user/echoActor/$c] [接收消息]:消息:2, Actor:akka://dispatcherSystem/user/echoActor/$c, Thread:dispatcherSystem-defaultDispatcher1-7[INFO] [01/29/2018 18:02:44.603] [dispatcherSystem-defaultDispatcher1-5] [akka://dispatcherSystem/user/echoActor/$a] [接收消息]:消息:3, Actor:akka://dispatcherSystem/user/echoActor/$a, Thread:dispatcherSystem-defaultDispatcher1-5[INFO] [01/29/2018 18:02:44.603] [dispatcherSystem-defaultDispatcher1-6] [akka://dispatcherSystem/user/echoActor/$b] [接收消息]:消息:4, Actor:akka://dispatcherSystem/user/echoActor/$b, Thread:dispatcherSystem-defaultDispatcher1-6[INFO] [01/29/2018 18:02:44.603] [dispatcherSystem-defaultDispatcher1-7] [akka://dispatcherSystem/user/echoActor/$c] [接收消息]:消息:5, Actor:akka://dispatcherSystem/user/echoActor/$c, Thread:dispatcherSystem-defaultDispatcher1-7[INFO] [01/29/2018 18:02:44.603] [dispatcherSystem-defaultDispatcher1-8] [akka://dispatcherSystem/user/echoActor/$a] [接收消息]:消息:6, Actor:akka://dispatcherSystem/user/echoActor/$a, Thread:dispatcherSystem-defaultDispatcher1-8[INFO] [01/29/2018 18:02:44.603] [dispatcherSystem-defaultDispatcher1-9] [akka://dispatcherSystem/user/echoActor/$b] [接收消息]:消息:7, Actor:akka://dispatcherSystem/user/echoActor/$b, Thread:dispatcherSystem-defaultDispatcher1-9[INFO] [01/29/2018 18:02:44.603] [dispatcherSystem-defaultDispatcher1-8] [akka://dispatcherSystem/user/echoActor/$a] [接收消息]:消息:9, Actor:akka://dispatcherSystem/user/echoActor/$a, Thread:dispatcherSystem-defaultDispatcher1-8[INFO] [01/29/2018 18:02:44.604] [dispatcherSystem-defaultDispatcher1-10] [akka://dispatcherSystem/user/echoActor/$c] [接收消息]:消息:8, Actor:akka://dispatcherSystem/user/echoActor/$c, Thread:dispatcherSystem-defaultDispatcher1-10` Akka Remote应用 Akka是一种消息驱动运算模式，它实现跨JVM程序运算的方式是通过能跨JVM的消息系统来调动分布在不同JVM上ActorSystem中的Actor进行运算，前题是Akka的地址系统可以支持跨JVM定位。Akka的消息系统最高境界可以实现所谓的Actor位置透明化，这样在Akka编程中就无须关注Actor具体在哪个JVM上运行，分布式Actor编程从方式上跟普通Actor编程就不会有什么区别了。Akka的Remoting是一种点对点的跨JVM消息通道，让一个JVM上ActorSystem中的某个Actor可以连接另一个JVM上ActorSystem中的另一个Actor。两个JVM上的ActorSystem之间只需具备TCP网络连接功能就可以实现Akka Remoting了。Akka-Remoting还没有实现完全的位置透明化，因为用户还必须在代码里或者配置文件里指明目标Actor的具体地址. Maven依赖12345678910&lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-actor_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-remote_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.2&lt;/version&gt;&lt;/dependency&gt; 服务端消息处理123456789101112131415161718public class EchoActor extends AbstractLoggingActor&#123; private static AtomicInteger instance = new AtomicInteger(0); @Override public void preStart() &#123; log().info("启动实例 #" + instance.incrementAndGet() + ", Hashcode #" + this.hashCode()); &#125; @Override public Receive createReceive() &#123; return receiveBuilder() .match(String.class, x-&gt;&#123; log().info("[接收消息]:&#123;&#125;, Actor:&#123;&#125;", x, getSender().path()); &#125;) .build(); &#125;&#125; 服务端主类示例主类代码 1234567891011121314151617public class ServerSystem &#123; public static void main(String[] args) &#123; ActorSystem system = ActorSystem.create("ServerSys", serverConfig()); ActorRef actor = system.actorOf(Props.create(EchoActor.class), "echoActor"); actor.tell("服务端启动...", ActorRef.noSender()); &#125; public static Config serverConfig()&#123; Map&lt;String, Object&gt; map = Maps.newHashMap(); map.put("akka.actor.provider", "akka.remote.RemoteActorRefProvider"); map.put("akka.remote.transport", "akka.remote.netty.NettyRemoteTransport"); map.put("akka.remote.netty.tcp.hostname", "127.0.0.1"); map.put("akka.remote.netty.tcp.port", "2500"); return ConfigFactory.parseMap(map); &#125;&#125; 启动日志 12345[INFO] [01/29/2018 16:48:17.553] [main] [akka.remote.Remoting] Starting remoting[INFO] [01/29/2018 16:48:17.708] [main] [akka.remote.Remoting] Remoting started; listening on addresses :[akka.tcp://ServerSys@127.0.0.1:2500][INFO] [01/29/2018 16:48:17.709] [main] [akka.remote.Remoting] Remoting now listens on addresses: [akka.tcp://ServerSys@127.0.0.1:2500][INFO] [01/29/2018 16:48:17.734] [ServerSys-akka.actor.default-dispatcher-2] [akka.tcp://ServerSys@127.0.0.1:2500/user/echoActor] 启动实例 #1, Hashcode #242972067[INFO] [01/29/2018 16:48:17.734] [ServerSys-akka.actor.default-dispatcher-2] [akka.tcp://ServerSys@127.0.0.1:2500/user/echoActor] [接收消息]:服务端启动..., Actor:akka://ServerSys/user/echoActor 客户端主类示例主类代码 1234567891011121314151617181920public class ClientSystem &#123; public static void main(String[] args) &#123; final ActorSystem system = ActorSystem.create("ClientSys", clientConfig()); String path = "akka.tcp://ServerSys@127.0.0.1:2500/user/echoActor"; ActorRef remoteActor = system.actorFor(path); for(int i=0; i&lt;5; i++)&#123; remoteActor.tell("客户端推送发送" + i, ActorRef.noSender()); &#125; &#125; public static Config clientConfig()&#123; Map&lt;String, Object&gt; map = Maps.newHashMap(); map.put("akka.actor.provider", "akka.remote.RemoteActorRefProvider"); map.put("akka.remote.transport", "akka.remote.netty.NettyRemoteTransport"); map.put("akka.remote.netty.tcp.hostname", "127.0.0.1"); map.put("akka.remote.netty.tcp.port", "2600"); return ConfigFactory.parseMap(map); &#125;&#125; 启动日志 123[INFO] [01/29/2018 16:48:26.999] [main] [akka.remote.Remoting] Starting remoting[INFO] [01/29/2018 16:48:27.168] [main] [akka.remote.Remoting] Remoting started; listening on addresses :[akka.tcp://ClientSys@127.0.0.1:2600][INFO] [01/29/2018 16:48:27.169] [main] [akka.remote.Remoting] Remoting now listens on addresses: [akka.tcp://ClientSys@127.0.0.1:2600] 服务端日志 12345678910[INFO] [01/29/2018 16:48:17.553] [main] [akka.remote.Remoting] Starting remoting[INFO] [01/29/2018 16:48:17.708] [main] [akka.remote.Remoting] Remoting started; listening on addresses :[akka.tcp://ServerSys@127.0.0.1:2500][INFO] [01/29/2018 16:48:17.709] [main] [akka.remote.Remoting] Remoting now listens on addresses: [akka.tcp://ServerSys@127.0.0.1:2500][INFO] [01/29/2018 16:48:17.734] [ServerSys-akka.actor.default-dispatcher-2] [akka.tcp://ServerSys@127.0.0.1:2500/user/echoActor] 启动实例 #1, Hashcode #242972067[INFO] [01/29/2018 16:48:17.734] [ServerSys-akka.actor.default-dispatcher-2] [akka.tcp://ServerSys@127.0.0.1:2500/user/echoActor] [接收消息]:服务端启动..., Actor:akka://ServerSys/user/echoActor[INFO] [01/29/2018 16:48:27.326] [ServerSys-akka.actor.default-dispatcher-4] [akka.tcp://ServerSys@127.0.0.1:2500/user/echoActor] [接收消息]:客户端推送发送0, Actor:akka://ServerSys/user/echoActor[INFO] [01/29/2018 16:48:27.326] [ServerSys-akka.actor.default-dispatcher-4] [akka.tcp://ServerSys@127.0.0.1:2500/user/echoActor] [接收消息]:客户端推送发送1, Actor:akka://ServerSys/user/echoActor[INFO] [01/29/2018 16:48:27.326] [ServerSys-akka.actor.default-dispatcher-4] [akka.tcp://ServerSys@127.0.0.1:2500/user/echoActor] [接收消息]:客户端推送发送2, Actor:akka://ServerSys/user/echoActor[INFO] [01/29/2018 16:48:27.326] [ServerSys-akka.actor.default-dispatcher-4] [akka.tcp://ServerSys@127.0.0.1:2500/user/echoActor] [接收消息]:客户端推送发送3, Actor:akka://ServerSys/user/echoActor[INFO] [01/29/2018 16:48:27.326] [ServerSys-akka.actor.default-dispatcher-4] [akka.tcp://ServerSys@127.0.0.1:2500/user/echoActor] [接收消息]:客户端推送发送4, Actor:akka://ServerSys/user/echoActor 相关文档 http://www.cnblogs.com/tiger-xc/p/7063301.html]]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Akka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Akka介绍与入门示例]]></title>
    <url>%2F2017%2F02%2F01%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FAkka%2FAkka%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%85%A5%E9%97%A8%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[Akka介绍 Akka是用scala编写的actor模型框架。它在使用中不需要锁和多线程，每个Actor在独立空间中进行数据操作，Actor之间并不直接通信,而是通过了消息来相互沟通,每一个Actor都把它要做的事情都封装在了它的内部，操作都是异步进行的。理论上来讲,每一个Actor都拥有属于自己的轻量级线程,保护它不会被系统中的其他部分影响.因此,我们在编写Actor时,就不用担心并发的问题, 通过Actor能够简化锁以及线程管理.它可以用于高并发、分布式场景，需要注意的是，Akka消息的传递不保证绝对可靠投递，当然这带来了好处是整个实现简单. Actor具有以下的特性 提供了一种高级的抽象,能够封装状态和操作.简化并发应用的开发. 提供了异步的非阻塞的/高性能的事件驱动模型 超级轻量级的线程事件处理能力. 概念认知在了解Akka前先了解几个概念 并发&amp;并行并发: 指的是两个或多个任务都有进展，即使他们没有被同时执行。例如可以这样实现：划分出时间片，几个任务交叉执行，尽管时间片的执行是线性的。并行: 指可以真正同时执行。 异步&amp;同步所谓同步，就是在发出一个功能调用时，在没有得到结果之前，该调用就不返回。当一个异步过程调用发出后，调用者不能立刻得到结果。实际处理这个调用的部件在完成后，通过状态、通知和回调来通知调用者。 阻塞&amp;非阻塞阻塞调用是指调用结果返回之前，当前线程会被挂起。函数只有在得到结果之后才会返回。有人也许会把阻塞调用和同步调用等同起来，实际上他是不同的。对于同步调用来说，很多时候当前线程还是激活的，只是从逻辑上当前函数没有返回而已。非阻塞和阻塞的概念相对应，指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回 Akka模块介绍 akka-actor:最核心的依赖包,里面实现了Actor模型的大部分东西 akka-agent:代理/整合了Scala中的一些STM特性 akka-camel:整合了Apache的Camel akka-cluster:akka集群依赖,封装了集群成员的管理和路由 akka-kernel:akka的一个极简化的应用服务器,可以脱离项目单独运行. akka-osgi:对OSGI容器的支持,有akka的最基本的Bundle akka-remote:akka远程调用 akka-slf4j:Akka的日志事件监听 akka-testkit:Akka的各种测试工具 akka-zeromq:整合ZeroMQ 其中最总要的就是akka-actor,最简单的AKKA使用的话,只需要引入这个包就可以了. 建议一个Akka系统的步骤 建立角色系统，ActorSystem; 定义角色，actor; 注册角色，actorSystem.actorOf； 消息的传递，tell； 在actor内消息的处理（处理完后，也可通过actorSelect路径选择其它actor，并发送消息) 相关模块说明 角色系统ActorSystem 在akka中，所有角色都需在角色系统中，可以通过ActorSystem.create方法建立，同时可以指定角色系统名称，这会在actor路径中体现出来； 角色(Actor) 通过继承AbstractActor类，实现抽象方法createReceive,并在此方法定义消息处理模块(使用reciveBuilder.build进行构造), 接收消息（消息类型，其实也就是消息的class类型区分消息），并进行消息处理； 角色(Actor)注册/创建 通过ActorSystem的actorOf方法创建actor，创建时可以通过new BalancingPool(3)指定该actor的工作worker数量，以及actor的名称，这会在路径中体现出来。 角色(Actor)路径及查找 Akka内的各个actor采用类似于分布式文件系统（如hdfs）进行管理，actor类似于文件系统的各个文件。 Akka主要根路径有三个：/（根路径），/system（系统相关的路径），/user(用户路径，通常定义的actor在此路径下)。 有了路径，可以通过getContext().actorSelect()方法基于路径,获取到相关actor引用，进而发送消息. /user: 守护Actor 这个名为”/user”的守护者，作为所有用户创建actor的父actor，可能是需要打交道最多的。使用system.actorOf()创建的actor都是其子actor。这意味着，当该守护者终止时，系统中所有的普通actor都将被关闭。同时也意味着，该守护者的监管策略决定了普通顶级actor是如何被监督的. /system: 系统守护者 这个特殊的守护者被引入，是为了实现正确的关闭顺序，即日志（logging）要保持可用直到所有普通actor终止，即使日志本身也是用actor实现的。其实现方法是：系统守护者观察user守护者，并在收到Terminated消息初始化其自己的关闭过程。顶级的系统actor被监管的策略是，对收到的除ActorInitializationException和ActorKilledException之外的所有Exception无限地执行重启，这也将终止其所有子actor。所有其他Throwable被上升，然后将导致整个actor系统的关闭。 /: 根守护者 根守护者所谓“顶级”actor的祖父，它监督所有在Actor路径的顶级作用域中定义的特殊actor，使用发现任何Exception就终止子actor的SupervisorStrategy.stoppingStrategy策略。其他所有Throwable都会被上升……但是上升给谁？所有的真实actor都有一个监管者，但是根守护者没有父actor，因为它就是整个树结构的根。因此这里使用一个虚拟的ActorRef，在发现问题后立即停掉其子actor，并在根守护者完全终止之后（所有子actor递归停止），立即把actor系统的isTerminated置为true。 Akka监管策略Akka中有两种类型的监管策略：OneForOneStrategy 和AllForOneStrategy. 两者都配置有从异常类型监管指令间的映射（见上文），并限制了一个孩子被终止之前允许失败的次数。它们之间的区别在于，前者只将所获得的指令应用在发生故障的子actor上，而后者则是应用在所有孩子上。通常情况下，你应该使用OneForOneStrategy，这也是默认的策略。 Akka消息传递机制有三种基本类型： 至多一次投递的意思是对该机制下的每条消息，会被投递0或1次；更随意的说法就是，它意味着消息可能会丢失。 至少一次投递的意思对该机制下的每条消息，有可能为投递进行多次尝试，以使得至少有一个成功；更随意的说法就是，消息可能重复，但不会丢失。 恰好一次投递的意思对该机制下的每条消息，接收者会正好得到一次投递；消息既不能丢，也不会重复。 第一种是最廉价的——性能最高，实现开销最少——因为它可以用打后不管（fire-and-forget）的方式完成，不需要在发送端或传输机制中保留状态。第二种方式要求重试来对抗传输丢失，这意味着需要在发送端保持状态，并在接收端使用确认机制。第三种是最昂贵的——并因此表现最差——因为除了需要第二种方式的机制以外，还需要在接收端保持状态，以过滤重复的投递. Akka入门示例Maven依赖12345&lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-actor_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.9&lt;/version&gt;&lt;/dependency&gt; 定义Actor1234567891011121314151617public class HelloActor extends AbstractLoggingActor &#123; @Override public Receive createReceive() &#123; return receiveBuilder() .match(Number.class, m -&gt; &#123; log().info("[ACCEPT][String]:&#123;&#125;", m); System.out.println(m); &#125;) .match(String.class, StringUtils::isNotBlank, m -&gt; &#123; log().info("[Accept][Number]:&#123;&#125;", m); System.out.println(m); &#125;) .build(); &#125;&#125; 程序主类12345678910111213public class HelloMain &#123; public static void main(String[] args) &#123; // 1 ActorSystem system = ActorSystem.create("helloSystem"); // 2 ActorRef helloActor = system.actorOf(Props.create(HelloActor.class), "helloActor"); // 3 helloActor.tell("Hello, World!", ActorRef.noSender()); helloActor.tell(123, ActorRef.noSender()); helloActor.tell("Have Fun with Akka!", ActorRef.noSender()); &#125;&#125; 程序解读 创建一个Actor系统,命名为”helloSystem” 每个Actor对外封闭,使用ActorRef的actorOf方法创建Actor,示例创建了一个名为”helloActor”的Actor,绑定HelloActor事件处理. 发送消息给”helloActor”,tell()方法是异步的，它只给Actor的邮箱放一封邮件，然后就返回了。tell()方法的第一个参数是消息，第二个参数是发送者，这样接收者Actor就知道是谁给自己发的消息了. 示例中在主方法中发送消息,所以首次消息发送者为ActorRef.noSender(). 执行输出123456Hello, World!123Have Fun with Akka![INFO] [01/26/2018 15:33:06.870] [helloSystem-akka.actor.default-dispatcher-5] [akka://helloSystem/user/helloActor] [Accept][Number]:Hello, World![INFO] [01/26/2018 15:33:06.871] [helloSystem-akka.actor.default-dispatcher-5] [akka://helloSystem/user/helloActor] [ACCEPT][String]:123[INFO] [01/26/2018 15:33:06.871] [helloSystem-akka.actor.default-dispatcher-5] [akka://helloSystem/user/helloActor] [Accept][Number]:Have Fun with Akka! Akka消息传递示例使用Akka实现一个MapReduce的经典示例WordCount Maven依赖12345&lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-actor_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.9&lt;/version&gt;&lt;/dependency&gt; 工程结构12345678910111213141516171819├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── elons│ │ └── wordcount│ │ └── akka│ │ ├── actor│ │ │ ├── InputActor.java│ │ │ ├── MapperActor.java│ │ │ ├── OutputActor.java│ │ │ └── ReduceActor.java│ │ ├── main│ │ │ └── WcApplication.java│ │ └── trans│ │ └── Message.java│ └── resources│ └── application.properties 消息定义12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class Message &#123; @Data @Builder @NoArgsConstructor @AllArgsConstructor public static final class Count implements Serializable&#123; private String word; private Integer count; @Override public String toString() &#123; return JSON.toJSONString(this); &#125; &#125; @Data @Builder @NoArgsConstructor public static final class Mapper implements Serializable&#123; private List&lt;Count&gt; data; public Mapper(List&lt;Count&gt; data) &#123; this.data = Collections.unmodifiableList(data); &#125; @Override public String toString() &#123; return JSON.toJSONString(this); &#125; &#125; @Data @Builder @NoArgsConstructor public static final class Reduce implements Serializable&#123; private Map&lt;String, Integer&gt; data; public Reduce(Map&lt;String, Integer&gt; reduceDataList) &#123; this.data = Collections.unmodifiableMap(reduceDataList); &#125; @Override public String toString() &#123; return JSON.toJSONString(this); &#125; &#125; public static final class Result implements Serializable&#123; &#125;&#125; Actor定义InputActor定义 123456789101112131415161718192021222324252627public class InputActor extends AbstractLoggingActor&#123; // 链式逆向织入 private ActorRef wcOutputActor = context().system() .actorOf(Props.create(OutputActor.class), "outputActor"); private ActorRef reduceActor = context().system() .actorOf(Props.create(ReduceActor.class, wcOutputActor), "reduceActor"); private ActorRef mapperActor = context().system() .actorOf(Props.create(MapperActor.class, reduceActor), "mapperActor"); @Override public Receive createReceive() &#123; return receiveBuilder() .match(String.class, x -&gt; &#123; log().info("[事件][数据录入]: &#123;&#125;", x); mapperActor.tell(x, self()); &#125;) .match(Message.Result.class, x -&gt; &#123; log().info("[事件][结果统计]"); wcOutputActor.tell(x, self()); &#125;) .build(); &#125;&#125; MapperActor定义 123456789101112131415161718192021222324252627282930public class MapperActor extends AbstractLoggingActor&#123; private ActorRef reduceActor = null; public MapperActor(ActorRef reduceActor) &#123; this.reduceActor = reduceActor; &#125; @Override public Receive createReceive() &#123; return receiveBuilder() .match(String.class, x -&gt; &#123; Message.Mapper mapper = mapper(x); reduceActor.tell(mapper, self()); &#125;) .build(); &#125; private Message.Mapper mapper(String line) &#123; List&lt;Message.Count&gt; list = Lists.newArrayList(); StringTokenizer parser = new StringTokenizer(line); while (parser.hasMoreTokens()) &#123; String word = parser.nextToken().toLowerCase(); if (StringUtils.isNotBlank(word)) &#123; list.add(new Message.Count(word, 1)); &#125; &#125; return new Message.Mapper(list); &#125;&#125; ReduceActor定义 123456789101112131415161718192021222324252627282930313233343536373839public class ReduceActor extends AbstractLoggingActor&#123; /** * 结果集统计Actor */ private ActorRef wcOutputActor; public ReduceActor(ActorRef wcOutputActor)&#123; this.wcOutputActor = wcOutputActor; &#125; @Override public AbstractActor.Receive createReceive() &#123; return receiveBuilder() .match(Message.Mapper.class, x -&gt; &#123; log().info("[计算][Mapper]: &#123;&#125;", x); // reduce the incoming data Message.Reduce reduceData = reduce(x.getData()); // forward the result to aggregate actor wcOutputActor.tell(reduceData, self()); &#125;) .build(); &#125; private Message.Reduce reduce(List&lt;Message.Count&gt; coll) &#123; HashMap&lt;String, Integer&gt; reducedMap = Maps.newHashMap(); for (Message.Count count: coll) &#123; if (reducedMap.containsKey(count.getWord())) &#123; Integer value = reducedMap.get(count.getWord()); value++; reducedMap.put(count.getWord(), value); &#125; else &#123; reducedMap.put(count.getWord(), 1); &#125; &#125; return new Message.Reduce(reducedMap); &#125;&#125; OutputActor定义 123456789101112131415161718192021222324252627282930public class OutputActor extends AbstractLoggingActor &#123; private Map&lt;String, Integer&gt; wcCount = Maps.newHashMap(); @Override public Receive createReceive() &#123; return receiveBuilder() .match(Message.Reduce.class, x -&gt; &#123; log().info("[计算][Reduce]: &#123;&#125;", x); Map&lt;String, Integer&gt; map = x.getData(); countReduce(map); &#125;) .match(Message.Result.class, x -&gt; &#123; System.out.println("结果:" + wcCount); &#125;) .build(); &#125; private void countReduce(Map&lt;String, Integer&gt; reduce) &#123; Integer count = null; for (String key : reduce.keySet()) &#123; if (wcCount.containsKey(key)) &#123; count = wcCount.get(key) + reduce.get(key) ; wcCount.put(key, count); &#125; else &#123; wcCount.put(key, reduce.get(key)); &#125; &#125; &#125;&#125; WcApplication测试主类定义 12345678910111213141516171819public class WcApplication &#123; public static void main(String[] args) throws Exception &#123; ActorSystem system = ActorSystem.create("wordCountSystem"); ActorRef inputActor = system.actorOf(Props.create(InputActor.class),"inputActor"); // 推送待统计数据 inputActor.tell("Akka Map Reduce Demo", ActorRef.noSender()); inputActor.tell("Word Count With Akka , Map Reduce Test.", ActorRef.noSender()); Thread.sleep(500); // 获取计算结果 inputActor.tell(new Message.Result(), null); Thread.sleep(500); system.terminate(); &#125;&#125; 执行输出12345678[INFO] [01/29/2018 12:00:27.089] [wordCountSystem-akka.actor.default-dispatcher-3] [akka://wordCountSystem/user/inputActor] [事件][数据录入]: Akka Map Reduce Demo[INFO] [01/29/2018 12:00:27.089] [wordCountSystem-akka.actor.default-dispatcher-3] [akka://wordCountSystem/user/inputActor] [事件][数据录入]: Word Count With Akka , Map Reduce Test.[INFO] [01/29/2018 12:00:27.224] [wordCountSystem-akka.actor.default-dispatcher-3] [akka://wordCountSystem/user/reduceActor] [计算][Mapper]: &#123;"data":[&#123;"count":1,"word":"akka"&#125;,&#123;"count":1,"word":"map"&#125;,&#123;"count":1,"word":"reduce"&#125;,&#123;"count":1,"word":"demo"&#125;]&#125;[INFO] [01/29/2018 12:00:27.224] [wordCountSystem-akka.actor.default-dispatcher-3] [akka://wordCountSystem/user/reduceActor] [计算][Mapper]: &#123;"data":[&#123;"count":1,"word":"word"&#125;,&#123;"count":1,"word":"count"&#125;,&#123;"count":1,"word":"with"&#125;,&#123;"count":1,"word":"akka"&#125;,&#123;"count":1,"word":","&#125;,&#123;"count":1,"word":"map"&#125;,&#123;"count":1,"word":"reduce"&#125;,&#123;"count":1,"word":"test."&#125;]&#125;[INFO] [01/29/2018 12:00:27.228] [wordCountSystem-akka.actor.default-dispatcher-5] [akka://wordCountSystem/user/outputActor] [计算][Reduce]: &#123;"data":&#123;"reduce":1,"akka":1,"map":1,"demo":1&#125;&#125;[INFO] [01/29/2018 12:00:27.229] [wordCountSystem-akka.actor.default-dispatcher-5] [akka://wordCountSystem/user/outputActor] [计算][Reduce]: &#123;"data":&#123;"reduce":1,"with":1,"count":1,",":1,"word":1,"akka":1,"map":1,"test.":1&#125;&#125;结果:&#123;reduce=2, with=1, count=1, ,=1, akka=2, map=2, demo=1, word=1, test.=1&#125;[INFO] [01/29/2018 12:00:27.583] [wordCountSystem-akka.actor.default-dispatcher-4] [akka://wordCountSystem/user/inputActor] [事件][结果统计] 相关文档 https://doc.yonyoucloud.com/doc/akka-doc-cn/2.3.6/scala/book/chapter1/introduction.html http://blog.csdn.net/jmppok/article/details/17264495]]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Akka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Akka分布式计算应用]]></title>
    <url>%2F2017%2F02%2F01%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FAkka%2FAkka%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[介绍Akka-Cluster前 需要先说一下Akka-Remoting. Akka-Remoting一种ActorSystem之间Actor对Actor点对点的沟通协议.通过Akka-Remoting来实现一个ActorSystem中的一个Actor与另一个ActorSystem中的另一个Actor之间的沟通.在Remoting功能之后,Akka又发展了集群Cluster功能.Akka-Cluster是基于Akka-Remoting之上的新一代分布式运算环境,所以Remoting已经成为了Akka-Cluster的内部支持功能,在生产环境中的分布式运算应该尽量使用Akka-Cluster; Akka-Cluster可以在一部物理机或一组网络连接的服务器上搭建部署. 简单来说Akka-Cluster将多个JVM连接整合起来,实现消息地址的透明化和统一化使用管理,集成一体化的消息驱动系统.最终目的是能够把一个大型程序分割成多个子程序,然后部署到很多JVM上去实现程序的分布式并行运算.更重要的是：Cluster的构建过程与Actor编程没有牵连,当Cluster把多个ActorSystem集合成一个统一系统后,我们可以用在单一ActorSystem里编程的习惯方式编写分布式运算程序.由于在单一机器上就可以配置多个节点形成一个集群,我们开发的分布式程序可以在单机或多机群上运行,不同的只是如何部署和配置集群环境. 工程结构123456789101112131415161718192021222324[root@localhost akka-task ]$ tree akka-taskakka-task├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── elonsu│ │ └── cluster│ │ └── akka│ │ ├── Message.java│ │ ├── client│ │ │ ├── DispatchActor.java│ │ │ └── TaskClusterClient.java│ │ └── server│ │ ├── TaskActor.java│ │ ├── TaskClusterServer1.java│ │ ├── TaskClusterServer2.java│ │ └── TaskClusterServer3.java│ └── resources│ ├── akka-client.conf│ ├── akka-node1.conf│ ├── akka-node2.conf│ └── akka-node3.conf Maven依赖12345678910111213141516171819202122232425&lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-cluster_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-actor_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-remote_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.41&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.18&lt;/version&gt;&lt;/dependency&gt; 模块说明消息相关 Message: 示例中客户端和服务端通讯的消息定义 服务端相关 TaskActor: 服务端事件处理类 TaskClusterServer* : 服务端实例主类(示例中定义了3台实例) akka-node*.conf : 服务端配置 客户端相关 DispatchActor: 消息投递转发类, 用于对于服务端节点进行选取和消息投递 TaskClusterClient: 客户端示例主类 akka-client.conf : 客户端配置 代码示例(服务端)示例中定义的服务端端端口分别为:2551、2552、2553 消息定义说明: 消息定义必须实现序列化接口 1234567891011121314151617181920212223242526272829303132333435public class Message implements Serializable&#123; @Data @Builder @AllArgsConstructor @NoArgsConstructor public static final class Task implements Serializable&#123; private long taskId; private String content; @Override public String toString() &#123; return JSON.toJSONString(this); &#125; &#125; @Data @Builder @NoArgsConstructor @AllArgsConstructor public static final class Result implements Serializable &#123; private long taskId; private String result; @Override public String toString() &#123; return JSON.toJSONString(this); &#125; &#125;&#125; 服务端配置akka-node1.conf配置内容 123456789101112131415161718192021222324252627akka &#123; loglevel = "INFO" actor &#123; provider = "akka.cluster.ClusterActorRefProvider" &#125; remote &#123; log-remote-lifecycle-events = on netty.tcp &#123; hostname = "172.30.5.13" port = 2551 &#125; &#125; cluster &#123; roles = ["server"] seed-nodes = [ "akka.tcp://simpleAkkaCluster@172.30.5.13:2551", "akka.tcp://simpleAkkaCluster@172.30.5.13:2552", "akka.tcp://simpleAkkaCluster@172.30.5.13:2553"] auto-down-unreachable-after = 5s auto-down-unreachable-after = 5s metrics.enabled = off &#125;&#125; akka-node2.conf配置内容 123456789101112131415161718192021222324252627akka &#123; loglevel = "INFO" actor &#123; provider = "akka.cluster.ClusterActorRefProvider" &#125; remote &#123; log-remote-lifecycle-events = on netty.tcp &#123; hostname = "172.30.5.13" port = 2552 &#125; &#125; cluster &#123; roles = ["server"] seed-nodes = [ "akka.tcp://simpleAkkaCluster@172.30.5.13:2551", "akka.tcp://simpleAkkaCluster@172.30.5.13:2552", "akka.tcp://simpleAkkaCluster@172.30.5.13:2553"] auto-down-unreachable-after = 5s auto-down-unreachable-after = 5s metrics.enabled = off &#125;&#125; akka-node3.conf配置内容 123456789101112131415161718192021222324252627akka &#123; loglevel = "INFO" actor &#123; provider = "akka.cluster.ClusterActorRefProvider" &#125; remote &#123; log-remote-lifecycle-events = on netty.tcp &#123; hostname = "172.30.5.13" port = 2553 &#125; &#125; cluster &#123; roles = ["server"] seed-nodes = [ "akka.tcp://simpleAkkaCluster@172.30.5.13:2551", "akka.tcp://simpleAkkaCluster@172.30.5.13:2552", "akka.tcp://simpleAkkaCluster@172.30.5.13:2553"] auto-down-unreachable-after = 5s auto-down-unreachable-after = 5s metrics.enabled = off &#125;&#125; 服务端Actor123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public class TaskActor extends AbstractLoggingActor &#123; // 集群监听 private Cluster cluster = Cluster.get(getContext().system()); //subscribe to cluster changes @Override public void preStart() &#123; //#subscribe cluster.subscribe(getSelf(), ClusterEvent.initialStateAsEvents(), ClusterEvent.MemberEvent.class, ClusterEvent.UnreachableMember.class); //#subscribe &#125; //re-subscribe when restart @Override public void postStop() &#123; cluster.unsubscribe(getSelf()); &#125; @Override public Receive createReceive() &#123; return receiveBuilder() .match(Message.Task.class, x-&gt; &#123; log().info("[接收消息]:&#123;&#125;, Actor:&#123;&#125;, Thread:&#123;&#125;", x, getSelf().path(), Thread.currentThread().getName()); getSender().tell(new Message.Result(x.getTaskId(), x.getContent() + "处理完成"), self()); &#125; ) .match(ClusterEvent.MemberUp.class, x -&gt; log().info("Member is Up: &#123;&#125;", x.member()) ) .match(ClusterEvent.CurrentClusterState.class, x -&gt; log().info("Cluster State: &#123;&#125;", x.getMembers()) ) .match(ClusterEvent.UnreachableMember.class, x -&gt; log().info("Member detected as unreachable: &#123;&#125;", x.member()) ) .match(ClusterEvent.MemberRemoved.class, x -&gt; log().info("Member is Removed: &#123;&#125;", x.member()) ) .match(ClusterEvent.MemberEvent.class, x -&gt; log().info("ignore") ) .matchAny( x -&gt; unhandled(x) ) .build(); &#125;&#125; 服务端主类这里只列出Node1上的主类, 服务端节点中每个示例的内容基本一样,不同的是各自加载的配置文件&quot;akka-node*.conf&quot;与实例对应. 123456789public class TaskClusterServer1 &#123; public static void main(String[] args) &#123; ActorSystem system = ActorSystem.create("simpleAkkaCluster", ConfigFactory.load("akka-node1.conf")); ActorRef taskActor = system.actorOf(Props.create(TaskActor.class), "taskActor"); System.out.println("Akka Cluster Node1 启动完成" + taskActor.path()); &#125;&#125; 集群状态监听示例场景: 分别启动服务端节点Node1、Node2、Node3、然后关停Node2、再重新启动,观察集群状态日志 Node1日志123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[INFO] [01/31/2018 11:45:06.365] [main] [akka.remote.Remoting] Starting remoting[INFO] [01/31/2018 11:45:06.565] [main] [akka.remote.Remoting] Remoting started; listening on addresses :[akka.tcp://simpleAkkaCluster@172.30.5.13:2551][INFO] [01/31/2018 11:45:06.567] [main] [akka.remote.Remoting] Remoting now listens on addresses: [akka.tcp://simpleAkkaCluster@172.30.5.13:2551][INFO] [01/31/2018 11:45:06.578] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Starting up...[INFO] [01/31/2018 11:45:06.728] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Registered cluster JMX MBean [akka:type=Cluster][INFO] [01/31/2018 11:45:06.728] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Started up successfullyAkka Cluster Node1 启动完成akka://simpleAkkaCluster/user/taskActor[WARN] [01/31/2018 11:45:06.765] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/system/cluster/core/daemon/downingProvider] Don't use auto-down feature of Akka Cluster in production. See 'Auto-downing (DO NOT USE)' section of Akka Cluster documentation.[WARN] [01/31/2018 11:45:06.862] [New I/O boss #3] [NettyTransport(akka://simpleAkkaCluster)] Remote connection to [null] failed with java.net.ConnectException: Connection refused: /172.30.5.13:2553[WARN] [01/31/2018 11:45:06.862] [New I/O boss #3] [NettyTransport(akka://simpleAkkaCluster)] Remote connection to [null] failed with java.net.ConnectException: Connection refused: /172.30.5.13:2552[WARN] [01/31/2018 11:45:06.863] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-5] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2553-1] Association with remote system [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] has failed, address is now gated for [5000] ms. Reason: [Association failed with [akka.tcp://simpleAkkaCluster@172.30.5.13:2553]] Caused by: [Connection refused: /172.30.5.13:2553][WARN] [01/31/2018 11:45:06.863] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-18] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2552-0] Association with remote system [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] has failed, address is now gated for [5000] ms. Reason: [Association failed with [akka.tcp://simpleAkkaCluster@172.30.5.13:2552]] Caused by: [Connection refused: /172.30.5.13:2552][INFO] [01/31/2018 11:45:06.867] [simpleAkkaCluster-akka.actor.default-dispatcher-17] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#-113492512] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:45:06.867] [simpleAkkaCluster-akka.actor.default-dispatcher-17] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#-113492512] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:45:07.782] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#-113492512] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:45:07.782] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#-113492512] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [4] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:45:08.783] [simpleAkkaCluster-akka.actor.default-dispatcher-17] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#-113492512] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [5] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:45:08.783] [simpleAkkaCluster-akka.actor.default-dispatcher-17] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#-113492512] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:45:09.781] [simpleAkkaCluster-akka.actor.default-dispatcher-17] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#-113492512] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [7] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:45:09.781] [simpleAkkaCluster-akka.actor.default-dispatcher-17] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#-113492512] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [8] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:45:10.783] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#-113492512] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [9] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:45:10.783] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#-113492512] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [10] dead letters encountered, no more dead letters will be logged. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:45:11.792] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] is JOINING, roles [server, dc-default][INFO] [01/31/2018 11:45:11.802] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Leader is moving node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] to [Up][INFO] [01/31/2018 11:45:11.806] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2551, status = Up)[INFO] [01/31/2018 11:45:13.616] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Received InitJoin message from [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/cluster/core/daemon/joinSeedNodeProcess-1#-138411316]] to [akka.tcp://simpleAkkaCluster@172.30.5.13:2551][INFO] [01/31/2018 11:45:13.616] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Sending InitJoinAck message from node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] to [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/cluster/core/daemon/joinSeedNodeProcess-1#-138411316]][INFO] [01/31/2018 11:45:13.662] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] is JOINING, roles [server, dc-default][INFO] [01/31/2018 11:45:13.664] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] ignore[INFO] [01/31/2018 11:45:13.770] [simpleAkkaCluster-akka.actor.default-dispatcher-19] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Leader is moving node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] to [Up][INFO] [01/31/2018 11:45:13.771] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2552, status = Up)[INFO] [01/31/2018 11:45:17.496] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Received InitJoin message from [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/cluster/core/daemon/joinSeedNodeProcess-1#-505101083]] to [akka.tcp://simpleAkkaCluster@172.30.5.13:2551][INFO] [01/31/2018 11:45:17.496] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Sending InitJoinAck message from node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] to [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/cluster/core/daemon/joinSeedNodeProcess-1#-505101083]][INFO] [01/31/2018 11:45:17.523] [simpleAkkaCluster-akka.actor.default-dispatcher-23] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] is JOINING, roles [server, dc-default][INFO] [01/31/2018 11:45:17.523] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] ignore[INFO] [01/31/2018 11:45:18.770] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Leader is moving node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] to [Up][INFO] [01/31/2018 11:45:18.771] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2553, status = Up)[INFO] [01/31/2018 11:46:45.977] [simpleAkkaCluster-akka.actor.default-dispatcher-19] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] ignore[INFO] [01/31/2018 11:46:47.769] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Leader is moving node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] to [Exiting][INFO] [01/31/2018 11:46:47.771] [simpleAkkaCluster-akka.actor.default-dispatcher-23] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] ignore[INFO] [01/31/2018 11:46:48.776] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Exiting confirmed [akka.tcp://simpleAkkaCluster@172.30.5.13:2552][ERROR] [01/31/2018 11:46:48.798] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-18] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2552-0/endpointWriter] AssociationError [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] -&gt; [akka.tcp://simpleAkkaCluster@172.30.5.13:2552]: Error [Shut down address: akka.tcp://simpleAkkaCluster@172.30.5.13:2552] [akka.remote.ShutDownAssociation: Shut down address: akka.tcp://simpleAkkaCluster@172.30.5.13:2552Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.][ERROR] [01/31/2018 11:46:48.798] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-6] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/system/endpointManager/endpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2552-2] AssociationError [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] &lt;- [akka.tcp://simpleAkkaCluster@172.30.5.13:2552]: Error [Shut down address: akka.tcp://simpleAkkaCluster@172.30.5.13:2552] [akka.remote.ShutDownAssociation: Shut down address: akka.tcp://simpleAkkaCluster@172.30.5.13:2552Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.][INFO] [01/31/2018 11:46:50.771] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Leader is removing confirmed Exiting node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552][INFO] [01/31/2018 11:46:50.772] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] Member is Removed: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2552, status = Removed)[INFO] [01/31/2018 11:46:57.342] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Received InitJoin message from [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/cluster/core/daemon/joinSeedNodeProcess-1#-1328666486]] to [akka.tcp://simpleAkkaCluster@172.30.5.13:2551][INFO] [01/31/2018 11:46:57.342] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Sending InitJoinAck message from node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] to [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/cluster/core/daemon/joinSeedNodeProcess-1#-1328666486]][INFO] [01/31/2018 11:46:57.358] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] is JOINING, roles [server, dc-default][INFO] [01/31/2018 11:46:57.358] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] ignore[INFO] [01/31/2018 11:46:57.769] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Leader is moving node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] to [Up][INFO] [01/31/2018 11:46:57.770] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2552, status = Up) Node2日志首次加入集群 123456789101112131415161718192021[INFO] [01/31/2018 11:45:12.982] [main] [akka.remote.Remoting] Starting remoting[INFO] [01/31/2018 11:45:13.249] [main] [akka.remote.Remoting] Remoting started; listening on addresses :[akka.tcp://simpleAkkaCluster@172.30.5.13:2552][INFO] [01/31/2018 11:45:13.251] [main] [akka.remote.Remoting] Remoting now listens on addresses: [akka.tcp://simpleAkkaCluster@172.30.5.13:2552][INFO] [01/31/2018 11:45:13.267] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Starting up...[INFO] [01/31/2018 11:45:13.401] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Registered cluster JMX MBean [akka:type=Cluster][INFO] [01/31/2018 11:45:13.401] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Started up successfullyAkka Cluster Node1 启动完成akka://simpleAkkaCluster/user/taskActor[WARN] [01/31/2018 11:45:13.433] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/cluster/core/daemon/downingProvider] Don't use auto-down feature of Akka Cluster in production. See 'Auto-downing (DO NOT USE)' section of Akka Cluster documentation.[WARN] [01/31/2018 11:45:13.532] [New I/O boss #3] [NettyTransport(akka://simpleAkkaCluster)] Remote connection to [null] failed with java.net.ConnectException: Connection refused: /172.30.5.13:2553[WARN] [01/31/2018 11:45:13.536] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-15] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2553-1] Association with remote system [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] has failed, address is now gated for [5000] ms. Reason: [Association failed with [akka.tcp://simpleAkkaCluster@172.30.5.13:2553]] Caused by: [Connection refused: /172.30.5.13:2553][INFO] [01/31/2018 11:45:13.540] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/joinSeedNodeProcess-1#-138411316] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:45:13.770] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Welcome from [akka.tcp://simpleAkkaCluster@172.30.5.13:2551][INFO] [01/31/2018 11:45:13.779] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2551, status = Up)[INFO] [01/31/2018 11:45:13.779] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] ignore[INFO] [01/31/2018 11:45:13.811] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2552, status = Up)[INFO] [01/31/2018 11:45:17.496] [simpleAkkaCluster-akka.actor.default-dispatcher-21] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Received InitJoin message from [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/cluster/core/daemon/joinSeedNodeProcess-1#-505101083]] to [akka.tcp://simpleAkkaCluster@172.30.5.13:2552][INFO] [01/31/2018 11:45:17.496] [simpleAkkaCluster-akka.actor.default-dispatcher-21] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Sending InitJoinAck message from node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] to [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/cluster/core/daemon/joinSeedNodeProcess-1#-505101083]][INFO] [01/31/2018 11:45:17.780] [simpleAkkaCluster-akka.actor.default-dispatcher-18] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] ignore[INFO] [01/31/2018 11:45:17.803] [simpleAkkaCluster-akka.actor.default-dispatcher-21] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#-1800037640] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:45:18.372] [simpleAkkaCluster-akka.actor.default-dispatcher-18] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/cluster/core/daemon] Message [akka.cluster.GossipEnvelope] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon#-2084557458] to Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/cluster/core/daemon#-53996821] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:45:18.776] [simpleAkkaCluster-akka.actor.default-dispatcher-25] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2553, status = Up) 断开后再次加入集群 12345678910111213[INFO] [01/31/2018 11:46:56.861] [main] [akka.remote.Remoting] Starting remoting[INFO] [01/31/2018 11:46:57.054] [main] [akka.remote.Remoting] Remoting started; listening on addresses :[akka.tcp://simpleAkkaCluster@172.30.5.13:2552][INFO] [01/31/2018 11:46:57.055] [main] [akka.remote.Remoting] Remoting now listens on addresses: [akka.tcp://simpleAkkaCluster@172.30.5.13:2552][INFO] [01/31/2018 11:46:57.069] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Starting up...[INFO] [01/31/2018 11:46:57.184] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Registered cluster JMX MBean [akka:type=Cluster][INFO] [01/31/2018 11:46:57.184] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Started up successfullyAkka Cluster Node1 启动完成akka://simpleAkkaCluster/user/taskActor[WARN] [01/31/2018 11:46:57.210] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/cluster/core/daemon/downingProvider] Don't use auto-down feature of Akka Cluster in production. See 'Auto-downing (DO NOT USE)' section of Akka Cluster documentation.[INFO] [01/31/2018 11:46:57.375] [simpleAkkaCluster-akka.actor.default-dispatcher-5] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Welcome from [akka.tcp://simpleAkkaCluster@172.30.5.13:2551][INFO] [01/31/2018 11:46:57.378] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2551, status = Up)[INFO] [01/31/2018 11:46:57.379] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] ignore[INFO] [01/31/2018 11:46:57.379] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2553, status = Up)[INFO] [01/31/2018 11:46:58.227] [simpleAkkaCluster-akka.actor.default-dispatcher-19] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2552, status = Up) Node3日志1234567891011121314151617181920212223242526272829303132[INFO] [01/31/2018 11:45:17.020] [main] [akka.remote.Remoting] Starting remoting[INFO] [01/31/2018 11:45:17.213] [main] [akka.remote.Remoting] Remoting started; listening on addresses :[akka.tcp://simpleAkkaCluster@172.30.5.13:2553][INFO] [01/31/2018 11:45:17.215] [main] [akka.remote.Remoting] Remoting now listens on addresses: [akka.tcp://simpleAkkaCluster@172.30.5.13:2553][INFO] [01/31/2018 11:45:17.226] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] - Starting up...[INFO] [01/31/2018 11:45:17.332] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] - Registered cluster JMX MBean [akka:type=Cluster][INFO] [01/31/2018 11:45:17.332] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] - Started up successfullyAkka Cluster Node1 启动完成akka://simpleAkkaCluster/user/taskActor[WARN] [01/31/2018 11:45:17.359] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/cluster/core/daemon/downingProvider] Don't use auto-down feature of Akka Cluster in production. See 'Auto-downing (DO NOT USE)' section of Akka Cluster documentation.[INFO] [01/31/2018 11:45:17.544] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] - Welcome from [akka.tcp://simpleAkkaCluster@172.30.5.13:2551][INFO] [01/31/2018 11:45:17.549] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2551, status = Up)[INFO] [01/31/2018 11:45:17.550] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2552, status = Up)[INFO] [01/31/2018 11:45:17.551] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] ignore[INFO] [01/31/2018 11:45:19.381] [simpleAkkaCluster-akka.actor.default-dispatcher-17] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2553, status = Up)[INFO] [01/31/2018 11:46:46.368] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] ignore[INFO] [01/31/2018 11:46:48.777] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] - Exiting confirmed [akka.tcp://simpleAkkaCluster@172.30.5.13:2552][INFO] [01/31/2018 11:46:48.779] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] ignore[ERROR] [01/31/2018 11:46:48.806] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-13] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/endpointManager/endpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2552-3] AssociationError [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] &lt;- [akka.tcp://simpleAkkaCluster@172.30.5.13:2552]: Error [Shut down address: akka.tcp://simpleAkkaCluster@172.30.5.13:2552] [akka.remote.ShutDownAssociation: Shut down address: akka.tcp://simpleAkkaCluster@172.30.5.13:2552Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.][ERROR] [01/31/2018 11:46:48.806] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-14] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2552-1/endpointWriter] AssociationError [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] -&gt; [akka.tcp://simpleAkkaCluster@172.30.5.13:2552]: Error [Shut down address: akka.tcp://simpleAkkaCluster@172.30.5.13:2552] [akka.remote.ShutDownAssociation: Shut down address: akka.tcp://simpleAkkaCluster@172.30.5.13:2552Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.][INFO] [01/31/2018 11:46:49.365] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.GossipStatus] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon#-53996821] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:46:49.563] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#1010718345] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:46:50.573] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#1010718345] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 11:46:51.378] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] Member is Removed: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2552, status = Removed)[INFO] [01/31/2018 11:46:57.342] [simpleAkkaCluster-akka.actor.default-dispatcher-23] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] - Received InitJoin message from [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/cluster/core/daemon/joinSeedNodeProcess-1#-1328666486]] to [akka.tcp://simpleAkkaCluster@172.30.5.13:2553][INFO] [01/31/2018 11:46:57.342] [simpleAkkaCluster-akka.actor.default-dispatcher-23] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] - Sending InitJoinAck message from node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] to [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/cluster/core/daemon/joinSeedNodeProcess-1#-1328666486]][INFO] [01/31/2018 11:46:57.366] [simpleAkkaCluster-akka.actor.default-dispatcher-19] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] ignore[INFO] [01/31/2018 11:46:57.772] [simpleAkkaCluster-akka.actor.default-dispatcher-19] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2552, status = Up) 通过日志示例和服务端代码可以看到集群事件是通过MemberEvent传递的,这个其实就是每个成员所可能拥有的events,一个成员在它的生命周期中有以下的events: ClusterEvent.MemberJoined - 新的节点加入集群,此时的状态是Joining； ClusterEvent.MemberUp - 新的节点加入集群,此时的状态是Up； ClusterEvent.MemberExited - 节点正在离开集群,此时的状态是Exiting； ClusterEvent.MemberRemoved - 节点已经离开集群,此时的状态是Removed； ClusterEvent.UnreachableMember - 节点被标记为不可触达； ClusterEvent.ReachableMember - 节点被标记为可触达； 状态说明： Joining: 加入集群的瞬间状态 Up: 正常服务状态 Leaving / Exiting: 正常移出中状态 Down: 被标记为停机（不再是集群决策的一部分） Removed: 已从集群中移除 代码示例(客户端)示例中定义的客户端端口为:2600 客户端配置123456789101112131415161718192021222324252627akka &#123; loglevel = "INFO" actor &#123; provider = "akka.cluster.ClusterActorRefProvider" &#125; remote &#123; log-remote-lifecycle-events = on netty.tcp &#123; hostname = "172.30.5.13" port = 2600 &#125; &#125; cluster &#123; roles = ["router", "client"] seed-nodes = [ "akka.tcp://simpleAkkaCluster@172.30.5.13:2551", "akka.tcp://simpleAkkaCluster@172.30.5.13:2552", "akka.tcp://simpleAkkaCluster@172.30.5.13:2553"] auto-down-unreachable-after = 5s auto-down-unreachable-after = 5s metrics.enabled = off &#125;&#125; 客户端消息转发123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899public class DispatchActor extends AbstractLoggingActor &#123; // 服务路径 private String actorPath; // 服务集合 private List&lt;ActorRef&gt; servers = Lists.newArrayList(); // 集群监听 private Cluster cluster = Cluster.get(context().system()); // 服务地址统计 private AtomicInteger count = new AtomicInteger(0); public DispatchActor(String actorPath) &#123; this.actorPath = actorPath; &#125; @Override public void preStart() throws Exception &#123; cluster.subscribe(self(), ClusterEvent.MemberEvent.class, ClusterEvent.ReachabilityEvent.class); &#125; @Override public void postStop() throws Exception &#123; cluster.unsubscribe(self()); &#125; @Override public Receive createReceive() &#123; return receiveBuilder() .match(Message.Task.class, x -&gt; servers.isEmpty(), x -&gt; sender().tell(new Message.Result(-1L, "服务暂不可用"), sender()) ) .match(Message.Task.class, x -&gt; &#123; log().info("[Dispatcher] [消息转发] 集群节点数量:&#123;&#125;, 待处理任务:&#123;&#125;", servers.size(), x.getTaskId()); int index = count.incrementAndGet() % servers.size(); for(int i=0; i&lt;servers.size(); i++)&#123; System.out.println("[Dispatcher][服务选取] 选中[" + (i == index)+"] 地址: " + servers.get(i).path()); &#125; System.out.println(); servers.get(index).forward(x, getContext()); &#125; ) .match(Terminated.class, x -&gt; unRegister(x.getActor()) ) .match(ClusterEvent.CurrentClusterState.class, state -&gt; &#123; // 当前节点在刚刚加入集群时,会收到CurrentClusterState消息,从中可以解析出集群中的所有前端节点 servers.clear(); for (Member member : state.getMembers()) &#123; if (member.status().equals(MemberStatus.up())) &#123; register(member); &#125; &#125; &#125;) .match(ClusterEvent.MemberUp.class, x -&gt; register(x.member()) ) .match(ClusterEvent.MemberEvent.class, x -&gt; unRegister(x.member()) ) .match(ClusterEvent.UnreachableMember.class, x -&gt; unRegister(x.member()) ) .match(ClusterEvent.ReachableMember.class, x -&gt; unRegister(x.member()) ).build(); &#125; public void register(Member member)&#123; if(null != member &amp;&amp; member.hasRole("server"))&#123; register(getContext().actorFor(member.address() + "/user/" + this.actorPath)); &#125; &#125; public void unRegister(Member member)&#123; if(null != member &amp;&amp; member.hasRole("server"))&#123; unRegister(getContext().actorFor(member.address() + "/user/" + this.actorPath)); &#125; &#125; public void register(ActorRef actorRef)&#123; if(null != actorRef)&#123; servers.add(actorRef); &#125; &#125; public void unRegister(ActorRef actorRef)&#123; if(null != actorRef)&#123; servers.remove(actorRef); &#125; &#125;&#125; 客户端主类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class TaskClusterClient &#123; public static void main(String[] args) throws Exception &#123; ActorSystem system = ActorSystem.create("simpleAkkaCluster", ConfigFactory.load("akka-client.conf")); // 单节点处理 directorClient(system); // 集群处理 clusterClient(system); Thread.sleep(1000); system.terminate(); &#125; /** * 直接指定特定节点处理 * @throws Exception */ public static void directorClient(ActorSystem system) throws Exception &#123; ActorRef directClient = system.actorFor("akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor"); System.out.println("Akka Direct Client 启动完成. " + directClient.path()); remoteInvoke("[指定处理]", directClient); &#125; /** * 指定集群处理(集群节点选组由Dispatcher处理) * @throws Exception */ public static void clusterClient(ActorSystem system) throws Exception &#123; ActorRef clusterClient = system.actorOf(Props.create(DispatchActor.class, "taskActor"), "clientActor"); System.out.println("Akka Cluster Client 启动完成. " + clusterClient.path()); for(int i=0; i&lt;2; i++)&#123; System.out.println(); Thread.sleep(2000); remoteInvoke("[集群处理]", clusterClient); &#125; &#125; /** * 和远程端进行数据交互并获取处理结果 * @param depict 交互类型描述 * @param endpoint 服务节点地址 * @throws Exception */ public static void remoteInvoke(String depict, ActorRef endpoint) throws Exception &#123; List&lt;Future&lt;Object&gt;&gt; futures = Lists.newArrayList(); Timeout timeout = new Timeout(Duration.create(10, TimeUnit.SECONDS)); for(int i=0; i&lt;5; i++)&#123; String text = depict + "消息" + i + UUID.randomUUID().toString(); Future&lt;Object&gt; future = Patterns.ask(endpoint, new Message.Task(100L +i, text), timeout); futures.add(future); &#125; for(Future&lt;Object&gt; future: futures)&#123; Message.Result result = (Message.Result) Await.result(future, timeout.duration()); System.out.println("服务端返回:" + result); &#125; &#125;&#125; 客户端分别演示了指定节点处理和基于集群处理. 分布式计算启动服务端: 分别启动服务端节点(Node1、Node2、Node3)、启动客户端: 启动客户端主类模拟调用 服务端日志Node1日志 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[INFO] [01/31/2018 12:09:28.361] [main] [akka.remote.Remoting] Starting remoting[INFO] [01/31/2018 12:09:28.528] [main] [akka.remote.Remoting] Remoting started; listening on addresses :[akka.tcp://simpleAkkaCluster@172.30.5.13:2551][INFO] [01/31/2018 12:09:28.530] [main] [akka.remote.Remoting] Remoting now listens on addresses: [akka.tcp://simpleAkkaCluster@172.30.5.13:2551][INFO] [01/31/2018 12:09:28.541] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Starting up...[INFO] [01/31/2018 12:09:28.658] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Registered cluster JMX MBean [akka:type=Cluster][INFO] [01/31/2018 12:09:28.658] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Started up successfullyAkka Cluster Node1 启动完成akka://simpleAkkaCluster/user/taskActor[WARN] [01/31/2018 12:09:28.684] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/system/cluster/core/daemon/downingProvider] Don't use auto-down feature of Akka Cluster in production. See 'Auto-downing (DO NOT USE)' section of Akka Cluster documentation.[WARN] [01/31/2018 12:09:28.775] [New I/O boss #3] [NettyTransport(akka://simpleAkkaCluster)] Remote connection to [null] failed with java.net.ConnectException: Connection refused: /172.30.5.13:2552[WARN] [01/31/2018 12:09:28.775] [New I/O boss #3] [NettyTransport(akka://simpleAkkaCluster)] Remote connection to [null] failed with java.net.ConnectException: Connection refused: /172.30.5.13:2553[WARN] [01/31/2018 12:09:28.777] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-17] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2553-1] Association with remote system [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] has failed, address is now gated for [5000] ms. Reason: [Association failed with [akka.tcp://simpleAkkaCluster@172.30.5.13:2553]] Caused by: [Connection refused: /172.30.5.13:2553][WARN] [01/31/2018 12:09:28.777] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-5] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2552-0] Association with remote system [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] has failed, address is now gated for [5000] ms. Reason: [Association failed with [akka.tcp://simpleAkkaCluster@172.30.5.13:2552]] Caused by: [Connection refused: /172.30.5.13:2552][INFO] [01/31/2018 12:09:28.780] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#657073580] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:09:28.780] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#657073580] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:09:29.707] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#657073580] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:09:29.707] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#657073580] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [4] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:09:30.697] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#657073580] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [5] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:09:30.697] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#657073580] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:09:31.695] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#657073580] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [7] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:09:31.696] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#657073580] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [8] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:09:32.707] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#657073580] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [9] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:09:32.708] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/firstSeedNodeProcess-1#657073580] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [10] dead letters encountered, no more dead letters will be logged. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:09:33.715] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] is JOINING, roles [server, dc-default][INFO] [01/31/2018 12:09:33.722] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Leader is moving node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] to [Up][INFO] [01/31/2018 12:09:33.726] [simpleAkkaCluster-akka.actor.default-dispatcher-19] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2551, status = Up)[INFO] [01/31/2018 12:09:39.312] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Received InitJoin message from [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/cluster/core/daemon/joinSeedNodeProcess-1#-987896376]] to [akka.tcp://simpleAkkaCluster@172.30.5.13:2551][INFO] [01/31/2018 12:09:39.312] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Sending InitJoinAck message from node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] to [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/cluster/core/daemon/joinSeedNodeProcess-1#-987896376]][INFO] [01/31/2018 12:09:39.353] [simpleAkkaCluster-akka.actor.default-dispatcher-18] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] is JOINING, roles [server, dc-default][INFO] [01/31/2018 12:09:39.354] [simpleAkkaCluster-akka.actor.default-dispatcher-18] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] ignore[INFO] [01/31/2018 12:09:39.696] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Leader is moving node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] to [Up][INFO] [01/31/2018 12:09:39.696] [simpleAkkaCluster-akka.actor.default-dispatcher-18] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2552, status = Up)[INFO] [01/31/2018 12:09:48.491] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Received InitJoin message from [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/cluster/core/daemon/joinSeedNodeProcess-1#1412926543]] to [akka.tcp://simpleAkkaCluster@172.30.5.13:2551][INFO] [01/31/2018 12:09:48.491] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Sending InitJoinAck message from node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] to [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/cluster/core/daemon/joinSeedNodeProcess-1#1412926543]][INFO] [01/31/2018 12:09:48.703] [simpleAkkaCluster-akka.actor.default-dispatcher-18] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] ignore[INFO] [01/31/2018 12:09:49.694] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Leader is moving node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] to [Up][INFO] [01/31/2018 12:09:49.696] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2553, status = Up)[INFO] [01/31/2018 12:10:22.308] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Received InitJoin message from [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2600/system/cluster/core/daemon/joinSeedNodeProcess-1#-1040563122]] to [akka.tcp://simpleAkkaCluster@172.30.5.13:2551][INFO] [01/31/2018 12:10:22.308] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Sending InitJoinAck message from node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] to [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2600/system/cluster/core/daemon/joinSeedNodeProcess-1#-1040563122]][INFO] [01/31/2018 12:10:22.697] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] ignore[INFO] [01/31/2018 12:10:23.693] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Leader is moving node [akka.tcp://simpleAkkaCluster@172.30.5.13:2600] to [Up][INFO] [01/31/2018 12:10:23.694] [simpleAkkaCluster-akka.actor.default-dispatcher-23] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2600, status = Up)[INFO] [01/31/2018 12:10:24.653] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] [接收消息]:&#123;"content":"[集群处理]消息2fe9b091d-dbca-4a43-bb8f-4e11fa7db348","taskId":102&#125;, Actor:akka://simpleAkkaCluster/user/taskActor, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-3[WARN] [SECURITY][01/31/2018 12:10:24.654] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-14] [akka.serialization.Serialization(akka://simpleAkkaCluster)] Using the default Java serializer for class [com.elonsu.cluster.akka.Message$Result] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'[INFO] [01/31/2018 12:10:26.665] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] [接收消息]:&#123;"content":"[集群处理]消息071a6714d-94af-47e2-b3e2-e298ca60e6f7","taskId":100&#125;, Actor:akka://simpleAkkaCluster/user/taskActor, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-15[INFO] [01/31/2018 12:10:26.665] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] [接收消息]:&#123;"content":"[集群处理]消息34ce25577-f548-4836-8f86-a7e789f3284f","taskId":103&#125;, Actor:akka://simpleAkkaCluster/user/taskActor, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-15[ERROR] [01/31/2018 12:10:27.698] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-17] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2600-4/endpointWriter] AssociationError [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] &lt;- [akka.tcp://simpleAkkaCluster@172.30.5.13:2600]: Error [Shut down address: akka.tcp://simpleAkkaCluster@172.30.5.13:2600] [akka.remote.ShutDownAssociation: Shut down address: akka.tcp://simpleAkkaCluster@172.30.5.13:2600Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.][WARN] [01/31/2018 12:10:31.694] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/system/cluster/core/daemon] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Marking node(s) as UNREACHABLE [Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2600, status = Up)]. Node roles [server, dc-default][INFO] [01/31/2018 12:10:31.696] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] Member detected as unreachable: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2600, status = Up)[WARN] [01/31/2018 12:10:32.727] [New I/O boss #3] [NettyTransport(akka://simpleAkkaCluster)] Remote connection to [null] failed with java.net.ConnectException: Connection refused: /172.30.5.13:2600[WARN] [01/31/2018 12:10:32.728] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-5] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2600-5] Association with remote system [akka.tcp://simpleAkkaCluster@172.30.5.13:2600] has failed, address is now gated for [5000] ms. Reason: [Association failed with [akka.tcp://simpleAkkaCluster@172.30.5.13:2600]] Caused by: [Connection refused: /172.30.5.13:2600][INFO] [01/31/2018 12:10:36.711] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Leader is auto-downing unreachable node [akka.tcp://simpleAkkaCluster@172.30.5.13:2600]. Don't use auto-down feature of Akka Cluster in production. See 'Auto-downing (DO NOT USE)' section of Akka Cluster documentation.[INFO] [01/31/2018 12:10:36.712] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Marking unreachable node [akka.tcp://simpleAkkaCluster@172.30.5.13:2600] as [Down][WARN] [01/31/2018 12:10:38.727] [New I/O boss #3] [NettyTransport(akka://simpleAkkaCluster)] Remote connection to [null] failed with java.net.ConnectException: Connection refused: /172.30.5.13:2600[WARN] [01/31/2018 12:10:38.727] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-14] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2600-5] Association with remote system [akka.tcp://simpleAkkaCluster@172.30.5.13:2600] has failed, address is now gated for [5000] ms. Reason: [Association failed with [akka.tcp://simpleAkkaCluster@172.30.5.13:2600]] Caused by: [Connection refused: /172.30.5.13:2600][INFO] [01/31/2018 12:10:39.684] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2551] - Leader is removing unreachable node [akka.tcp://simpleAkkaCluster@172.30.5.13:2600][INFO] [01/31/2018 12:10:39.686] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor] Member is Removed: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2600, status = Removed) Node2日志 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[INFO] [01/31/2018 12:09:38.816] [main] [akka.remote.Remoting] Starting remoting[INFO] [01/31/2018 12:09:38.986] [main] [akka.remote.Remoting] Remoting started; listening on addresses :[akka.tcp://simpleAkkaCluster@172.30.5.13:2552][INFO] [01/31/2018 12:09:38.988] [main] [akka.remote.Remoting] Remoting now listens on addresses: [akka.tcp://simpleAkkaCluster@172.30.5.13:2552][INFO] [01/31/2018 12:09:39.009] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Starting up...[INFO] [01/31/2018 12:09:39.126] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Registered cluster JMX MBean [akka:type=Cluster][INFO] [01/31/2018 12:09:39.126] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Started up successfullyAkka Cluster Node1 启动完成akka://simpleAkkaCluster/user/taskActor[WARN] [01/31/2018 12:09:39.150] [simpleAkkaCluster-akka.actor.default-dispatcher-5] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/cluster/core/daemon/downingProvider] Don't use auto-down feature of Akka Cluster in production. See 'Auto-downing (DO NOT USE)' section of Akka Cluster documentation.[WARN] [01/31/2018 12:09:39.236] [New I/O boss #3] [NettyTransport(akka://simpleAkkaCluster)] Remote connection to [null] failed with java.net.ConnectException: Connection refused: /172.30.5.13:2553[WARN] [01/31/2018 12:09:39.237] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-6] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2553-1] Association with remote system [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] has failed, address is now gated for [5000] ms. Reason: [Association failed with [akka.tcp://simpleAkkaCluster@172.30.5.13:2553]] Caused by: [Connection refused: /172.30.5.13:2553][INFO] [01/31/2018 12:09:39.240] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/joinSeedNodeProcess-1#-987896376] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:09:39.431] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Welcome from [akka.tcp://simpleAkkaCluster@172.30.5.13:2551][INFO] [01/31/2018 12:09:39.434] [simpleAkkaCluster-akka.actor.default-dispatcher-5] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2551, status = Up)[INFO] [01/31/2018 12:09:39.434] [simpleAkkaCluster-akka.actor.default-dispatcher-5] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] ignore[INFO] [01/31/2018 12:09:39.707] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2552, status = Up)[INFO] [01/31/2018 12:09:48.492] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Received InitJoin message from [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/cluster/core/daemon/joinSeedNodeProcess-1#1412926543]] to [akka.tcp://simpleAkkaCluster@172.30.5.13:2552][INFO] [01/31/2018 12:09:48.492] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Sending InitJoinAck message from node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] to [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/cluster/core/daemon/joinSeedNodeProcess-1#1412926543]][INFO] [01/31/2018 12:09:48.520] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] is JOINING, roles [server, dc-default][INFO] [01/31/2018 12:09:48.521] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] ignore[INFO] [01/31/2018 12:09:49.700] [simpleAkkaCluster-akka.actor.default-dispatcher-5] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2553, status = Up)[INFO] [01/31/2018 12:10:22.308] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Received InitJoin message from [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2600/system/cluster/core/daemon/joinSeedNodeProcess-1#-1040563122]] to [akka.tcp://simpleAkkaCluster@172.30.5.13:2552][INFO] [01/31/2018 12:10:22.308] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Sending InitJoinAck message from node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] to [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2600/system/cluster/core/daemon/joinSeedNodeProcess-1#-1040563122]][INFO] [01/31/2018 12:10:22.330] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2600] is JOINING, roles [router, client, dc-default][INFO] [01/31/2018 12:10:22.331] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] ignore[INFO] [01/31/2018 12:10:24.651] [simpleAkkaCluster-akka.actor.default-dispatcher-19] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] [接收消息]:&#123;"content":"[集群处理]消息0d5dbe1b1-5d5d-48cb-8279-7afc9d013633","taskId":100&#125;, Actor:akka://simpleAkkaCluster/user/taskActor, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-19[INFO] [01/31/2018 12:10:24.652] [simpleAkkaCluster-akka.actor.default-dispatcher-19] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] [接收消息]:&#123;"content":"[集群处理]消息338f8e10b-ac1a-4714-9642-8ee64cb4c31f","taskId":103&#125;, Actor:akka://simpleAkkaCluster/user/taskActor, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-19[WARN] [SECURITY][01/31/2018 12:10:24.652] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-18] [akka.serialization.Serialization(akka://simpleAkkaCluster)] Using the default Java serializer for class [com.elonsu.cluster.akka.Message$Result] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'[INFO] [01/31/2018 12:10:25.169] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2600, status = Up)[INFO] [01/31/2018 12:10:26.665] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] [接收消息]:&#123;"content":"[集群处理]消息1104806a5-2a35-4328-ae7d-8d39052c5cf4","taskId":101&#125;, Actor:akka://simpleAkkaCluster/user/taskActor, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-20[INFO] [01/31/2018 12:10:26.667] [simpleAkkaCluster-akka.actor.default-dispatcher-23] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] [接收消息]:&#123;"content":"[集群处理]消息4113e1b57-d258-4ac4-96da-2dcdc531f823","taskId":104&#125;, Actor:akka://simpleAkkaCluster/user/taskActor, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-23[ERROR] [01/31/2018 12:10:27.695] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-7] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2600-4/endpointWriter] AssociationError [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] &lt;- [akka.tcp://simpleAkkaCluster@172.30.5.13:2600]: Error [Shut down address: akka.tcp://simpleAkkaCluster@172.30.5.13:2600] [akka.remote.ShutDownAssociation: Shut down address: akka.tcp://simpleAkkaCluster@172.30.5.13:2600Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.][INFO] [01/31/2018 12:10:28.450] [simpleAkkaCluster-akka.actor.default-dispatcher-21] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#145674424] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:10:29.453] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#145674424] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:10:30.452] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#145674424] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [4] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:10:31.452] [simpleAkkaCluster-akka.actor.default-dispatcher-19] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#145674424] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [5] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[WARN] [01/31/2018 12:10:32.162] [simpleAkkaCluster-akka.actor.default-dispatcher-19] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/cluster/core/daemon] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2552] - Marking node(s) as UNREACHABLE [Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2600, status = Up)]. Node roles [server, dc-default][INFO] [01/31/2018 12:10:32.164] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] Member detected as unreachable: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2600, status = Up)[INFO] [01/31/2018 12:10:32.451] [simpleAkkaCluster-akka.actor.default-dispatcher-21] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#145674424] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[WARN] [01/31/2018 12:10:33.457] [New I/O boss #3] [NettyTransport(akka://simpleAkkaCluster)] Remote connection to [null] failed with java.net.ConnectException: Connection refused: /172.30.5.13:2600[WARN] [01/31/2018 12:10:33.457] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-18] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2600-5] Association with remote system [akka.tcp://simpleAkkaCluster@172.30.5.13:2600] has failed, address is now gated for [5000] ms. Reason: [Association failed with [akka.tcp://simpleAkkaCluster@172.30.5.13:2600]] Caused by: [Connection refused: /172.30.5.13:2600][INFO] [01/31/2018 12:10:33.458] [simpleAkkaCluster-akka.actor.default-dispatcher-23] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#145674424] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [7] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:10:34.451] [simpleAkkaCluster-akka.actor.default-dispatcher-21] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#145674424] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [8] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:10:35.450] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#145674424] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [9] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:10:36.450] [simpleAkkaCluster-akka.actor.default-dispatcher-23] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#145674424] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [10] dead letters encountered, no more dead letters will be logged. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[WARN] [01/31/2018 12:10:39.455] [New I/O boss #3] [NettyTransport(akka://simpleAkkaCluster)] Remote connection to [null] failed with java.net.ConnectException: Connection refused: /172.30.5.13:2600[WARN] [01/31/2018 12:10:39.457] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-18] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2600-5] Association with remote system [akka.tcp://simpleAkkaCluster@172.30.5.13:2600] has failed, address is now gated for [5000] ms. Reason: [Association failed with [akka.tcp://simpleAkkaCluster@172.30.5.13:2600]] Caused by: [Connection refused: /172.30.5.13:2600][INFO] [01/31/2018 12:10:40.168] [simpleAkkaCluster-akka.actor.default-dispatcher-5] [akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor] Member is Removed: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2600, status = Removed) Node3日志 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[INFO] [01/31/2018 12:09:48.069] [main] [akka.remote.Remoting] Starting remoting[INFO] [01/31/2018 12:09:48.226] [main] [akka.remote.Remoting] Remoting started; listening on addresses :[akka.tcp://simpleAkkaCluster@172.30.5.13:2553][INFO] [01/31/2018 12:09:48.227] [main] [akka.remote.Remoting] Remoting now listens on addresses: [akka.tcp://simpleAkkaCluster@172.30.5.13:2553][INFO] [01/31/2018 12:09:48.238] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] - Starting up...[INFO] [01/31/2018 12:09:48.338] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] - Registered cluster JMX MBean [akka:type=Cluster][INFO] [01/31/2018 12:09:48.338] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] - Started up successfullyAkka Cluster Node1 启动完成akka://simpleAkkaCluster/user/taskActor[WARN] [01/31/2018 12:09:48.361] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/cluster/core/daemon/downingProvider] Don't use auto-down feature of Akka Cluster in production. See 'Auto-downing (DO NOT USE)' section of Akka Cluster documentation.[INFO] [01/31/2018 12:09:48.537] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] - Welcome from [akka.tcp://simpleAkkaCluster@172.30.5.13:2552][INFO] [01/31/2018 12:09:48.541] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2551, status = Up)[INFO] [01/31/2018 12:09:48.541] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2552, status = Up)[INFO] [01/31/2018 12:09:48.541] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] ignore[INFO] [01/31/2018 12:09:50.169] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2553, status = Up)[INFO] [01/31/2018 12:10:22.320] [simpleAkkaCluster-akka.actor.default-dispatcher-19] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] - Received InitJoin message from [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2600/system/cluster/core/daemon/joinSeedNodeProcess-1#-1040563122]] to [akka.tcp://simpleAkkaCluster@172.30.5.13:2553][INFO] [01/31/2018 12:10:22.321] [simpleAkkaCluster-akka.actor.default-dispatcher-19] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] - Sending InitJoinAck message from node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] to [Actor[akka.tcp://simpleAkkaCluster@172.30.5.13:2600/system/cluster/core/daemon/joinSeedNodeProcess-1#-1040563122]][INFO] [01/31/2018 12:10:22.428] [simpleAkkaCluster-akka.actor.default-dispatcher-21] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] [接收消息]:&#123;"content":"[指定处理]消息0185c0218-8792-44a3-9bbe-c79fd5d1fe5e","taskId":100&#125;, Actor:akka://simpleAkkaCluster/user/taskActor, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-21[INFO] [01/31/2018 12:10:22.429] [simpleAkkaCluster-akka.actor.default-dispatcher-21] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] [接收消息]:&#123;"content":"[指定处理]消息1572b5e48-16dd-4137-92b8-604d92e1fe0e","taskId":101&#125;, Actor:akka://simpleAkkaCluster/user/taskActor, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-21[INFO] [01/31/2018 12:10:22.429] [simpleAkkaCluster-akka.actor.default-dispatcher-21] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] [接收消息]:&#123;"content":"[指定处理]消息2defa8129-a316-434d-a749-7081a5b11a50","taskId":102&#125;, Actor:akka://simpleAkkaCluster/user/taskActor, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-21[INFO] [01/31/2018 12:10:22.429] [simpleAkkaCluster-akka.actor.default-dispatcher-21] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] [接收消息]:&#123;"content":"[指定处理]消息375821f49-51e7-4530-8728-c05e96f334fc","taskId":103&#125;, Actor:akka://simpleAkkaCluster/user/taskActor, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-21[INFO] [01/31/2018 12:10:22.429] [simpleAkkaCluster-akka.actor.default-dispatcher-21] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] [接收消息]:&#123;"content":"[指定处理]消息4764d693f-b803-4b6c-87db-a6c8dcd9ce68","taskId":104&#125;, Actor:akka://simpleAkkaCluster/user/taskActor, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-21[INFO] [01/31/2018 12:10:22.430] [simpleAkkaCluster-akka.actor.default-dispatcher-18] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] ignore[WARN] [SECURITY][01/31/2018 12:10:22.430] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-13] [akka.serialization.Serialization(akka://simpleAkkaCluster)] Using the default Java serializer for class [com.elonsu.cluster.akka.Message$Result] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'[INFO] [01/31/2018 12:10:24.373] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2600, status = Up)[INFO] [01/31/2018 12:10:24.571] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] [接收消息]:&#123;"content":"[集群处理]消息15cbfbf9d-323d-428a-bbd5-dc66dec9b3e3","taskId":101&#125;, Actor:akka://simpleAkkaCluster/user/taskActor, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-15[INFO] [01/31/2018 12:10:24.572] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] [接收消息]:&#123;"content":"[集群处理]消息4f65324da-5eee-4815-9c3d-6541cfc09446","taskId":104&#125;, Actor:akka://simpleAkkaCluster/user/taskActor, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-15[INFO] [01/31/2018 12:10:26.665] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] [接收消息]:&#123;"content":"[集群处理]消息2320bd63b-9911-49bb-a58a-2cc12eac733f","taskId":102&#125;, Actor:akka://simpleAkkaCluster/user/taskActor, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-15[ERROR] [01/31/2018 12:10:27.706] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-14] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2600-4/endpointWriter] AssociationError [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] &lt;- [akka.tcp://simpleAkkaCluster@172.30.5.13:2600]: Error [Shut down address: akka.tcp://simpleAkkaCluster@172.30.5.13:2600] [akka.remote.ShutDownAssociation: Shut down address: akka.tcp://simpleAkkaCluster@172.30.5.13:2600Caused by: akka.remote.transport.Transport$InvalidAssociationException: The remote system terminated the association because it is shutting down.][INFO] [01/31/2018 12:10:28.558] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#2131930055] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:10:29.558] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#2131930055] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:10:30.558] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#2131930055] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [3] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:10:31.369] [simpleAkkaCluster-akka.actor.default-dispatcher-23] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.GossipStatus] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon#-1409330530] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [4] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:10:31.558] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#2131930055] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [5] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:10:31.708] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] Member detected as unreachable: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2600, status = Up)[WARN] [01/31/2018 12:10:32.371] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/cluster/core/daemon] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2553] - Marking node(s) as UNREACHABLE [Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2600, status = Up)]. Node roles [server, dc-default][INFO] [01/31/2018 12:10:32.557] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#2131930055] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[WARN] [01/31/2018 12:10:33.565] [New I/O boss #3] [NettyTransport(akka://simpleAkkaCluster)] Remote connection to [null] failed with java.net.ConnectException: Connection refused: /172.30.5.13:2600[WARN] [01/31/2018 12:10:33.565] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-5] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2600-5] Association with remote system [akka.tcp://simpleAkkaCluster@172.30.5.13:2600] has failed, address is now gated for [5000] ms. Reason: [Association failed with [akka.tcp://simpleAkkaCluster@172.30.5.13:2600]] Caused by: [Connection refused: /172.30.5.13:2600][INFO] [01/31/2018 12:10:33.566] [simpleAkkaCluster-akka.actor.default-dispatcher-23] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#2131930055] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [7] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:10:34.558] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#2131930055] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [8] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:10:35.557] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#2131930055] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [9] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[INFO] [01/31/2018 12:10:36.558] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.ClusterHeartbeatSender$Heartbeat] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/heartbeatSender#2131930055] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [10] dead letters encountered, no more dead letters will be logged. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[WARN] [01/31/2018 12:10:39.561] [New I/O boss #3] [NettyTransport(akka://simpleAkkaCluster)] Remote connection to [null] failed with java.net.ConnectException: Connection refused: /172.30.5.13:2600[WARN] [01/31/2018 12:10:39.562] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-13] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/system/endpointManager/reliableEndpointWriter-akka.tcp%3A%2F%2FsimpleAkkaCluster%40172.30.5.13%3A2600-5] Association with remote system [akka.tcp://simpleAkkaCluster@172.30.5.13:2600] has failed, address is now gated for [5000] ms. Reason: [Association failed with [akka.tcp://simpleAkkaCluster@172.30.5.13:2600]] Caused by: [Connection refused: /172.30.5.13:2600][INFO] [01/31/2018 12:10:39.694] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor] Member is Removed: Member(address = akka.tcp://simpleAkkaCluster@172.30.5.13:2600, status = Removed) 客户端日志12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182[INFO] [01/31/2018 12:10:21.847] [main] [akka.remote.Remoting] Starting remoting[INFO] [01/31/2018 12:10:22.021] [main] [akka.remote.Remoting] Remoting started; listening on addresses :[akka.tcp://simpleAkkaCluster@172.30.5.13:2600][INFO] [01/31/2018 12:10:22.022] [main] [akka.remote.Remoting] Remoting now listens on addresses: [akka.tcp://simpleAkkaCluster@172.30.5.13:2600][INFO] [01/31/2018 12:10:22.036] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2600] - Starting up...[INFO] [01/31/2018 12:10:22.148] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2600] - Registered cluster JMX MBean [akka:type=Cluster][INFO] [01/31/2018 12:10:22.148] [main] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2600] - Started up successfullyAkka Direct Client 启动完成. akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor[WARN] [01/31/2018 12:10:22.179] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@172.30.5.13:2600/system/cluster/core/daemon/downingProvider] Don't use auto-down feature of Akka Cluster in production. See 'Auto-downing (DO NOT USE)' section of Akka Cluster documentation.[WARN] [SECURITY][01/31/2018 12:10:22.288] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-14] [akka.serialization.Serialization(akka://simpleAkkaCluster)] Using the default Java serializer for class [com.elonsu.cluster.akka.Message$Task] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'[INFO] [01/31/2018 12:10:22.351] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@172.30.5.13:2600] - Welcome from [akka.tcp://simpleAkkaCluster@172.30.5.13:2552]服务端返回:&#123;"result":"[指定处理]消息0185c0218-8792-44a3-9bbe-c79fd5d1fe5e处理完成","taskId":100&#125;服务端返回:&#123;"result":"[指定处理]消息1572b5e48-16dd-4137-92b8-604d92e1fe0e处理完成","taskId":101&#125;服务端返回:&#123;"result":"[指定处理]消息2defa8129-a316-434d-a749-7081a5b11a50处理完成","taskId":102&#125;服务端返回:&#123;"result":"[指定处理]消息375821f49-51e7-4530-8728-c05e96f334fc处理完成","taskId":103&#125;服务端返回:&#123;"result":"[指定处理]消息4764d693f-b803-4b6c-87db-a6c8dcd9ce68处理完成","taskId":104&#125;Akka Cluster Client 启动完成. akka://simpleAkkaCluster/user/clientActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor[Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor[Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor[INFO] [01/31/2018 12:10:24.567] [simpleAkkaCluster-akka.actor.default-dispatcher-17] [akka.tcp://simpleAkkaCluster@172.30.5.13:2600/user/clientActor] [Dispatcher] [消息转发] 集群节点数量:3, 待处理任务:100[Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor[INFO] [01/31/2018 12:10:24.568] [simpleAkkaCluster-akka.actor.default-dispatcher-17] [akka.tcp://simpleAkkaCluster@172.30.5.13:2600/user/clientActor] [Dispatcher] [消息转发] 集群节点数量:3, 待处理任务:101[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor[Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor[INFO] [01/31/2018 12:10:24.568] [simpleAkkaCluster-akka.actor.default-dispatcher-17] [akka.tcp://simpleAkkaCluster@172.30.5.13:2600/user/clientActor] [Dispatcher] [消息转发] 集群节点数量:3, 待处理任务:102[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor[Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor[INFO] [01/31/2018 12:10:24.568] [simpleAkkaCluster-akka.actor.default-dispatcher-17] [akka.tcp://simpleAkkaCluster@172.30.5.13:2600/user/clientActor] [Dispatcher] [消息转发] 集群节点数量:3, 待处理任务:103[INFO] [01/31/2018 12:10:24.569] [simpleAkkaCluster-akka.actor.default-dispatcher-17] [akka.tcp://simpleAkkaCluster@172.30.5.13:2600/user/clientActor] [Dispatcher] [消息转发] 集群节点数量:3, 待处理任务:104服务端返回:&#123;"result":"[集群处理]消息0d5dbe1b1-5d5d-48cb-8279-7afc9d013633处理完成","taskId":100&#125;服务端返回:&#123;"result":"[集群处理]消息15cbfbf9d-323d-428a-bbd5-dc66dec9b3e3处理完成","taskId":101&#125;服务端返回:&#123;"result":"[集群处理]消息2fe9b091d-dbca-4a43-bb8f-4e11fa7db348处理完成","taskId":102&#125;服务端返回:&#123;"result":"[集群处理]消息338f8e10b-ac1a-4714-9642-8ee64cb4c31f处理完成","taskId":103&#125;服务端返回:&#123;"result":"[集群处理]消息4f65324da-5eee-4815-9c3d-6541cfc09446处理完成","taskId":104&#125;[Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor[Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor[INFO] [01/31/2018 12:10:26.662] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.tcp://simpleAkkaCluster@172.30.5.13:2600/user/clientActor] [Dispatcher] [消息转发] 集群节点数量:3, 待处理任务:100[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor[Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor[INFO] [01/31/2018 12:10:26.662] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.tcp://simpleAkkaCluster@172.30.5.13:2600/user/clientActor] [Dispatcher] [消息转发] 集群节点数量:3, 待处理任务:101[Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor[INFO] [01/31/2018 12:10:26.662] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.tcp://simpleAkkaCluster@172.30.5.13:2600/user/clientActor] [Dispatcher] [消息转发] 集群节点数量:3, 待处理任务:102[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2551/user/taskActor[Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2552/user/taskActor[Dispatcher][服务选取] 选中[false] 地址: akka.tcp://simpleAkkaCluster@172.30.5.13:2553/user/taskActor[INFO] [01/31/2018 12:10:26.663] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.tcp://simpleAkkaCluster@172.30.5.13:2600/user/clientActor] [Dispatcher] [消息转发] 集群节点数量:3, 待处理任务:103[INFO] [01/31/2018 12:10:26.663] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.tcp://simpleAkkaCluster@172.30.5.13:2600/user/clientActor] [Dispatcher] [消息转发] 集群节点数量:3, 待处理任务:104服务端返回:&#123;"result":"[集群处理]消息071a6714d-94af-47e2-b3e2-e298ca60e6f7处理完成","taskId":100&#125;服务端返回:&#123;"result":"[集群处理]消息1104806a5-2a35-4328-ae7d-8d39052c5cf4处理完成","taskId":101&#125;服务端返回:&#123;"result":"[集群处理]消息2320bd63b-9911-49bb-a58a-2cc12eac733f处理完成","taskId":102&#125;服务端返回:&#123;"result":"[集群处理]消息34ce25577-f548-4836-8f86-a7e789f3284f处理完成","taskId":103&#125;服务端返回:&#123;"result":"[集群处理]消息4113e1b57-d258-4ac4-96da-2dcdc531f823处理完成","taskId":104&#125;[INFO] [01/31/2018 12:10:27.683] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-18] [akka.tcp://simpleAkkaCluster@172.30.5.13:2600/system/remoting-terminator] Shutting down remote daemon.[INFO] [01/31/2018 12:10:27.684] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-18] [akka.tcp://simpleAkkaCluster@172.30.5.13:2600/system/remoting-terminator] Remote daemon shut down; proceeding with flushing remote transports.[INFO] [01/31/2018 12:10:27.710] [simpleAkkaCluster-akka.actor.default-dispatcher-20] [akka.remote.Remoting] Remoting shut down[INFO] [01/31/2018 12:10:27.711] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-5] [akka.tcp://simpleAkkaCluster@172.30.5.13:2600/system/remoting-terminator] Remoting shut down. 相关文档 http://www.cnblogs.com/tiger-xc/p/7082905.html http://blog.csdn.net/tiger_xc/article/details/72857967 http://blog.csdn.net/zhanglh046/article/details/78621376]]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Akka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Akka与Spring整合应用]]></title>
    <url>%2F2017%2F02%2F01%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FAkka%2FAkka%E4%B8%8ESpring%E6%95%B4%E5%90%88%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本篇集合Akka应用-分布式计算应用展示在Spring项目中做分布式计算. 工程结构123456789101112131415161718192021222324252627[root@localhost spring-akka ]$ tree akka-taskakka-task├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── elonsu│ │ └── cluster│ │ └── akka│ │ ├── Application.java│ │ └── spring│ │ ├── actor│ │ │ ├── Message.java│ │ │ ├── RouterActor.java│ │ │ └── SimpleActor.java│ │ ├── config│ │ │ ├── ClientConfig.java│ │ │ └── ServerConfig.java│ │ ├── controller│ │ │ └── AkkaController.java│ │ └── support│ │ ├── SpringActorProducer.java│ │ └── SpringExtension.java│ └── resources│ ├── akka-client.conf│ └── akka-server.conf Maven依赖1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;properties&gt; &lt;akka.version&gt;2.5.9&lt;/akka.version&gt; &lt;guava.version&gt;18.0&lt;/guava.version&gt; &lt;lombok.version&gt;1.16.18&lt;/lombok.version&gt; &lt;fastjson.version&gt;1.2.41&lt;/fastjson.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-cluster_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;akka.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-actor_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;akka.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-remote_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;akka.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;$&#123;fastjson.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;$&#123;lombok.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;$&#123;guava.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 模块说明消息相关 Message: 示例中客户端和服务端通讯的消息定义 服务端相关 SimpleActor: 服务端事件处理类 ServerConfig : 服务端实例化配置 akka-server.conf : 服务端配置 客户端相关 RouterActor: 消息投递转发类, 用于对于服务端节点进行选取和消息投递 ClientConfig: 客户端端实例化配置 akka-client.conf : 客户端配置 应用启动类 Application: SpringBoot应用主类 容器衔接 SpringActorProducer &amp; SpringExtension: IndirectActorProducer是Akka提供的Actor生成接口, SpringActorProducer实现该接口, 以ApplicationContextAware为代理生成我们需要的Actor的Props 模块代码Spring衔接SpringActorProducer代码 12345678910111213141516171819202122public class SpringActorProducer implements IndirectActorProducer &#123; final ApplicationContext applicationContext; final String actorBeanName; final Object[] args; public SpringActorProducer(ApplicationContext applicationContext, String actorBeanName, Object... args) &#123; this.applicationContext = applicationContext; this.actorBeanName = actorBeanName; this.args = args; &#125; @Override public Actor produce() &#123; return (Actor) applicationContext.getBean(actorBeanName, args); &#125; @Override public Class&lt;? extends Actor&gt; actorClass() &#123; return (Class&lt;? extends Actor&gt;) applicationContext.getType(actorBeanName); &#125;&#125; SpringExtension代码 1234567891011121314151617181920212223public class SpringExtension extends AbstractExtensionId&lt;SpringExtension.SpringExt&gt; &#123; public static SpringExtension INSTANCE = new SpringExtension(); @Override public SpringExt createExtension(ExtendedActorSystem system) &#123; return new SpringExt(); &#125; public static class SpringExt implements Extension &#123; private volatile ApplicationContext applicationContext; public void initialize(ApplicationContext applicationContext) &#123; this.applicationContext = applicationContext; &#125; public Props props(String actorBeanName, Object... args) &#123; return Props.create(SpringActorProducer.class, applicationContext, actorBeanName, args); &#125; &#125;&#125;` 消息定义说明: 消息定义必须实现序列化接口 1234567891011121314151617181920212223242526272829303132333435public class Message implements Serializable&#123; @Data @Builder @AllArgsConstructor @NoArgsConstructor public static final class Task implements Serializable&#123; private long taskId; private String content; @Override public String toString() &#123; return JSON.toJSONString(this); &#125; &#125; @Data @Builder @NoArgsConstructor @AllArgsConstructor public static final class Result implements Serializable &#123; private long taskId; private String result; @Override public String toString() &#123; return JSON.toJSONString(this); &#125; &#125;&#125; 配置文件服务端配置 1234567891011121314151617181920212223242526akka &#123; loglevel = "INFO" actor &#123; provider = "akka.cluster.ClusterActorRefProvider" &#125; remote &#123; log-remote-lifecycle-events = on netty.tcp &#123; hostname = "127.0.0.1" port = 2555 &#125; &#125; cluster &#123; roles = ["server"] seed-nodes = [ "akka.tcp://simpleAkkaCluster@127.0.0.1:2555"] auto-down-unreachable-after = 1000s auto-down-unreachable-after = 1000s metrics.enabled = off jmx.multi-mbeans-in-same-jvm = on &#125;&#125; 客户端配置 123456789101112131415161718192021222324252627akka &#123; loglevel = "INFO" actor &#123; provider = "akka.cluster.ClusterActorRefProvider" &#125; remote &#123; log-remote-lifecycle-events = on netty.tcp &#123; hostname = "127.0.0.1" port = 2556 &#125; &#125; cluster &#123; roles = ["router", "client"] seed-nodes = [ "akka.tcp://simpleAkkaCluster@127.0.0.1:2555"] auto-down-unreachable-after = 1000s auto-down-unreachable-after = 1000s metrics.enabled = off jmx.multi-mbeans-in-same-jvm = on &#125;&#125; Actor示例服务端消息处理Actor 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Component@Scope("prototype")public class SimpleActor extends AbstractLoggingActor&#123; // 集群监听 private Cluster cluster = Cluster.get(getContext().system()); //subscribe to cluster changes @Override public void preStart() &#123; //#subscribe cluster.subscribe(getSelf(), ClusterEvent.initialStateAsEvents(), ClusterEvent.MemberEvent.class, ClusterEvent.UnreachableMember.class); //#subscribe &#125; //re-subscribe when restart @Override public void postStop() &#123; cluster.unsubscribe(getSelf()); &#125; @Override public Receive createReceive() &#123; return receiveBuilder() .match(Message.Task.class, x-&gt; &#123; log().info("[接收消息]:&#123;&#125;, Actor:&#123;&#125;, Thread:&#123;&#125;", x, getSelf().path(), Thread.currentThread().getName()); getSender().tell(new Message.Result(x.getTaskId(), x.getContent() + "处理完成"), self()); &#125; ) .match(ClusterEvent.MemberUp.class, x -&gt; log().info("Member is Up: &#123;&#125;", x.member()) ) .match(ClusterEvent.CurrentClusterState.class, x -&gt; log().info("Cluster State: &#123;&#125;", x.getMembers()) ) .match(ClusterEvent.UnreachableMember.class, x -&gt; log().info("Member detected as unreachable: &#123;&#125;", x.member()) ) .match(ClusterEvent.MemberRemoved.class, x -&gt; log().info("Member is Removed: &#123;&#125;", x.member()) ) .match(ClusterEvent.MemberEvent.class, x -&gt; log().info("ignore") ) .matchAny( x -&gt; unhandled(x) ) .build(); &#125;&#125; 客户端路由分发Actor 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103@Component@Scope("prototype")public class RouterActor extends AbstractLoggingActor &#123; // 服务集合 private List&lt;ActorRef&gt; servers = Lists.newArrayList(); // 集群监听 private Cluster cluster = Cluster.get(context().system()); private AtomicInteger count = new AtomicInteger(0); private String actorPath; public RouterActor(String actorPath) &#123; this.actorPath = actorPath; &#125; @Override public void preStart() throws Exception &#123; cluster.subscribe(self(), ClusterEvent.MemberEvent.class, ClusterEvent.ReachabilityEvent.class); &#125; @Override public void postStop() throws Exception &#123; cluster.unsubscribe(self()); &#125; @Override public Receive createReceive() &#123; return receiveBuilder() .match(Message.Task.class, x -&gt; servers.isEmpty(), x -&gt; sender().tell(new Message.Result(-1L, "服务暂不可用"), sender()) ) .match(Message.Task.class, x -&gt; &#123; log().info("[Dispatcher][消息转发] 集群节点数量:&#123;&#125;, 待处理任务:&#123;&#125;", servers.size(), x.getTaskId()); int index = count.incrementAndGet() % servers.size(); for(int i=0; i&lt;servers.size(); i++)&#123; log().info("[Dispatcher][服务选取] 选中[" + (i == index) + "] 地址: " + servers.get(i).path()); &#125; //System.out.println(); servers.get(index).forward(x, getContext()); &#125; ) .match(Terminated.class, x -&gt; unRegister(x.getActor()) ) .match(ClusterEvent.CurrentClusterState.class, state -&gt; &#123; // 当前节点在刚刚加入集群时，会收到CurrentClusterState消息，从中可以解析出集群中的所有前端节点 servers.clear(); for (Member member : state.getMembers()) &#123; if (member.status().equals(MemberStatus.up())) &#123; register(member); &#125; &#125; &#125;) .match(ClusterEvent.MemberUp.class, x -&gt; register(x.member()) ) .match(ClusterEvent.MemberEvent.class, x -&gt; unRegister(x.member()) ) .match(ClusterEvent.UnreachableMember.class, x -&gt; unRegister(x.member()) ) .match(ClusterEvent.ReachableMember.class, x -&gt; unRegister(x.member()) ).build(); &#125; public void register(Member member)&#123; if(null != member &amp;&amp; member.hasRole("server"))&#123; register(getContext().actorFor(member.address() + "/user/" + this.actorPath)); &#125; &#125; public void unRegister(Member member)&#123; if(null != member &amp;&amp; member.hasRole("server"))&#123; unRegister(getContext().actorFor(member.address() + "/user/" + this.actorPath)); &#125; &#125; public void register(ActorRef actorRef)&#123; if(null != actorRef)&#123; servers.add(actorRef); &#125; &#125; public void unRegister(ActorRef actorRef)&#123; if(null != actorRef)&#123; servers.remove(actorRef); &#125; &#125; public void setActorPath(String actorPath) &#123; this.actorPath = actorPath; &#125;&#125; 集群实例化服务端注解实例化 12345678910111213141516171819202122232425262728293031323334353637383940@Slf4j@Configurationpublic class ServerConfig &#123; @Autowired private ApplicationContext applicationContext; private Config config; @PostConstruct public void init()&#123; load("akka-server.conf"); &#125; private void load(String resource)&#123; config = ConfigFactory.load(this.getClass().getClassLoader(), resource); List&lt;String&gt; roles = config.getStringList("akka.cluster.roles"); log.info("[Cluster Client][Role] :&#123;&#125;", roles); &#125; @Bean("serverSystem") public ActorSystem actorSystem() &#123; ActorSystem system = ActorSystem.create("simpleAkkaCluster", config); // initialize the application context in the Akka Spring Extension SpringExtension.INSTANCE.get(system).initialize(applicationContext); log.info("[Cluster Client] [System] &#123;&#125;", system.name()); return system; &#125; @Bean("serverActor") public ActorRef actorRef()&#123; ActorSystem system = actorSystem(); ActorRef ebIndexActor = system.actorOf( SpringExtension.INSTANCE.get(system).props("simpleActor"), "SimpleEcho"); log.info("[Cluster Server Node] [Loaded] &#123;&#125; ", ebIndexActor.path()); return ebIndexActor; &#125;&#125; 客户端注解实例化 1234567891011121314151617181920212223242526272829303132333435363738@Slf4j@Configurationpublic class ClientConfig &#123; @Autowired private ApplicationContext applicationContext; private Config config; @PostConstruct public void init()&#123; load("akka-client.conf"); &#125; private void load(String resource)&#123; config = ConfigFactory.load(this.getClass().getClassLoader(), resource); List&lt;String&gt; roles = config.getStringList("akka.cluster.roles"); log.info("[Cluster Client][Role] :&#123;&#125;", roles); &#125; @Bean("clientSystem") public ActorSystem actorSystem() &#123; ActorSystem system = ActorSystem.create("simpleAkkaCluster", config); // initialize the application context in the Akka Spring Extension SpringExtension.INSTANCE.get(system).initialize(applicationContext); log.info("[Cluster Client] [System] &#123;&#125;", system.name()); return system; &#125; @Bean("clientActor") public ActorRef actorRef()&#123; ActorSystem system = actorSystem(); ActorRef clientActor = system.actorOf(SpringExtension.INSTANCE.get(system).props("routerActor", "SimpleEcho"), "clientActor"); log.info("[Cluster Client Node] [Loaded] &#123;&#125; ", clientActor.path()); return clientActor; &#125;&#125; Rest测试类1234567891011121314151617181920212223242526272829303132@Slf4j@RestController@RequestMapping("/api/akka")public class AkkaController &#123; @Resource(name = "clientActor") private ActorRef clientActor; /** * 和远程端进行数据交互并获取处理结果 * @throws Exception */ @GetMapping(value = "/cluster/event", produces = MediaType.APPLICATION_JSON_UTF8_VALUE) public ResponseEntity clusterEvent() throws Exception &#123; log.info("[Client] path:&#123;&#125;", clientActor.path()); List&lt;Future&lt;Object&gt;&gt; futures = Lists.newArrayList(); Timeout timeout = new Timeout(Duration.create(50, TimeUnit.SECONDS)); for(int i=0; i&lt;5; i++)&#123; String text = "集群消息" + i + UUID.randomUUID().toString(); Future&lt;Object&gt; future = Patterns.ask(clientActor, new Message.Task(100L +i, text), timeout); futures.add(future); &#125; for(Future&lt;Object&gt; future: futures)&#123; Message.Result result = (Message.Result) Await.result(future, timeout.duration()); System.out.println("服务端返回:" + result); &#125; return ResponseEntity.ok("test"); &#125;&#125; SpringBoot主类123456789@Slf4j@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) throws Exception&#123; SpringApplication.run(Application.class, args); log.info("[启动成功] 应用名称: &#123;&#125;", "Akka Spring Boot"); &#125;&#125; 访问测试启动SpringBoot应用 1234567891011122018-02-05 17:54:13.586 INFO 6523 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)2018-02-05 17:54:13.594 INFO 6523 --- [ main] com.elonsu.cluster.akka.Application : Started Application in 4.948 seconds (JVM running for 6.118)2018-02-05 17:54:13.594 INFO 6523 --- [ main] com.elonsu.cluster.akka.Application : [启动成功] 应用名称: Akka Spring Boot[INFO] [02/05/2018 17:54:17.602] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka://simpleAkkaCluster/deadLetters] Message [akka.cluster.InternalClusterAction$InitJoin$] from Actor[akka://simpleAkkaCluster/system/cluster/core/daemon/joinSeedNodeProcess-1#-1342745044] to Actor[akka://simpleAkkaCluster/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.[WARN] [02/05/2018 17:54:22.622] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/system/cluster/core/daemon/joinSeedNodeProcess-1] Couldn't join seed nodes after [2] attempts, will try again. seed-nodes=[akka.tcp://simpleAkkaCluster@127.0.0.1:2555][INFO] [02/05/2018 17:54:22.698] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@127.0.0.1:2555] - Received InitJoin message from [Actor[akka.tcp://simpleAkkaCluster@127.0.0.1:2556/system/cluster/core/daemon/joinSeedNodeProcess-1#-1342745044]] to [akka.tcp://simpleAkkaCluster@127.0.0.1:2555][INFO] [02/05/2018 17:54:22.698] [simpleAkkaCluster-akka.actor.default-dispatcher-3] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@127.0.0.1:2555] - Sending InitJoinAck message from node [akka.tcp://simpleAkkaCluster@127.0.0.1:2555] to [Actor[akka.tcp://simpleAkkaCluster@127.0.0.1:2556/system/cluster/core/daemon/joinSeedNodeProcess-1#-1342745044]][INFO] [02/05/2018 17:54:22.721] [simpleAkkaCluster-akka.actor.default-dispatcher-19] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@127.0.0.1:2555] - Node [akka.tcp://simpleAkkaCluster@127.0.0.1:2556] is JOINING, roles [router, client, dc-default][INFO] [02/05/2018 17:54:22.723] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] ignore[INFO] [02/05/2018 17:54:22.726] [simpleAkkaCluster-akka.actor.default-dispatcher-19] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@127.0.0.1:2555] - Leader is moving node [akka.tcp://simpleAkkaCluster@127.0.0.1:2556] to [Up][INFO] [02/05/2018 17:54:22.726] [simpleAkkaCluster-akka.actor.default-dispatcher-24] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] Member is Up: Member(address = akka.tcp://simpleAkkaCluster@127.0.0.1:2556, status = Up)[INFO] [02/05/2018 17:54:22.828] [simpleAkkaCluster-akka.actor.default-dispatcher-15] [akka.cluster.Cluster(akka://simpleAkkaCluster)] Cluster Node [akka.tcp://simpleAkkaCluster@127.0.0.1:2556] - Welcome from [akka.tcp://simpleAkkaCluster@127.0.0.1:2555 访问接口 12[root@localhost spring-akka ]$ curl http://localhost:8080/api/akka/cluster/eventtest 交互日志 12345678910111213141516171819202122232425262018-02-05 17:55:37.462 INFO 6523 --- [nio-8080-exec-9] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring FrameworkServlet 'dispatcherServlet'2018-02-05 17:55:37.462 INFO 6523 --- [nio-8080-exec-9] o.s.web.servlet.DispatcherServlet : FrameworkServlet 'dispatcherServlet': initialization started2018-02-05 17:55:37.490 INFO 6523 --- [nio-8080-exec-9] o.s.web.servlet.DispatcherServlet : FrameworkServlet 'dispatcherServlet': initialization completed in 28 ms2018-02-05 17:55:37.523 INFO 6523 --- [nio-8080-exec-9] c.e.c.a.s.controller.AkkaController : [Client] path:akka://simpleAkkaCluster/user/clientActor[INFO] [02/05/2018 17:55:37.526] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][消息转发] 集群节点数量:1, 待处理任务:100[INFO] [02/05/2018 17:55:37.526] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho[INFO] [02/05/2018 17:55:37.527] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][消息转发] 集群节点数量:1, 待处理任务:101[INFO] [02/05/2018 17:55:37.527] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho[INFO] [02/05/2018 17:55:37.527] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][消息转发] 集群节点数量:1, 待处理任务:102[INFO] [02/05/2018 17:55:37.527] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho[INFO] [02/05/2018 17:55:37.527] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][消息转发] 集群节点数量:1, 待处理任务:103[INFO] [02/05/2018 17:55:37.527] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho[INFO] [02/05/2018 17:55:37.527] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][消息转发] 集群节点数量:1, 待处理任务:104[INFO] [02/05/2018 17:55:37.527] [simpleAkkaCluster-akka.actor.default-dispatcher-4] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho[WARN] [SECURITY][02/05/2018 17:55:37.528] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-18] [akka.serialization.Serialization(akka://simpleAkkaCluster)] Using the default Java serializer for class [com.elonsu.cluster.akka.spring.actor.Message$Task] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'[INFO] [02/05/2018 17:55:37.616] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] [接收消息]:&#123;"content":"集群消息0526add9d-29ba-409f-ae74-cc7b61606613","taskId":100&#125;, Actor:akka://simpleAkkaCluster/user/SimpleEcho, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-16[INFO] [02/05/2018 17:55:37.616] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] [接收消息]:&#123;"content":"集群消息16d03f841-67d4-4169-b747-dc0a8f2e02dd","taskId":101&#125;, Actor:akka://simpleAkkaCluster/user/SimpleEcho, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-16[INFO] [02/05/2018 17:55:37.616] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] [接收消息]:&#123;"content":"集群消息25c2c2579-71c5-45a8-97e6-434a95a186df","taskId":102&#125;, Actor:akka://simpleAkkaCluster/user/SimpleEcho, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-16[INFO] [02/05/2018 17:55:37.617] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] [接收消息]:&#123;"content":"集群消息3f4ad2fe3-d624-416d-a01a-e346914fcbbb","taskId":103&#125;, Actor:akka://simpleAkkaCluster/user/SimpleEcho, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-16[INFO] [02/05/2018 17:55:37.617] [simpleAkkaCluster-akka.actor.default-dispatcher-16] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] [接收消息]:&#123;"content":"集群消息421b5b619-345d-4eb0-8682-df8955ea70df","taskId":104&#125;, Actor:akka://simpleAkkaCluster/user/SimpleEcho, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-16[WARN] [SECURITY][02/05/2018 17:55:37.617] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-21] [akka.serialization.Serialization(akka://simpleAkkaCluster)] Using the default Java serializer for class [com.elonsu.cluster.akka.spring.actor.Message$Result] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'服务端返回:&#123;"result":"集群消息0526add9d-29ba-409f-ae74-cc7b61606613处理完成","taskId":100&#125;服务端返回:&#123;"result":"集群消息16d03f841-67d4-4169-b747-dc0a8f2e02dd处理完成","taskId":101&#125;服务端返回:&#123;"result":"集群消息25c2c2579-71c5-45a8-97e6-434a95a186df处理完成","taskId":102&#125;服务端返回:&#123;"result":"集群消息3f4ad2fe3-d624-416d-a01a-e346914fcbbb处理完成","taskId":103&#125;服务端返回:&#123;"result":"集群消息421b5b619-345d-4eb0-8682-df8955ea70df处理完成","taskId":104&#125; 集群部署上面示例演示的在同一台机器上的不同端口进行通讯. 接下来演示在多台机器上的不同配置 服务端配置 123456789101112131415161718192021222324252627akka &#123; loglevel = "INFO" actor &#123; provider = "akka.cluster.ClusterActorRefProvider" &#125; remote &#123; log-remote-lifecycle-events = on netty.tcp &#123; port = 2555 &#125; &#125; cluster &#123; roles = ["server"] seed-nodes = [ "akka.tcp://simpleAkkaCluster@192.168.0.101:2555", "akka.tcp://simpleAkkaCluster@192.168.0.102:2555", "akka.tcp://simpleAkkaCluster@192.168.0.103:2555"] auto-down-unreachable-after = 1000s auto-down-unreachable-after = 1000s metrics.enabled = off jmx.multi-mbeans-in-same-jvm = on &#125;&#125; 客户端配置 12345678910111213141516171819202122232425262728akka &#123; loglevel = "INFO" actor &#123; provider = "akka.cluster.ClusterActorRefProvider" &#125; remote &#123; log-remote-lifecycle-events = on netty.tcp &#123; port = 2556 &#125; &#125; cluster &#123; roles = ["router", "client"] seed-nodes = [ "akka.tcp://simpleAkkaCluster@192.168.0.101:2555", "akka.tcp://simpleAkkaCluster@192.168.0.102:2555", "akka.tcp://simpleAkkaCluster@192.168.0.103:2555"] auto-down-unreachable-after = 1000s auto-down-unreachable-after = 1000s metrics.enabled = off jmx.multi-mbeans-in-same-jvm = on &#125;&#125; 相对于单节点配置,不同的是示例中服务节点都配置为不同IP, 同时摘掉了绑定的akka.remote.netty.tcp.hostname, 让Akka自动获取实例的通讯IP进行绑定. 配置Dispatcher客户端配置重定义默认的default-dispatcher配置thread-pool默认默认 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748akka &#123; loglevel = "INFO" actor &#123; provider = "akka.cluster.ClusterActorRefProvider" # default dispatcher default-dispatcher &#123; type = "Dispatcher" # What kind of ExecutionService to use executor = "thread-pool-executor" # Configuration for the thread pool thread-pool-executor &#123; # minimum number of threads to cap factor-based core number to core-pool-size-min = 3 # No of core threads ... ceil(available processors * factor) core-pool-size-factor = 2.0 # maximum number of threads to cap factor-based number to core-pool-size-max = 10 &#125; # Throughput defines the maximum number of messages to be # processed per actor before the thread jumps to the next actor. # Set to 1 for as fair as possible. throughput = 20 &#125; &#125; remote &#123; log-remote-lifecycle-events = on netty.tcp &#123; hostname = "127.0.0.1" port = 2556 &#125; &#125; cluster &#123; roles = ["router", "client"] seed-nodes = [ "akka.tcp://simpleAkkaCluster@127.0.0.1:2555"] auto-down-unreachable-after = 1000s auto-down-unreachable-after = 1000s metrics.enabled = off jmx.multi-mbeans-in-same-jvm = on &#125;&#125; 执行日志 12345678910111213141516171819202122232425262018-02-06 11:49:28.222 INFO 2218 --- [nio-8080-exec-7] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring FrameworkServlet 'dispatcherServlet'2018-02-06 11:49:28.223 INFO 2218 --- [nio-8080-exec-7] o.s.web.servlet.DispatcherServlet : FrameworkServlet 'dispatcherServlet': initialization started2018-02-06 11:49:28.238 INFO 2218 --- [nio-8080-exec-7] o.s.web.servlet.DispatcherServlet : FrameworkServlet 'dispatcherServlet': initialization completed in 15 ms2018-02-06 11:49:28.260 INFO 2218 --- [nio-8080-exec-7] c.e.c.a.s.controller.AkkaController : [Client] path:akka://simpleAkkaCluster/user/clientActor[INFO] [02/06/2018 11:49:28.262] [simpleAkkaCluster-akka.actor.default-dispatcher-9] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][消息转发] 集群节点数量:1, 待处理任务:100[INFO] [02/06/2018 11:49:28.262] [simpleAkkaCluster-akka.actor.default-dispatcher-9] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho[INFO] [02/06/2018 11:49:28.262] [simpleAkkaCluster-akka.actor.default-dispatcher-9] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][消息转发] 集群节点数量:1, 待处理任务:101[INFO] [02/06/2018 11:49:28.262] [simpleAkkaCluster-akka.actor.default-dispatcher-9] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho[INFO] [02/06/2018 11:49:28.262] [simpleAkkaCluster-akka.actor.default-dispatcher-9] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][消息转发] 集群节点数量:1, 待处理任务:102[INFO] [02/06/2018 11:49:28.262] [simpleAkkaCluster-akka.actor.default-dispatcher-9] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho[INFO] [02/06/2018 11:49:28.262] [simpleAkkaCluster-akka.actor.default-dispatcher-9] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][消息转发] 集群节点数量:1, 待处理任务:103[INFO] [02/06/2018 11:49:28.262] [simpleAkkaCluster-akka.actor.default-dispatcher-9] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho[INFO] [02/06/2018 11:49:28.262] [simpleAkkaCluster-akka.actor.default-dispatcher-9] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][消息转发] 集群节点数量:1, 待处理任务:104[INFO] [02/06/2018 11:49:28.262] [simpleAkkaCluster-akka.actor.default-dispatcher-9] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho[WARN] [SECURITY][02/06/2018 11:49:28.263] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-11] [akka.serialization.Serialization(akka://simpleAkkaCluster)] Using the default Java serializer for class [com.elonsu.cluster.akka.spring.actor.Message$Task] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'[INFO] [02/06/2018 11:49:28.329] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] [接收消息]:&#123;"content":"集群消息06ab614d0-f50d-4c6c-b91d-7b5af1e98187","taskId":100&#125;, Actor:akka://simpleAkkaCluster/user/SimpleEcho, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-22[INFO] [02/06/2018 11:49:28.330] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] [接收消息]:&#123;"content":"集群消息1057874e5-1219-4bea-81f5-5612707bf83d","taskId":101&#125;, Actor:akka://simpleAkkaCluster/user/SimpleEcho, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-22[INFO] [02/06/2018 11:49:28.330] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] [接收消息]:&#123;"content":"集群消息2467e56fa-e2dd-493a-a583-cf0496bbb96f","taskId":102&#125;, Actor:akka://simpleAkkaCluster/user/SimpleEcho, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-22[INFO] [02/06/2018 11:49:28.330] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] [接收消息]:&#123;"content":"集群消息3ccad0171-99b4-4384-b836-72115304f77a","taskId":103&#125;, Actor:akka://simpleAkkaCluster/user/SimpleEcho, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-22[INFO] [02/06/2018 11:49:28.330] [simpleAkkaCluster-akka.actor.default-dispatcher-22] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] [接收消息]:&#123;"content":"集群消息427786c84-4971-4f4d-9c52-a4975b9548bf","taskId":104&#125;, Actor:akka://simpleAkkaCluster/user/SimpleEcho, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-22[WARN] [SECURITY][02/06/2018 11:49:28.330] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-5] [akka.serialization.Serialization(akka://simpleAkkaCluster)] Using the default Java serializer for class [com.elonsu.cluster.akka.spring.actor.Message$Result] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'服务端返回:&#123;"result":"集群消息06ab614d0-f50d-4c6c-b91d-7b5af1e98187处理完成","taskId":100&#125;服务端返回:&#123;"result":"集群消息1057874e5-1219-4bea-81f5-5612707bf83d处理完成","taskId":101&#125;服务端返回:&#123;"result":"集群消息2467e56fa-e2dd-493a-a583-cf0496bbb96f处理完成","taskId":102&#125;服务端返回:&#123;"result":"集群消息3ccad0171-99b4-4384-b836-72115304f77a处理完成","taskId":103&#125;服务端返回:&#123;"result":"集群消息427786c84-4971-4f4d-9c52-a4975b9548bf处理完成","taskId":104&#125; 配置fork-join默认默认 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748akka &#123; loglevel = "INFO" actor &#123; provider = "akka.cluster.ClusterActorRefProvider" # self defined dispatcher default-dispatcher &#123; type = "Dispatcher" # What kind of ExecutionService to use executor = "fork-join-executor" # Configuration for the fork join pool fork-join-executor &#123; # Min number of threads to cap factor-based parallelism number to parallelism-min = 2 # Parallelism (threads) ... ceil(available processors * factor) parallelism-factor = 2.0 # Max number of threads to cap factor-based parallelism number to parallelism-max = 10 &#125; # Throughput defines the maximum number of messages to be # processed per actor before the thread jumps to the next actor. # Set to 1 for as fair as possible. throughput = 20 &#125; &#125; remote &#123; log-remote-lifecycle-events = on netty.tcp &#123; hostname = "127.0.0.1" port = 2556 &#125; &#125; cluster &#123; roles = ["router", "client"] seed-nodes = [ "akka.tcp://simpleAkkaCluster@127.0.0.1:2555"] auto-down-unreachable-after = 1000s auto-down-unreachable-after = 1000s metrics.enabled = off jmx.multi-mbeans-in-same-jvm = on &#125;&#125; 客户端配置自定义Dispatcher123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657akka &#123; loglevel = "INFO" actor &#123; provider = "akka.cluster.ClusterActorRefProvider" # self defined dispatcher thread-pool-dispatcher &#123; type = "Dispatcher" # What kind of ExecutionService to use executor = "thread-pool-executor" # Configuration for the thread pool thread-pool-executor &#123; # minimum number of threads to cap factor-based core number to core-pool-size-min = 3 # No of core threads ... ceil(available processors * factor) core-pool-size-factor = 2.0 # maximum number of threads to cap factor-based number to core-pool-size-max = 10 &#125; # Throughput defines the maximum number of messages to be # processed per actor before the thread jumps to the next actor. # Set to 1 for as fair as possible. throughput = 20 &#125; deployment &#123; /clientActor &#123; # 使用自定义的Dispatcher dispatcher = "akka.actor.thread-pool-dispatcher" &#125; &#125; &#125; remote &#123; log-remote-lifecycle-events = on netty.tcp &#123; hostname = "127.0.0.1" port = 2556 &#125; &#125; cluster &#123; roles = ["router", "client"] seed-nodes = [ "akka.tcp://simpleAkkaCluster@127.0.0.1:2555"] auto-down-unreachable-after = 1000s auto-down-unreachable-after = 1000s metrics.enabled = off jmx.multi-mbeans-in-same-jvm = on &#125;&#125; 执行输出 12345678910111213141516171819202122232425262018-02-06 11:55:59.839 INFO 2301 --- [nio-8080-exec-3] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring FrameworkServlet 'dispatcherServlet'2018-02-06 11:55:59.839 INFO 2301 --- [nio-8080-exec-3] o.s.web.servlet.DispatcherServlet : FrameworkServlet 'dispatcherServlet': initialization started2018-02-06 11:55:59.852 INFO 2301 --- [nio-8080-exec-3] o.s.web.servlet.DispatcherServlet : FrameworkServlet 'dispatcherServlet': initialization completed in 13 ms2018-02-06 11:55:59.872 INFO 2301 --- [nio-8080-exec-3] c.e.c.a.s.controller.AkkaController : [Client] path:akka://simpleAkkaCluster/user/clientActor[INFO] [02/06/2018 11:55:59.873] [simpleAkkaCluster-akka.actor.thread-pool-dispatcher-26] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][消息转发] 集群节点数量:1, 待处理任务:100[INFO] [02/06/2018 11:55:59.873] [simpleAkkaCluster-akka.actor.thread-pool-dispatcher-26] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho[INFO] [02/06/2018 11:55:59.874] [simpleAkkaCluster-akka.actor.thread-pool-dispatcher-26] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][消息转发] 集群节点数量:1, 待处理任务:101[INFO] [02/06/2018 11:55:59.874] [simpleAkkaCluster-akka.actor.thread-pool-dispatcher-26] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho[INFO] [02/06/2018 11:55:59.874] [simpleAkkaCluster-akka.actor.thread-pool-dispatcher-26] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][消息转发] 集群节点数量:1, 待处理任务:102[INFO] [02/06/2018 11:55:59.874] [simpleAkkaCluster-akka.actor.thread-pool-dispatcher-26] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho[INFO] [02/06/2018 11:55:59.874] [simpleAkkaCluster-akka.actor.thread-pool-dispatcher-26] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][消息转发] 集群节点数量:1, 待处理任务:103[INFO] [02/06/2018 11:55:59.874] [simpleAkkaCluster-akka.actor.thread-pool-dispatcher-26] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho[INFO] [02/06/2018 11:55:59.874] [simpleAkkaCluster-akka.actor.thread-pool-dispatcher-26] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][消息转发] 集群节点数量:1, 待处理任务:104[INFO] [02/06/2018 11:55:59.874] [simpleAkkaCluster-akka.actor.thread-pool-dispatcher-26] [akka.tcp://simpleAkkaCluster@127.0.0.1:2556/user/clientActor] [Dispatcher][服务选取] 选中[true] 地址: akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho[WARN] [SECURITY][02/06/2018 11:55:59.874] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-5] [akka.serialization.Serialization(akka://simpleAkkaCluster)] Using the default Java serializer for class [com.elonsu.cluster.akka.spring.actor.Message$Task] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'[INFO] [02/06/2018 11:55:59.926] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] [接收消息]:&#123;"content":"集群消息0a860813a-d7c3-4a2a-b93a-35b034016731","taskId":100&#125;, Actor:akka://simpleAkkaCluster/user/SimpleEcho, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-2[INFO] [02/06/2018 11:55:59.926] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] [接收消息]:&#123;"content":"集群消息1c3148d4d-de3f-4483-b668-19b608b8d83d","taskId":101&#125;, Actor:akka://simpleAkkaCluster/user/SimpleEcho, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-2[INFO] [02/06/2018 11:55:59.926] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] [接收消息]:&#123;"content":"集群消息26707b125-a867-484a-9c5c-21a085d31b89","taskId":102&#125;, Actor:akka://simpleAkkaCluster/user/SimpleEcho, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-2[INFO] [02/06/2018 11:55:59.926] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] [接收消息]:&#123;"content":"集群消息334e7fdd0-c3cc-4192-8ee5-89f28c1dbe28","taskId":103&#125;, Actor:akka://simpleAkkaCluster/user/SimpleEcho, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-2[INFO] [02/06/2018 11:55:59.927] [simpleAkkaCluster-akka.actor.default-dispatcher-2] [akka.tcp://simpleAkkaCluster@127.0.0.1:2555/user/SimpleEcho] [接收消息]:&#123;"content":"集群消息450ee1130-33fb-4f23-9372-34cff2d52488","taskId":104&#125;, Actor:akka://simpleAkkaCluster/user/SimpleEcho, Thread:simpleAkkaCluster-akka.actor.default-dispatcher-2[WARN] [SECURITY][02/06/2018 11:55:59.926] [simpleAkkaCluster-akka.remote.default-remote-dispatcher-16] [akka.serialization.Serialization(akka://simpleAkkaCluster)] Using the default Java serializer for class [com.elonsu.cluster.akka.spring.actor.Message$Result] which is not recommended because of performance implications. Use another serializer or disable this warning using the setting 'akka.actor.warn-about-java-serializer-usage'服务端返回:&#123;"result":"集群消息0a860813a-d7c3-4a2a-b93a-35b034016731处理完成","taskId":100&#125;服务端返回:&#123;"result":"集群消息1c3148d4d-de3f-4483-b668-19b608b8d83d处理完成","taskId":101&#125;服务端返回:&#123;"result":"集群消息26707b125-a867-484a-9c5c-21a085d31b89处理完成","taskId":102&#125;服务端返回:&#123;"result":"集群消息334e7fdd0-c3cc-4192-8ee5-89f28c1dbe28处理完成","taskId":103&#125;服务端返回:&#123;"result":"集群消息450ee1130-33fb-4f23-9372-34cff2d52488处理完成","taskId":104&#125; 相关文档 https://doc.akka.io/docs/akka/current/general/configuration.html]]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Akka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Web應用會話管理]]></title>
    <url>%2F2017%2F01%2F21%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2Fskill-tookit-web-session-manage%2F</url>
    <content type="text"><![CDATA[Session和Cookie由于HTTP协议是无状态的协议,所以服务端需要记录用户的状态时,就需要用某种机制来识具体的用户,这个机制就是Session。 Session是保存在服务端的,有一个唯一标识。在大型的网站,一般会有专门的Session服务器集群,这个时候 Session 信息都是放在内存的。 思考一下服务端如何识别特定的客户？这个时候Cookie就登场了。每次HTTP请求的时候,客户端都会发送相应的Cookie信息到服务端。 实际上大多数的应用都是用 Cookie 来实现Session跟踪的,第一次创建Session的时候,服务端会在HTTP协议中告诉客户端, 需要在 Cookie 里面记录一个Session ID,以后每次请求把这个会话ID发送到服务器,我就知道你是谁了。 有人问,如果客户端的浏览器禁用了 Cookie 怎么办？一般这种情况下,会使用一种叫做URL重写的技术来进行会话跟踪, 即每次HTTP交互,URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数,服务端据此来识别用户。 Cookie其实还可以用在一些方便用户的场景下,设想你某次登陆过一个网站,下次登录的时候不想再次输入账号了,怎么办？ 这个信息可以写到Cookie里面,访问网站的时候,网站页面的脚本可以读取这个信息,就自动帮你把用户名给填了,能够方便一下用户。 这也是Cookie名称的由来,给用户的一点甜头。 所以,总结一下： session 在服务器端,cookie 在客户端（浏览器） session 默认被存在在服务器的一个文件里（不是内存） session 的运行依赖 session id,而 session_id是存在cookie中的,也就是说,如果浏览器禁用了cookie, 同时session也会失效（但是可以通过其它方式实现,比如在 url 中传递 session_id） session 可以放在 文件、数据库、或内存中都可以。 用户验证这种场合一般会用 session 因此,维持一个会话的核心就是客户端的唯一标识,即 session id 种常见的实现web应用会话管理的方式： 基于server端session的管理方式 基于cookie的管理方式 基于token的管理方式 基于Server端session在早期web应用中,通常使用服务端session来管理用户的会话。 1）服务端session是用户第一次访问应用时,服务器就会创建的对象,代表用户的一次会话过程,可以用来存放数据。 服务器为每一个session都分配一个唯一的sessionid,以保证每个用户都有一个不同的session对象。 2）服务器在创建完session后,会把sessionid通过cookie返回给用户所在的浏览器,这样当用户第二次及以后向服务器发送请求的时候, 就会通过cookie把sessionid传回给服务器,以便服务器能够根据sessionid找到与该用户对应的session对象。 3）session通常有失效时间的设定,比如2个小时。当失效时间到,服务器会销毁之前的session,并创建新的session返回给用户。 但是只要用户在失效时间内,有发送新的请求给服务器,通常服务器都会把他对应的session的失效时间根据当前的请求时间再延长2个小时。 4）session在一开始并不具备会话管理的作用。它只有在用户登录认证成功之后,并且往sesssion对象里面放入了用户登录成功的凭证, 才能用来管理会话。管理会话的逻辑也很简单,只要拿到用户的session对象,看它里面有没有登录成功的凭证,就能判断这个用户是否已经登录。 当用户主动退出的时候,会把它的session对象里的登录凭证清掉。 所以在用户登录前或退出后或者session对象失效时,肯定都是拿不到需要的登录凭证的。 可简单使用流程图描述如下： 主流的web开发平台都原生支持这种会话管理的方式,而且开发起来很简单。它还有一个比较大的优点就是安全性好, 因为在浏览器端与服务器端保持会话状态的媒介始终只是一个sessionid串,只要这个串够随机,攻击者就不能轻易冒充他人的sessionid进行操作； 除非通过CSRF或http劫持的方式,才有可能冒充别人进行操作；即使冒充成功,也必须被冒充的用户session里面包含有效的登录凭证才行。 但是这种方式也有几个问题需要解决： 1）这种方式将会话信息存储在web服务器里面,所以在用户同时在线量比较多时,这些会话信息会占据比较多的内存； 2）当应用采用集群部署的时候,会遇到多台web服务器之间如何做session共享的问题。因为session是由单个服务器创建的, 但是处理用户请求的服务器不一定是那个创建session的服务器,这样他就拿不到之前已经放入到session中的登录凭证之类的信息了； 3）多个应用要共享session时,除了以上问题,还会遇到跨域问题,因为不同的应用可能部署的主机不一样,需要在各个应用做好cookie跨域的处理。 针对问题1和问题2,我见过的解决方案是采用redis/memcached这种中间服务器来管理session的增删改查, 一来减轻web服务器的负担,二来解决不同web服务器共享session的问题。 针对问题3,由于服务端的session依赖cookie来传递sessionid,所以在实际项目中,只要解决各个项目里面如何实现sessionid的cookie跨域访问即可, 这个是可以实现的,就是比较麻烦,前后端有可能都要做处理。 如果在一些小型的web应用中使用,可以不用考虑上面三个问题,所以很适合这种方式。 基于cookie由于前一种方式会增加服务器的负担和架构的复杂性,所以后来就有人想出直接把用户的登录凭证直接存到客户端的方案, 当用户登录成功之后,把登录凭证写到cookie里面,并给cookie设置有效期,后续请求直接验证存有登录凭证的cookie是否存在以及凭证是否有效, 即可判断用户的登录状态。使用它来实现会话管理的整体流程如下： 1）用户发起登录请求,服务端根据传入的用户密码之类的身份信息,验证用户是否满足登录条件,如果满足,就根据用户信息创建一个登录凭证, 这个登录凭证简单来说就是一个对象,最简单的形式可以只包含用户id,凭证创建时间和过期时间三个值。 2）服务端把上一步创建好的登录凭证,先对它做数字签名,然后再用对称加密算法做加密处理,将签名、加密后的字串,写入cookie。 cookie的名字必须固定（如ticket）,因为后面再获取的时候,还得根据这个名字来获取cookie值。 这一步添加数字签名的目的是防止登录凭证里的信息被篡改,因为一旦信息被篡改,那么下一步做签名验证的时候肯定会失败。 做加密的目的,是防止cookie被别人截取的时候,无法轻易读到其中的用户信息。 3）用户登录后发起后续请求,服务端根据上一步存登录凭证的cookie名字,获取到相关的cookie值。然后先做解密处理,再做数字签名的认证, 如果这两步都失败,说明这个登录凭证非法；如果这两步成功,接着就可以拿到原始存入的登录凭证了。然后用这个凭证的过期时间和当前时间做对比, 判断凭证是否过期,如果过期,就需要用户再重新登录；如果未过期,则允许请求继续。 可简单使用流程图描述如下： 它的缺点也比较明显： 1）cookie有大小限制,存储不了太多数据 2）每次传送cookie,增加了请求的数量,对访问性能也有影响； 3）也有跨域问题,毕竟还是要用cookie。 相比起第一种方式,基于cookie方案明显还是要好一些,目前好多web开发平台或框架都默认使用这种方式来做会话管理。 跨域的问题可以用CORS（跨域资源共享）的方式来快速解决。 基于token前面两种会话管理方式因为都用到cookie,不适合用在移动端native app里面,native app不好管理cookie,毕竟它不是浏览器。 这两种方案都不适合用来做纯api服务的登录认证,就要考虑第三种会话管理方式,也就是token认证。 这种方式从流程和实现上来说,跟cookie-based的方式没有太多区别,只不过cookie-based里面写到cookie里面的ticket在这种方式下称为token, 这个token在返回给客户端之后,后续请求都必须通过url参数或者是http header的形式,主动带上token, 这样服务端接收到请求之后就能直接从http header或者url里面取到token进行验证： 这种方式不通过cookie进行token的传递,而是每次请求的时候,主动把token加到http header里面或者url后面, 所以即使在native app里面也能使用它来调用我们通过web发布的api接口。app里面还要做两件事情： 1）有效存储token,得保证每次调接口的时候都能从同一个位置拿到同一个token； 2）每次调接口的的代码里都得把token加到header或者接口地址里面。 可简单使用流程图描述如下： 这种方式同样适用于网页应用,token可以存于localStorage或者sessionStorage里面,然后每发ajax请求的时候, 都把token拿出来放到ajax请求的header里即可。不过如果是非接口的请求,比如直接通过点击链接请求一个页面这种, 是无法自动带上token的。所以这种方式也仅限于走纯接口的web应用。 基于token的标准实现: JWT 现在SPA应用,前后端完全分离,基于API接口的应用越来越多,这时候基于token的认证就是最好的选择方式了。 好在这个方式的技术其实早就有很多实现了,而且还有现成的标准可用,这个标准就是JWT(json-web-token)。 JWT本身并没有做任何技术实现,它只是定义了token-based的管理方式该如何实现, 它规定了token的应该包含的标准内容以及token的生成过程和方法。目前实现了这个标准的技术已经有非常多： 官方网站：https://jwt.io/#libraries-ioGit主页：https://github.com/auth0/java-jwt]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>会话</tag>
        <tag>Session</tag>
        <tag>Cookie</tag>
        <tag>Jwt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装配置 - Weex]]></title>
    <url>%2F2017%2F01%2F10%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE-Weex%2F</url>
    <content type="text"><![CDATA[Weex介绍 Weex 是一套简单易用的跨平台开发方案,能以web的开发体验构建高性能、可扩展的native应用,出自阿里团队. Weex安装安装前置条件: Weex安装前需要安装Node环境,点此下载 检测Node环境1234[elonsu@localhost Elonsu ]$ node -vv7.3.0[elonsu@localhost Elonsu ]$ npm -v3.10.10 Npm安装weex npm 是一个 JavaScript 包管理工具,它可以让开发者轻松共享和重用代码。 1[elonsu@localhost Elonsu ]$ npm install -g week-toolkit 国内开发者可以考虑使用淘宝的npm镜像cnpm安装weex-toolkit 12[elonsu@localhost Elonsu ]$ sudo npm install -g cnpm[elonsu@localhost Elonsu ]$ sudo cnpm install -g weex-toolkit 安装结束后你可以直接使用weex命令验证是否安装成功,它会显示weex命令行工具各参数： 1[elonsu@localhost Elonsu ]$ weex Weex开篇HelloWorld示例程序在上面安装正常的情况下, 创建我们的示例hello.we文件; 文件内容如下: 12345678910111213&lt;template&gt; &lt;div&gt; &lt;text class="text" style="color: #FF0000;"&gt;Hello world&lt;/text&gt; &lt;/div&gt;&lt;/template&gt;&lt;style&gt; .text&#123; font-size: 50; &#125;&lt;/style&gt;&lt;script&gt;&lt;/script&gt; 说明: &lt;template&gt;、&lt;style&gt; 、&lt;script&gt; 分别对应于Web中的 HTML,CSS(&lt;style&gt; 标签),JavaScript(&lt;script&gt; 标签)。注意: Weex遵循HTML特性命名规范,所以特性命名时请不要使用陀峰格式(CamelCase),采用以“-”分割的 long-name 形式。 编译运行123[elonsu@localhost Elonsu ]$ weex hello.weinfo Sat Jan 07 2017 17:27:24 GMT+0800 (CST)WebSocket is listening on port 8082info Sat Jan 07 2017 17:27:24 GMT+0800 (CST)http is listening on port 8081 执行结果: 我们会在浏览器页面中看到执行成功后页面,显示红色的”Hello world”效果. 终端预览手机端预览效果需要安装Playground App(https://weex-project.io/download.html)我们在编译运行的命令后面增加--qr参数,表示生成二维码。 1[elonsu@localhost Elonsu ]$ weex hello.we --qr 执行上述命令,会在控制台生成一个二维码, 手机端使用Playground App扫描二维码即可看到效果. 开发调试1[elonsu@localhost Elonsu ]$ weex debug hello.we 创建工程12345678910[elonsu@localhost Elonsu ]$ mkdir wuyu-platform-weex[elonsu@localhost Elonsu ]$ cd wuyu-platform-weex[elonsu@localhost wuyu-platform-weex ]$ weex initprompt: Project Name: (wuyu-platform-weex)file: .gitignore created.file: README.md created.file: index.html created.file: package.json created.file: src/weex-bootstrap.we created.file: webpack.config.js created. 上面操作我们创建了一个工程,工程名叫wuyu-platform-weex.接下来执行工程依赖包安装,以及编译并运行. 123[elonsu@localhost wuyu-platform-weex ]$ npm install[elonsu@localhost wuyu-platform-weex ]$ npm run dev[elonsu@localhost wuyu-platform-weex ]$ npm run serve Weex开发套件WeexpackWeexpack 是 Weex 新一代的工程开发套件,它允许开发者通过简单的命令,创建 weex 工程项目,将项目运行在不同的开发平台上。 Weexpack安装Weexpack安装使用命令npm install weexpack -g或者cnpm install weexpack -g, 这里我们使用淘宝镜像。 1[elonsu@localhost Elonsu ]$ sudo cnpm install weexpack -g Weexpack初始化工程我们创建一个工程wuyu-weex,使用weexpack init 工程名 会自动创建一个以工程名命名的文件夹,并进行工程初始化. 12[elonsu@localhost Elonsu ]$ weexpack init wuyu-weex=&gt; Initialize a new Weex app (wuyu-weex) 进入工程安装依赖 12[elonsu@localhost Elonsu ]$ cd wuyu-weex[elonsu@localhost wuyu-weex ]$ npm install Weexpack初始化工程说明Tree形结构展示目录结构, 笔者在Mac环境下.使用如下命令展示tree形目录结构 1[elonsu@localhost wuyu-weex ]$ find . -print | sed -e 's;[^/]*/;|____;g;s;____|; |;g' Weexpack初始化的后面结构大致如下: 12345678910111213141516171819-&gt; /wuyu-weex.|—— .gitignore|—— README.md|—— package.json|-- android.config.json|-- ios.config.json|—— webpack.config.js|—— /src| |—— index.we|—— /html5| |—— index.html|—— /ios| |—— /playground| |—— /sdk| |—— /WXDevtool|—— /android| |—— /playground| |—— /appframework 目录结构说明: webpack.config.js 是 webpack 配置文件,用于生成 .we 文件的 JSBunlde ios.config.json 是 iOS 项目配置文件 android.config.json 是 Android 项目配置文件 /src 目录放置 Weex 页面 /html5 是 H5 端入口文件 /ios 放置 iOS 项目 /android 放置 Android 项目 工程运行与打包如果一切运行正常, 便可以使用weexpack打包或模拟器运行了. Android平台Android下打包和构建是一体的 1[elonsu@localhost wuyu-weex ]$ weexpack run android Ios平台Ios下运行 1[elonsu@localhost wuyu-weex ]$ weexpack run ios Ios下打包1[elonsu@localhost wuyu-weex ]$ weexpack build ios]]></content>
      <categories>
        <category>运维部署</category>
      </categories>
      <tags>
        <tag>App</tag>
        <tag>Weex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java日期常用示例]]></title>
    <url>%2F2016%2F12%2F30%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2Fskill-tutorial-java-date-locale-date-using%2F</url>
    <content type="text"><![CDATA[日期格式化java.util.date和java.time.LocalDateTime格式化 应用示例1234567891011121314151617181920212223242526272829/*** 格式化日期* @param date 待格式化的日期* @param pattern 格式化正则* @return 格式化结果串*/public static String format(Date date, String pattern)&#123; return new SimpleDateFormat(pattern).format(date);&#125;/*** 格式化日期* @param localDateTime 待格式化的日期* @param pattern 格式化正式* @return 格式化结果串*/public static String format(LocalDateTime localDateTime, String pattern)&#123; return localDateTime.format(DateTimeFormatter.ofPattern(pattern));&#125;/*** 格式化日期* @param localDate 待格式化的日期* @param pattern 格式化正则, 这里使用的类型 &#123;@link LocalDate&#125;, 所以正则只能设定到天* @return 格式化结果串*/public static String format(LocalDate localDate, String pattern)&#123; return localDate.format(DateTimeFormatter.ofPattern(pattern));&#125; 示例测试123456// 2017-08-28 15:45:02System.out.println(format(new Date(), "yyyy-MM-dd HH:mm:ss"));// 2017-08-28 15:45:02System.out.println(format((LocalDateTime.now()), "yyyy-MM-dd HH:mm:ss"));// 2017-08-28System.out.println(format((LocalDateTime.now().toLocalDate()), "yyyy-MM-dd")); 日期转换java.util.date和java.time.LocalDateTime互相转换 应用示例123456789101112131415161718/*** 将 &#123;@link LocalDateTime&#125; 转换成 &#123;@link Date&#125;* @param localDateTime &#123;@link LocalDateTime&#125; 待转换的日期* @return 转换成Date结果*/public static Date from(LocalDateTime localDateTime)&#123; Instant instant = localDateTime.atZone(ZoneId.systemDefault()).toInstant(); return Date.from(instant);&#125;/*** 将 &#123;@link Date&#125; 转换成 &#123;@link LocalDateTime&#125;* @param date &#123;@link Date&#125; 待转换的日期* @return 转换成 &#123;@link LocalDateTime&#125; 结果*/public static LocalDateTime from(Date date)&#123; return LocalDateTime.ofInstant(date.toInstant(), ZoneId.systemDefault());&#125; 示例测试123456String patternTime = "yyyy-MM-dd HH:mm:ss";Date now = new Date();// 2017-08-28 14:47:10System.out.println(format(from(now), patternTime));// 2017-08-28 14:47:10System.out.println(format(from(LocalDateTime.now()), patternTime)); 区间集合计算两端日期之间内的日期天数集合 示例代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/*** 获取&#123;@link Date&#125;在开始时间和结束时间内的日期时间段&#123;@link Date&#125;集合* @param start 开始时间* @param end 结束时间* @return 时间天数集合*/public static List&lt;Date&gt; dateZones(Date start, Date end)&#123; return dateZones(from(start), from(end));&#125;/*** 获取 &#123;@link LocalDate&#125; 在开始时间和结束时间内的日期时间段 &#123;@link LocalDate&#125; 集合* @param start 开始时间* @param end 结束时间* @return 时间集合*/public static List&lt;Date&gt; dateZones(LocalDate start, LocalDate end)&#123; return Stream.iterate(start, x -&gt; x.plusDays(1)) .limit(ChronoUnit.DAYS.between(start, end) + 1) .map(e -&gt; Date.from(e.atStartOfDay().atZone(ZoneId.systemDefault()).toInstant())) .collect(Collectors.toList());&#125; /*** 获取&#123;@link LocalDateTime&#125; 在开始时间和结束时间内的日期时间段&#123;@link Date&#125;集合* @param start 开始时间* @param end 结束时间* @return 时间天数集合*/public static List&lt;Date&gt; dateZones(LocalDateTime start, LocalDateTime end)&#123; // 用起始时间作为流的源头，按照每次加一天的方式创建一个无限流 return Stream.iterate(start.toLocalDate(), x -&gt; x.plusDays(1)) // 截断无限流，长度为起始时间和结束时间的差+1个 .limit(ChronoUnit.DAYS.between(start, end) + 1) // 由于最后要的是字符串，所以map转换一下 .map(e -&gt; Date.from(e.atStartOfDay().atZone(ZoneId.systemDefault()).toInstant())) // 把流收集为List .collect(Collectors.toList());&#125;/*** 获取&#123;@link Date&#125;在开始时间和结束时间内的日期时间段&#123;@link LocalDate&#125;集合* @param start 开始时间* @param end 结束时间* @return 时间集合*/public static List&lt;LocalDate&gt; localDateZones(Date start, Date end)&#123; return localDateZones(from(start), from(end));&#125;/*** 获取 &#123;@link LocalDate&#125; 在开始时间和结束时间内的日期时间段 &#123;@link LocalDate&#125; 集合* @param start 开始时间* @param end 结束时间* @return 时间集合*/public static List&lt;LocalDate&gt; localDateZones(LocalDate start, LocalDate end)&#123; return Stream.iterate(start, x -&gt; x.plusDays(1)) .limit(ChronoUnit.DAYS.between(start, end) + 1) .collect(Collectors.toList());&#125;/*** 获取 &#123;@link LocalDateTime&#125; 在开始时间和结束时间内的日期时间段 &#123;@link LocalDate&#125; 集合* @param start 开始时间* @param end 结束时间* @return 时间集合*/public static List&lt;LocalDate&gt; localDateZones(LocalDateTime start, LocalDateTime end)&#123; // 用起始时间作为流的源头，按照每次加一天的方式创建一个无限流 return Stream.iterate(start.toLocalDate(), x -&gt; x.plusDays(1)) // 截断无限流，长度为起始时间和结束时间的差+1个 .limit(ChronoUnit.DAYS.between(start, end) + 1) .map(e -&gt; e.atStartOfDay().toLocalDate()) // 把流收集为List .collect(Collectors.toList());&#125; 示例测试12345678910111213141516171819202122232425262728String patternDate = "yyyy-MM-dd";List&lt;Date&gt; dateList = Arrays.asList(new Date(2017-1900, 11, 30), new Date(2018-1900, 0, 3));// 2017-12-30System.out.println("开始时间:" + format(dateList.get(0), patternDate) + ", 结束时间:" + format(dateList.get(1), patternDate));// [2017-12-30, 2017-12-31, 2018-01-01, 2018-01-02, 2018-01-03]System.out.println(dateZones(dateList.get(0), dateList.get(1)).stream().map(x -&gt; format(x, patternDate)).collect(Collectors.toList()));// [2017-12-30, 2017-12-31, 2018-01-01, 2018-01-02, 2018-01-03]System.out.println(localDateZones(dateList.get(0), dateList.get(1)).stream().map(x -&gt; format(x, patternDate)).collect(Collectors.toList()));LocalDateTime now = LocalDateTime.of(2017, Month.DECEMBER, 30, 0, 0, 0);// 2017-12-30System.out.println(format(now, patternDate));// [2017-12-30, 2017-12-31, 2018-01-01, 2018-01-02, 2018-01-03]System.out.println(dateZones(now, now.plus(4, ChronoUnit.DAYS)) .stream().map(x -&gt; format(x, patternDate)).collect(Collectors.toList()));// [2017-12-30, 2017-12-31, 2018-01-01, 2018-01-02, 2018-01-03]System.out.println(localDateZones(now, now.plus(4, ChronoUnit.DAYS)) .stream().map(x -&gt; format(x, patternDate)).collect(Collectors.toList()));// [2017-12-30, 2017-12-31, 2018-01-01, 2018-01-02, 2018-01-03]System.out.println(localDateZones(now.toLocalDate(), now.toLocalDate().plus(4, ChronoUnit.DAYS)) .stream().map(x -&gt; format(x, patternDate)).collect(Collectors.toList())); 日期加减示例代码123456789101112131415161718192021222324252627282930String patternDate = "yyyy-MM-dd HH:mm:ss";LocalDateTime now = LocalDateTime.of(2017, Month.DECEMBER, 30, 0, 0, 0);// 当前时间: 2017-12-30 00:00:00System.out.println("当前时间: " + format(now, patternDate));// 30秒前: 2017-12-29 23:59:30System.out.println("30秒前: " + format(now.plus(-30, ChronoUnit.SECONDS), patternDate));// 5分钟后: 2017-12-30 00:05:00System.out.println("5分钟后: " + format(now.plus(5, ChronoUnit.MINUTES), patternDate));// 2天前: 2017-12-28 00:00:00System.out.println("2天前: " + format(now.plus(-2, ChronoUnit.DAYS), patternDate));// 2天后: 2018-01-01 00:00:00System.out.println("2天后: " + format(now.plus(2, ChronoUnit.DAYS), patternDate));// 1周后: 2018-01-06 00:00:00System.out.println("1周后: " + format(now.plusWeeks(1), patternDate)); // 1月前: 2017-11-30 00:00:00System.out.println("1月前: " + format(now.plus(-1, ChronoUnit.MONTHS), patternDate));// 1月后: 2018-01-30 00:00:00System.out.println("1月后: " + format(now.plus(1, ChronoUnit.MONTHS), patternDate));// 1年后: 2018-12-30 00:00:00System.out.println("1年后: " + format(now.plus(1, ChronoUnit.YEARS), patternDate)); 日期推算示例代码12345678910111213141516171819202122232425262728293031String patternDate = "yyyy-MM-dd";LocalDateTime now = LocalDateTime.of(2017, Month.DECEMBER, 30, 0, 0, 0);// 当前时间: 2017-12-30System.out.println("当前时间: " + format(now, patternDate) + " ,是否闰年: " + now.toLocalDate().isLeapYear());// 当前月份: 十二月System.out.println("当前月份: " + Month.from(now).getDisplayName(TextStyle.FULL, Locale.CHINA));// 当前星期: 星期六System.out.println("当前星期: " + DayOfWeek.from(now).getDisplayName(TextStyle.FULL, Locale.CHINA));// 需要注意:java8提供的获取的本周第一天和本周最后一天是西方的界定方式, 第一天是周末, 最后一天是周六, 和中国的不太一样// 本周初第一天:2017-12-24System.out.println("本周初第一天: " + format(now.with(WeekFields.of(Locale.CHINA).dayOfWeek(),1L), patternDate));// 本周最后一天:2017-12-30System.out.println("本周最后一天: " + format(now.with(WeekFields.of(Locale.CHINA).dayOfWeek(),7L), patternDate));// 本月初第一天:2017-12-01System.out.println("本月初第一天: " + format(now.with(TemporalAdjusters.firstDayOfMonth()), patternDate));// 本月最后一天:2017-12-31System.out.println("本月最后一天: " + format(now.with(TemporalAdjusters.lastDayOfMonth()), patternDate));// 本年最后一天:2017-01-01System.out.println("本年最后一天: " + format(now.with(TemporalAdjusters.firstDayOfYear()), patternDate));// 本年最后一天:2017-12-31System.out.println("本年最后一天: " + format(now.with(TemporalAdjusters.lastDayOfYear()), patternDate));]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Date</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装配置 - ReactNative]]></title>
    <url>%2F2016%2F12%2F11%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE-ReactNative%2F</url>
    <content type="text"><![CDATA[ReactNative和React的关系 React用于web应用开发,ReactNative采用React方式进行移动应用开发.ReactNative采用React预发,用于进行JavaScript跨终端应用开发,既拥有原生Native的交互体验,又能够保留React的开发效率.使用灵活的Html和Css布局,使用React语法构建组件,然后同时运行在Ios和Android平台上. ReactNative安装前提Mac下安装Homebrew(非必须)安装操作 123/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"[elonsu@localhost ~ ]$ brew -vHomebrew 0.9.5 (git revision a8d6; last commit 2016-03-15) 建议不定时的更新 1[elonsu@localhost ~]$ brew update &amp;&amp; brew upgrade 安装NodeJs安装react前需要保证已经安装了nodeJs,下载地址:https://nodejs.org/en/ 1[elonsu@localhost ~]$ brew install node ReactNative安装mac系统安装react-native 123[elonsu@localhost ~ ]$ node --versionv7.2.0[elonsu@localhost ~ ]$ sudo npm install -g react-native-cli 安装Watchman和Flow,监控文件变化和类型检查的。 12[elonsu@localhost ~ ]$ brew install watchman[elonsu@localhost ~ ]$ brew install flow ReactNative创建项目初始化项目使用命令react-native init 工程名称 1[elonsu@localhost wuyu-platform (master ✗)]$ react-native init wuyuApp 初始化完会看到如下提示: 12345678To run your app on iOS: react-native run-ios - or - Open ios/wuyuApp.xcodeproj in Xcode Hit the Run buttonTo run your app on Android: Have an Android emulator running (quickest way to get started), or a device connected react-native run-android 启动调试 12345[elonsu@localhost wuyu-platform (master ✗)]$ cd wuyuApp[elonsu@localhost wuyuApp (master ✗)]$ ls__tests__ android index.android.js index.ios.js ios node_modules package.json[elonsu@localhost wuyuApp (master ✗)]$ react-native run-ios[elonsu@localhost wuyuApp (master ✗)]$ react-native run-android ReactNative开发工具推荐开发工具Atom,下载地址:https://atom.io/Atom常用插件推荐: open-in-browser: 浏览器浏览功能 atom-html-preview: 分屏展示html页面效果 autocomplete-path: 文件路径补全 ReactNative资料React官网: https://facebook.github.io/react/ReactNative中文网: http://reactnative.cn/]]></content>
      <categories>
        <category>运维部署</category>
      </categories>
      <tags>
        <tag>Node</tag>
        <tag>React-native</tag>
        <tag>App</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装配置 - React]]></title>
    <url>%2F2016%2F12%2F11%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2F%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE-React%2F</url>
    <content type="text"><![CDATA[React介绍 React是一个用于构建用户界面的JavaScript库。React主要用于构建UI,其本事不是一个MVC框架,只是是MVC中的V（视图）。React起源于Facebook的开源项目,诞生之初用来架设Instagram网站。 React特点声明式设计 − React采用声明范式，可以轻松描述应用。高效 − React通过对DOM的模拟，最大限度地减少与DOM的交互。灵活 − React可以与已知的库或框架很好地配合。JSX − JSX 是 JavaScript 语法的扩展。React 开发不一定使用 JSX ，但我们建议使用它。组件 − 通过 React 构建组件，使得代码更加容易得到复用，能够很好的应用在大项目的开发中。单向响应的数据流 − React 实现了单向响应的数据流，从而减少了重复代码，这也是它为什么比传统数据绑定更简单。 React入门示例示例使用React版本v15.4.1; 库下载地址: https://github.com/facebook/react/releases123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;&lt;/title&gt; &lt;!-- react.js是react的核心库--&gt; &lt;script src="./build/react.js" charset="utf-8"&gt;&lt;/script&gt; &lt;!-- react-dom.js的作用是提供与Dom相关的功能--&gt; &lt;script src="./build/react-dom.js" charset="utf-8"&gt;&lt;/script&gt; &lt;!-- browser.min.js的作用是讲JSX语法转换成JavaScript预发 --&gt; &lt;script src="http://static.runoob.com/assets/react/browser.min.js"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- React渲染的模板内容会插入到该Dom节点中,作为一个容器--&gt; &lt;div id="container"&gt;&lt;/div&gt;&lt;/body&gt;&lt;!-- 在React开发中,使用JSX,跟JavaScript不兼容,在使用JSX的地方,要设置type:text/babel babel是一个转换编译器,ES6转成可以在浏览器中运行的代码--&gt;&lt;script type="text/babel"&gt; ReactDOM.render( &lt;h1&gt;第一个React示例程序&lt;/h1&gt;, document.getElementById("container") );&lt;/script&gt;&lt;/html&gt; 说明: build中的js库来自https://github.com/facebook/react/releases。 在React开发中,使用JSX语法,跟JavaScript不兼容,在使用JSX的地方,要设置type:text/babel; babel是一个转换编译器,ES6转成可以在浏览器中运行的代码 ReactDom.render()是最基本的React语法,支持三个参数:(参数1:模板渲染的内容,为html形式, 参数2:模板需要插入的dom结点, 参数3: 渲染后的回调,一般不用) React JSX React使用JSX来替代常规的 avaScript; JSX是一个看起来很像XML的JavaScript语法扩展。 JSX必须借助React环境运行,JSX语法能够让我们更直观的看到组件的Dom结构,不能直接在浏览器上运行,最终会转换成JavaScript。JSX标签其实就是HTML标签,只不过在JavaScript中书写这些标签时,不需要使用””括起来, 可以像xml一样书写 12345678&lt;script type="text/babel"&gt; ReactDOM.render( &lt;h1&gt; 第一个React示例程序 &lt;/h1&gt;, document.getElementById("container") );&lt;/script&gt; JSX中运行JavaScript代码,使用{}括起来, 语法格式为:{表达式} 1234567&lt;script type="text/babel"&gt; var text = "第一个React示例程序"; ReactDOM.render( &lt;h1&gt;&#123;text&#125;&lt;/h1&gt;, document.getElementById("container") );&lt;/script&gt; React定义组件React中创建组件类以大写字母开头,驼峰命名法。在React中使用React.createClass方法创建一个组件类。每个组件类都必须实现自己的render方法,输出定义好的组件模板,返回值:null、false、组件模板; 12345678910111213&lt;script type="text/babel"&gt; // 定义组件类 var HellComponent = React.createClass(&#123; render: function()&#123; return &lt;h1&gt;第一个自定义组件&lt;/h1&gt;; &#125; &#125;); // 渲染dom ReactDOM.render( &lt;HellComponent/&gt;, document.getElementById("container") );&lt;/script&gt; React定义样式React组件样式样式,有三种: 内连样式(示例div元素)、对象样式(示例h1元素)、选择器样式(示例h2元素)。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;&lt;/title&gt; &lt;!-- react.js是react的核心库--&gt; &lt;script src="./build/react.js" charset="utf-8"&gt;&lt;/script&gt; &lt;!-- react-dom.js的作用是提供与Dom相关的功能--&gt; &lt;script src="./build/react-dom.js" charset="utf-8"&gt;&lt;/script&gt; &lt;!-- browser.min.js的作用是讲JSX语法转换成JavaScript预发 --&gt; &lt;script src="http://static.runoob.com/assets/react/browser.min.js"&gt;&lt;/script&gt; &lt;style&gt; .ft&#123; color: blue;&#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- React渲染的模板内容会插入到该Dom节点中,作为一个容器--&gt; &lt;div id="container"&gt;&lt;/div&gt;&lt;/body&gt;&lt;!-- 在React开发中,使用JSX,跟JavaScript不兼容,在使用JSX的地方,要设置type:text/babel babel是一个转换编译器,ES6转成可以在浏览器中运行的代码--&gt;&lt;script type="text/babel"&gt; // react中定义对象样式 var hellStyle = &#123; backgroundColor: "green", color: "red", &#125; // 定义组件类, this.props定义组件属性 var HellComponent = React.createClass(&#123; render: function()&#123; return ( &lt;div style=&#123;&#123;backgroudColor:"yellow", borderWidth:5&#125;&#125;&gt; &lt;h1 style=&#123;hellStyle&#125;&gt;第一个自定义组件,动态参数&#123;this.props.param1&#125;&lt;/h1&gt; &lt;h2 className="ft"&gt;第一个自定义组件-动态参数:&#123;this.props.param2&#125;&lt;/h2&gt; &lt;/div&gt; ); &#125; &#125;); // 渲染dom ReactDOM.render( &lt;HellComponent param1="参数1" param2="参数2"/&gt;, document.getElementById("container") );&lt;/script&gt;&lt;/html&gt; 在React和Html5中设置样式时书写格式是有区别的 html5样式以;结尾,React以,结尾 Html5中key,value都不需要加引号, React中属于JavaScript对象, key的名字不能出现“-”, 需要使用驼峰命名法, 如果value为字符串需要加&quot;&quot; Html5中value如果是数字,需要带单位,React中不需要带单位. Html5中选择器样式使用class, React中class是关键字,使用选择器样式时, 属性名使用className, 类似的也有使用htmlFor替换for. React事件操作React事件示例 12345678910111213141516171819&lt;script type="text/babel"&gt; var BtnAction = React.createClass(&#123; // react中事件命名规范: 首字母小写, 驼峰命名法 handleClick: function()&#123; alert("点击按钮触发事件"); &#125;, render: function()&#123; return ( &lt;button onClick=&#123;this.handleClick&#125;&gt;&#123;this.props.buttonTitle&#125;&lt;/button&gt; ); &#125; &#125;); // 渲染dom ReactDOM.render( &lt;BtnAction buttonTitle="React事件按钮"/&gt;, document.getElementById("container") );&lt;/script&gt; React状态操作React状态示例 1234567891011121314151617181920212223242526272829303132333435&lt;script type="text/babel"&gt; var CheckButton = React.createClass(&#123; // 定义初始状态 getInitialState: function()&#123; return &#123; // 在这个对象中设置的属性值,将会存储在stage中, isCheck: false &#125; &#125;, // 定义事件绑定方法 HandleChange: function()&#123; // 修改状态值,通过this.stage读取设置的状态 this.setState(&#123; isCheck: !this.state.isCheck &#125;); &#125;, render: function()&#123; // 根据状态设置显示的文字 // 在JSX语法中,不能直接使用if,需要使用三目运算符 var text = this.state.isCheck? "已选中": "未选中"; return ( &lt;div&gt; &lt;input type="checkbox" onChange=&#123;this.HandleChange&#125; /&gt; &#123;text&#125; &lt;/div&gt; ); &#125; &#125;); // 渲染dom ReactDOM.render( &lt;CheckButton/&gt;, document.getElementById("container") );&lt;/script&gt; 当state发生变化时,会调用组件内部的render方法。 React资料 React教程: http://www.runoob.com/react/react-components.html]]></content>
      <categories>
        <category>运维部署</category>
      </categories>
      <tags>
        <tag>React</tag>
        <tag>Node</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解Java-CountDownLatch应用]]></title>
    <url>%2F2016%2F10%2F27%2F%E8%AF%AD%E8%A8%80%E8%AF%AD%E6%B3%95%2F%E7%90%86%E8%A7%A3Java-CountDownLatch%2F</url>
    <content type="text"><![CDATA[CountDownLatch说明CountDownLatch是一个同步辅助类,这个类能够使一个线程等待其他线程完成各自的工作后再执行。例如，应用程序的主线程希望在负责启动框架服务的线程已经启动所有的框架服务之后再执行。 CountDownLatch实现概要CountDownLatch是通过一个计数器来实现的，计数器的初始值为线程的数量。每当一个线程完成了自己的任务后，计数器的值就会减1。当计数器值到达0时，它表示所有的线程已经完成了任务，然后在闭锁上等待的线程就可以恢复执行任务。 CountDownLatch多线程应用列表循环遍历(耗时8697ms)123456789101112131415161718@Testpublic void optimizeListV1()&#123; long start = System.currentTimeMillis(); try &#123; final List&lt;String&gt; lists = Arrays.asList("aa", "bb", "cc", "dd", "ee"); for(int i=0; i&lt;lists.size(); i++)&#123; if(i == 2)&#123; Thread.sleep(3000); &#125; Thread.sleep(1000); &#125; System.out.println("聚合完成"); &#125;catch (Exception e)&#123; &#125;finally &#123; MockTimeUtil.mockInvokeTime("循环列表场景模拟:", start); &#125;&#125; 多线程聚合列表(耗时4671ms)123456789101112131415161718192021222324252627282930@Testpublic void optimizeList()&#123; long start = System.currentTimeMillis(); try &#123; ExecutorService ex = Executors.newFixedThreadPool(5); final List&lt;String&gt; lists = Arrays.asList("aa", "bb", "cc", "dd", "ee"); final CountDownLatch latch = new CountDownLatch(lists.size()); for(int i=0; i&lt;lists.size(); i++)&#123; final int tmp = i; ex.submit(new Callable&lt;Object&gt;() &#123; @Override public Object call() throws Exception &#123; if(tmp == 2)&#123; Thread.sleep(3000); &#125; Thread.sleep(1000); latch.countDown(); return null; &#125; &#125;); &#125; //latch.await(); latch.await(3500, TimeUnit.MILLISECONDS); System.out.println("聚合完成"); &#125;catch (Exception e)&#123; &#125;finally &#123; MockTimeUtil.mockInvokeTime("线程列表场景模拟:", start); &#125;&#125; CountDownLatch方法说明CountDownLatch源码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class CountDownLatch &#123; /** * Synchronization control For CountDownLatch. * Uses AQS state to represent count. */ private static final class Sync extends AbstractQueuedSynchronizer &#123; private static final long serialVersionUID = 4982264981922014374L; Sync(int count) &#123; setState(count); &#125; int getCount() &#123; return getState(); &#125; protected int tryAcquireShared(int acquires) &#123; return (getState() == 0) ? 1 : -1; &#125; protected boolean tryReleaseShared(int releases) &#123; // Decrement count; signal when transition to zero for (;;) &#123; int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; &#125; &#125; &#125; private final Sync sync; public CountDownLatch(int count) &#123; if (count &lt; 0) throw new IllegalArgumentException("count &lt; 0"); this.sync = new Sync(count); &#125; public void await() throws InterruptedException &#123; sync.acquireSharedInterruptibly(1); &#125; public boolean await(long timeout, TimeUnit unit) throws InterruptedException &#123; return sync.tryAcquireSharedNanos(1, unit.toNanos(timeout)); &#125; public void countDown() &#123; sync.releaseShared(1); &#125; public long getCount() &#123; return sync.getCount(); &#125; public String toString() &#123; return super.toString() + "[Count = " + sync.getCount() + "]"; &#125;&#125; 方法说明: countDown()：当前线程调用此方法，则计数减一 await(): 调用此方法会一直阻塞当前线程，直到计时器的值为0]]></content>
      <categories>
        <category>语言语法</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>CountDownLatch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解Java-CompleteService应用]]></title>
    <url>%2F2016%2F10%2F27%2F%E8%AF%AD%E8%A8%80%E8%AF%AD%E6%B3%95%2F%E7%90%86%E8%A7%A3Java-CompleteService%2F</url>
    <content type="text"><![CDATA[CompleteService使用场景 当我们需要批量任务处理,但是并不关心任务完成的先后顺序,我们异步的提交任务,等待有任务执行完成之后然后对该完成结果处理,如此循环直到该批量任务完成.我们遵循异步处理完成后的操作原则时,谁先完成收割谁. 基于集合Future遍历处理针对这种场景,我们很可能会想到把所有的异步任务收集到一个集合,然后遍历这个集合(Future),调用future.get()获取处理结果,进行后续操作,然后我们就会写出如下代码 123456789101112131415161718192021ExecutorService pool = Executors.newFixedThreadPool(5);final List&lt;String&gt; dList = Arrays.asList("aa", "bb", "cc", "dd", "ee");List&lt;Future&gt; fList= new ArrayList&lt;Future&gt;();for(int i=0; i&lt;dList.size(); i++)&#123; final int tmp = i; Future future = pool.submit(new Callable&lt;Object&gt;() &#123; @Override public Object call() throws Exception &#123; if (tmp == 2) &#123; Thread.sleep(3000); &#125; Thread.sleep(1000); return "线程" + Thread.currentThread().getName() + "处理数据元素list(" + + tmp +") = " + dList.get(tmp) ; &#125; &#125;); fList.add(future);&#125;System.out.println("聚合完成");for (int i = 0; i &lt; fList.size(); i++) &#123; System.out.println(fList.get(i).get());&#125; 执行这段代码,会发现执行结果并没有按照我们所预期的来 123456聚合完成线程pool-1-thread-1处理数据元素list(0) = aa线程pool-1-thread-2处理数据元素list(1) = bb线程pool-1-thread-3处理数据元素list(2) = cc线程pool-1-thread-4处理数据元素list(3) = dd线程pool-1-thread-5处理数据元素list(4) = ee 可见,上面的执行结果并不是我们想要的,明显cc元素的执行比较耗时,但是我们的处理结果却是按照循环遍历的顺序来的,原因如下: 从list中遍历的每个Future对象并不一定处于完成状态，这时调用get()方法就会被阻塞住，如果系统是设计成每个线程完成后就能根据其结果继续做后面的事，这样对于处于list后面的但是先完成的线程就会增加了额外的等待时间。 基于CompletionService完成并行聚合1234567891011121314151617181920ExecutorService pool = Executors.newFixedThreadPool(5);CompletionService&lt;Object&gt; cs = new ExecutorCompletionService&lt;Object&gt;(pool);final List&lt;String&gt; dList = Arrays.asList("aa", "bb", "cc", "dd", "ee");for(int i=0; i&lt;dList.size(); i++)&#123; final int tmp = i; cs.submit(new Callable&lt;Object&gt;() &#123; @Override public Object call() throws Exception &#123; if (tmp == 2) &#123; Thread.sleep(3000); &#125; Thread.sleep(1000); return "线程" + Thread.currentThread().getName() + "处理数据元素list(" + + tmp +") = " + dList.get(tmp); &#125; &#125;);&#125;System.out.println("聚合完成");for (int i = 0; i &lt; dList.size(); i++) &#123; System.out.println(cs.take().get());&#125; 执行会发现这种结果才是我们真正要的 1234567聚合完成线程pool-1-thread-2处理数据元素list(1) = bb线程pool-1-thread-1处理数据元素list(0) = aa线程pool-1-thread-4处理数据元素list(3) = dd线程pool-1-thread-5处理数据元素list(4) = ee线程pool-1-thread-3处理数据元素list(2) = cc 我们能得到想要的结果,是因为CompleteService中内部维护着一个BlockingQueue.原理如下: CompletionService的实现是维护一个保存Future对象的BlockingQueue。只有当这个Future对象状态是结束的时候，才会加入到这个Queue中，take()方法其实就是Producer-Consumer中的Consumer。它会从Queue中取出Future对象，如果Queue是空的，就会阻塞在那里，直到有完成的Future对象加入到Queue中。 CompleteService中take()和poll()的区别查看CompleteService的接口定义 123456789101112public interface CompletionService&lt;V&gt; &#123; Future&lt;V&gt; submit(Callable&lt;V&gt; task); Future&lt;V&gt; submit(Runnable task, V result); Future&lt;V&gt; take() throws InterruptedException; Future&lt;V&gt; poll(); Future&lt;V&gt; poll(long timeout, TimeUnit unit) throws InterruptedException;&#125; 从接口中我们可以看到CompletionService中定义的summit相关的方法用来载入线程体(分别处理实现Callable或Runable的线程), poll()和take()用来获取返回结果集. 关于poll()和take()的区别 poll()是非阻塞的，若目前无结果，返回一个null，线程继续运行不阻塞。take()是阻塞的，若当前无结果，则线程阻塞，直到产生一个结果 示例poll()和take()的区别12345678910111213141516171819202122232425262728ExecutorService pool = Executors.newFixedThreadPool(5);CompletionService&lt;Object&gt; cs = new ExecutorCompletionService&lt;Object&gt;(pool);final List&lt;String&gt; dList = Arrays.asList("aa", "bb", "cc", "dd", "ee");for(int i=0; i&lt;dList.size(); i++)&#123; final int tmp = i; cs.submit(new Callable&lt;Object&gt;() &#123; @Override public Object call() throws Exception &#123; if (tmp == 2) &#123; Thread.sleep(3000); &#125; Thread.sleep(1000); return "线程" + Thread.currentThread().getName() + "处理数据元素list(" + + tmp +") = " + dList.get(tmp); &#125; &#125;);&#125;System.out.println("聚合完成");AtomicInteger index = new AtomicInteger(0);while(index.get()&lt;dList.size()) &#123; Future&lt;Object&gt; f = cs.poll(); if(f == null) &#123; System.out.println("没发现有完成的任务"); &#125;else &#123; System.out.println(f.get()); index.incrementAndGet(); &#125; Thread.sleep(500);&#125; 程序运行结果 12345678910聚合完成没发现有完成的任务没发现有完成的任务线程pool-1-thread-1处理数据元素list(0) = aa线程pool-1-thread-4处理数据元素list(3) = dd线程pool-1-thread-2处理数据元素list(1) = bb线程pool-1-thread-5处理数据元素list(4) = ee没发现有完成的任务没发现有完成的任务线程pool-1-thread-3处理数据元素list(2) = cc]]></content>
      <categories>
        <category>语言语法</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>CompleteService</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hystrix服务熔断和降级]]></title>
    <url>%2F2016%2F09%2F23%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FSpring%2FHystrix%E6%9C%8D%E5%8A%A1%E7%86%94%E6%96%AD%E5%92%8C%E9%99%8D%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[在微服务架构中,存在着那么多的服务单元,若一个单元出现故障,就会因依赖关系形成故障蔓延,最终导致整个系统的瘫痪,这样的架构相较传统架构就更加的不稳定;为了解决这样的问题,因此产生了断路器模式.在Spring Cloud中使用了Hystrix 来实现断路器的功能;Hystrix是Netflix开源的微服务框架套件之一,该框架目标在于通过控制那些访问远程系统、服务和第三方库的节点,从而对延迟和故障提供更强大的容错能力;Hystrix具备拥有回退机制和断路器功能的线程和信号隔离,请求缓存和请求打包,以及监控和配置等功能. Ribbon熔断工程结构12345678910111213141516ribbon-hystrix├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── cloud│ │ └── ribbon│ │ └── hystrix│ │ ├── Application.java│ │ ├── service│ │ │ └── RibbonService.java│ │ └── www│ │ └── RibbonController.java│ └── resources│ └── application.properties Maven依赖12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 主类示例在应用主类中使用@EnableCircuitBreaker或@EnableHystrix注解开启Hystrix的使用： 12345678910111213141516@EnableHystrix@EnableDiscoveryClient@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125; @Bean @LoadBalanced RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; 配置文件12345678910### 应用端口server.port=8080spring.application.name=consumer-ribbon### 应用注册信息eureka.client.service-url.defaultZone=http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node1.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/### 链路追踪spring.sleuth.sampler.percentage=1spring.zipkin.base-url=http://172.30.12.197:9411/ 熔断服务12345678910111213141516171819202122@Slf4j@Servicepublic class RibbonService &#123; @Autowired private RestTemplate restTemplate; /** * 接口熔断 * @param id * @return */ @HystrixCommand(fallbackMethod = "getRemoteFallback") public String getRemote(long id) &#123; Assert.isTrue(id &gt;0, "参数必须大于0"); return restTemplate.getForObject("http://SERVER-PROVIDER/api/select/&#123;0&#125;", String.class, id); &#125; private String getRemoteFallback(long id) &#123; return "服务熔断[Ribbon]: 入参:" + id; &#125;&#125; Feign熔断工程结构1234567891011121314151617feign-hystrix├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── cloud│ │ └── feign│ │ └── hystrix│ │ ├── Application.java│ │ ├── service│ │ │ ├── FeignService.java│ │ │ └── FeignServiceHystrix.java│ │ └── www│ │ └── FeignController.java│ └── resources│ └── application.properties Maven依赖12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-feign&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 主类示例123456789@EnableFeignClients@EnableDiscoveryClient@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 熔断服务接口定义123456@FeignClient(name="SERVER-PROVIDER", fallback = FeignServiceHystrix.class)public interface FeignService &#123; @GetMapping(value="/api/select/&#123;id&#125;") public String getRemote(@PathVariable("id") long id);&#125; 接口实现12345678@Componentpublic class FeignServiceHystrix implements FeignService&#123; @Override public String getRemote(long id) &#123; return "服务熔断[Feign]: 入参:" + id; &#125;&#125; 测试熔断启动注册中心、zipkin(可选项目)、服务提供者、服务消费者, 效果图如下: 正常访问测试1234[root@localhost ~ ]$ curl -XGET '172.30.12.197:8080/api/invoke/select/1'测试数据1[root@localhost ~ ]$ curl -XGET '172.30.12.197:9090/api/invoke/select/1'测试数据1 熔断访问测试停用服务端,进行客户端访问 1234[root@localhost ~ ]$ curl -XGET '172.30.12.197:8080/api/invoke/select/111'服务熔断[Ribbon]: 入参:111[root@localhost ~ ]$ curl -XGET '172.30.12.197:9090/api/invoke/select/111'服务熔断[Feign]: 入参:111 Hystrix监控应用添加熔断监控Hystrix Dashboard监控单实例节点需要通过访问实例的/hystrix.stream接口来实现,所以必须保证待监控示例中满足如下配置: Maven依赖12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 开启熔断确保在服务实例的主类中已经使用@EnableCircuitBreaker或@EnableHystrix注解,开启了断路器功能. 地址检测访问实例的hystrix.stream接口,检测地址是否可以正常访问. 12345[root@localhost ~]$ curl http://172.30.12.197:8080/hystrix.streamping:[root@localhost ~]$ curl http://172.30.12.197:9090/hystrix.streamping: Hystrix单体监控工程结构123456789101112hystrix-dashboard├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── cloud│ │ └── hystrix│ │ └── dashboard│ │ └── Application.java│ └── resources│ └── application.properties Maven依赖123456789101112131415161718192021222324252627282930&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix-dashboard&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 主类示例12345678@EnableHystrixDashboard@SpringCloudApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 注册查看 面板访问访问面板:http://172.30.12.197:9090/hystrix.stream Hystrix Dashboard 共支持三种不同的监控方式: 默认的集群监控：通过URL http://turbine-hostname:port/turbine.stream 开启,实现对默认集群的监控. 指定的集群监控：通过URL http://turbine-hostname:port/turbine.stream?cluster=[clusterName] 开启,实现对clusterName集群的监控. 单体应用的监控：通过URL http://hystrix-app:port/hystrix.stream 开启,实现对具体某个服务实例的监控. 前两者都是对集群的监控,需要整合Turbine才能实现,这里我们先来实现单个服务实例的监控. 监控单体实例监控Ribbon消费实例,在监控面板中键入Ribbon示例的示例监控地址http://172.30.12.197:8080/hystrix.stream; 调用Ribbon消费实例,查看监控面板. 监控Feign消费实例,在监控面板中键入Ribbon示例的示例监控地址http://172.30.12.197:9090/hystrix.stream; 调用Feign消费实例,查看监控面板. Hystrix聚合监控在复杂的分布式系统中,相同服务的节点经常需要部署上百甚至上千个,很多时候,运维人员希望能够把相同服务的节点状态以一个整体集群的形式展现出来,这样可以更好的把握整个系统的状态; 为此Netflix提供了一个开源项目（Turbine）来提供把多个hystrix.stream的内容聚合为一个数据源供Dashboard展示. 工程结构123456789101112hystrix-turbine├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── cloud│ │ └── turbine│ │ └── hystrix│ │ └── Application.java│ └── resources│ └── application.properties Maven依赖12345678910111213141516171819202122232425262728293031&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-turbine&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-netflix-turbine&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-hystrix-dashboard&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 主类示例使用@EnableTurbine注解开启Turbine. 1234567891011@EnableTurbine@EnableDiscoveryClient@SpringBootApplication@EnableHystrixDashboardpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 配置文件1234567891011### 应用基础配置server.port=9999spring.application.name=hystrix-turbine### 监控示例配置turbine.appConfig=CONSUMER-RIBBON,CONSUMER-FEIGNturbine.aggregator.clusterConfig= defaultturbine.clusterNameExpression= new String("default")### 应用注册信息eureka.client.service-url.defaultZone=http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node1.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/ 参数说明: turbine.appConfig: 配置Eureka中的serviceId列表,表明监控哪些服务. turbine.aggregator.clusterConfig: 指定聚合哪些集群,多个使用”,”分割,默认为default; 可使用 http://.../turbine.stream?cluster={clusterConfig之一} 访问 turbine.clusterNameExpression: clusterNameExpression指定集群名称,默认表达式appName；此时：turbine.aggregator.clusterConfig需要配置想要监控的应用名称； 当clusterNameExpression: default时,turbine.aggregator.clusterConfig可以不写,因为默认就是default； 当clusterNameExpression: metadata[‘cluster’]时,假设想要监控的应用配置了eureka.instance.metadata-map.cluster: ABC,则需要配置,同时turbine.aggregator.clusterConfig: ABC 注册查看 监控查看查看聚合监控 12[root@localhost ~]$ curl http://172.30.12.197:9999/turbine.stream: ping 访问监控面板http://172.30.12.197:9999/hystrix,监控聚合监控地址http://172.30.12.197:9999/turbine.stream,分别执行实例调用. 监控指标 参考资料: http://blog.didispace.com/spring-cloud-starter-dalston-5-1/ 技术博客 http://www.ymq.io/ http://blog.didispace.com]]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringCloud</tag>
        <tag>Hystrix</tag>
        <tag>Turbine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sleuth分布式链路追踪]]></title>
    <url>%2F2016%2F09%2F12%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FSpring%2FSleuth%E5%88%86%E5%B8%83%E5%BC%8F%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA%2F</url>
    <content type="text"><![CDATA[Sleuth是SpringCloud生态中用于解决分布式链路调用追踪的组件, 微服务架构中要实现链路追踪只需要在客户端和服务端的应用中增加如下依赖即可. Maven依赖12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt; &lt;version&gt;x.x.x.RELEASE&lt;/version&gt;&lt;/dependency&gt; 分布式链路示例这里我们基于之前文章《SpringCloud-Eureka服务注册与发现》中的示例改进 服务端工程工程结构12345678910111213eureka-provider├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── cloud│ │ └── provider│ │ ├── Application.java│ │ └── www│ │ └── SimpleController.java│ └── resources│ └── application.properties Maven依赖123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 服务示例12345678910111213@RestController@RequestMapping(value="/api")public class SimpleController &#123; private final Logger logger = LoggerFactory.getLogger(SimpleController.class); @GetMapping(value="/select/&#123;id&#125;", produces = MediaType.APPLICATION_JSON_UTF8_VALUE) public ResponseEntity getUserEntry(@PathVariable("id") long id) &#123; logger.info("SERVER-PROVIDER:服务端接口实现"); return ResponseEntity.ok("测试数据" + id); &#125;&#125; 客户端工程工程结构1234567891011121314eureka-consumer-ribbon├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── cloud│ │ └── consumer│ │ └── ribbon│ │ ├── Application.java│ │ └── www│ │ └── RibbonController.java│ └── resources│ └── application.properties Maven依赖123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-ribbon&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 调用示例123456789101112131415@RestController@RequestMapping(value="/api/invoke")public class RibbonController &#123; private static final Logger logger = LoggerFactory.getLogger(RibbonController.class); @Autowired private RestTemplate restTemplate; @GetMapping(value="/select/&#123;id&#125;") public String remoteInvoke(@PathVariable long id) &#123; logger.info("SERVER-PROVIDER:服务端接口调用"); return restTemplate.getForObject("http://SERVER-PROVIDER/api/select/&#123;0&#125;", String.class, id); &#125;&#125; 调用链追踪分别启动服务端和客户端,等应用注册到注册中心后进行请求访问 执行调用请求12[root@localhost ~]$ curl 172.30.12.197:8080/api/invoke/select/2222测试数据2222 调用链日志服务端日志 122018-01-04 18:53:37.832 INFO [server-provider,fbb118782a389c57,24b2c95193a0efc7,false] 9059 --- [nio-7001-exec-3] com.cloud.provider.www.SimpleController : SERVER-PROVIDER:服务端接口实现2018-01-04 18:53:40.841 INFO [server-provider,095b60553c871b1e,d9c9dae442a359f0,false] 9059 --- [nio-7001-exec-4] com.cloud.provider.www.SimpleController : SERVER-PROVIDER:服务端接口实现 客户端日志 122018-01-04 18:53:37.503 INFO [consumer-ribbon,fbb118782a389c57,fbb118782a389c57,false] 9171 --- [nio-8080-exec-1] c.c.c.ribbon.www.RibbonController : SERVER-PROVIDER:服务端接口调用2018-01-04 18:53:40.833 INFO [consumer-ribbon,095b60553c871b1e,095b60553c871b1e,false] 9171 --- [nio-8080-exec-2] c.c.c.ribbon.www.RibbonController : SERVER-PROVIDER:服务端接口调用 从控制台输出内容可以看到一些形如的如下的日志信息.这个是分布式服务调动链日志,含义如下: 1[server-provider,fbb118782a389c57,24b2c95193a0efc7,false] 调用链解读 第一个值: 记录应用名称,也就是application.properties中spring.application.name的值 第二个值: TraceId，用于表示一条请求链路, 一条请求链路有一个唯一的TraceId 第三个值: SpanId，表示一个基本的工作单元,一个请求链路中可能有好几个逻辑调用, 每个逻块为一个基本单元, 所以一个TraceId下会有多个SpanId. 第四个值: false表示是否要将改信息输出到Zipkin/Logstash等服务中来统计或展示. 采集的形式有多种: Logstash、Zipkin、Mq消息、Http等. 分布式链路收集这里以Zipkin为例展示收集调用链路,Zipkin是Twitter的开源项目,基于Google Dapper实现, 主要用来收集各个服务器上请求链路的跟踪数据，并通过RestApi接口来辅助查询展示. Zipkin组件介绍 Collector: 收集器组件 Storage: 存储组件 Rest Api: API组件 Web UI: UI展示组件 搭建Zipkin服务工程结构123456789101112zipkin-server├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── cloud│ │ └── zipkin│ │ └── server│ │ └── ZipkinApplication.java│ └── resources│ └── application.properties Maven依赖123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 主类示例@EnableZipkinServer开启Zipkin服务 123456789@EnableEurekaClient@EnableZipkinServer@SpringBootApplicationpublic class ZipkinApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ZipkinApplication.class, args); &#125;&#125; 配置文件123spring.application.name=zipkin-serverserver.port=9411eureka.client.service-url.defaultZone=http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node1.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/ 效果界面启动实例、访问172.30.12.197:9411效果如下: 应用配置服务端配置Maven依赖12345678910111213141516171819202122232425262728293031&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 配置文件123456789spring.application.name=server-providerserver.port=7001spring.http.encoding.charset=utf-8eureka.client.service-url.defaultZone=http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node1.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/spring.sleuth.sampler.percentage=1spring.zipkin.base-url=http://172.30.12.197:9411/ 客户端配置Maven依赖12345678910111213141516171819202122232425262728293031&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-ribbon&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 配置文件12345678910spring.application.name=consumer-ribbonserver.port=8080eureka.instance.lease-renewal-interval-in-seconds=5eureka.instance.lease-expiration-duration-in-seconds=10eureka.client.service-url.defaultZone=http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node1.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/spring.sleuth.sampler.percentage=1spring.zipkin.base-url=http://172.30.12.197:9411/ 注册效果 访问测试服务端日志 12018-01-04 20:09:33.547 INFO [server-provider,d9bbb0646c11853d,d0637e9ba0112a6f,true] 10401 --- [nio-7001-exec-3] com.cloud.provider.www.SimpleController : SERVER-PROVIDER:服务端接口实现 客户端日志 12018-01-04 20:09:33.542 INFO [consumer-ribbon,d9bbb0646c11853d,d9bbb0646c11853d,true] 10405 --- [nio-8080-exec-2] c.c.c.ribbon.www.RibbonController : SERVER-PROVIDER:服务端接口调用 Zipkin查询 链路采集存储zipkin默认采集数据是存储在内存中的，应用启动后会丢失,所以也可以存储到mysql或elasticsearch. 示例展示存储到Elastic Maven依赖12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;zipkin.version&gt;2.4.2&lt;/zipkin.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt; &lt;version&gt;$&#123;zipkin.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt; &lt;version&gt;$&#123;zipkin.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-storage-elasticsearch-http&lt;/artifactId&gt; &lt;version&gt;$&#123;zipkin.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 配置文件12345678910111213141516spring.application.name=zipkin-serverserver.port=9411eureka.client.service-url.defaultZone=http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node1.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/### 当前程序不使用sleuthspring.sleuth.enabled=false### 设置elasticsearch存储zipkin.storage.type=elasticsearchzipkin.storage.StorageComponent=elasticsearchzipkin.storage.elasticsearch.cluster=es-testzipkin.storage.elasticsearch.hosts=172.30.12.197:9200zipkin.storage.elasticsearch.index=zipkinzipkin.storage.elasticsearch.index-shards=3zipkin.storage.elasticsearch.index-replicas=1zipkin.storage.elasticsearch.max-requests=64]]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zuul统一微服务网关]]></title>
    <url>%2F2016%2F09%2F11%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FSpring%2FZuul%E7%BB%9F%E4%B8%80%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%BD%91%E5%85%B3%2F</url>
    <content type="text"><![CDATA[Zuul是Netflix开源的微服务网关,用于提供动态路由,监控,弹性,安全等边缘服务.可以和Eureka、Ribbon、Hystrix等组件配合使用; Zuul组件的核心是一系列的过滤器;这些过滤器可以完成以下功能： 身份认证和安全: 识别每一个资源的验证要求,并拒绝那些不符的请求 审查与监控: 动态路由: 动态将请求路由到不同后端集群. 压力测试:逐渐增加指向集群的流量,以了解性能 负载分配:为每一种负载类型分配对应容量,并弃用超出限定值的请求. 静态响应处理:边缘位置进行响应,避免转发到内部集群. 多区域弹性:跨域AWS Region进行请求路由,旨在实现ELB(ElasticLoad Balancing)使用多样化. Spring Cloud对Zuul进行了整合和增强; 目前Zuul使用的默认是Apache的HTTP Client,也可以使用Rest Client,可以设置ribbon.restclient.enabled=true. 注册中心示例注册中心地址 1http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node1.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/ 服务注册工程结构1234567891011121314zuul-server├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── cloud│ │ └── zuul│ │ └── server│ │ └── Application.java│ └── resources│ ├── application-node1.properties│ ├── application-node2.properties│ └── application.properties Maven配置1234567891011121314151617181920212223242526272829303132333435&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zuul&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 主类示例 @EnableZuulProxy开启网关服务 @EnableDiscoveryClient开始实例自动发现并注册到注册中心 123456789@EnableZuulProxy@EnableDiscoveryClient@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 配置示例application.properties主配置示例 123456789101112131415spring.application.name=gateway-server### 注册中心eureka.client.service-url.defaultZone=http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node1.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/### 链路存储spring.sleuth.sampler.percentage=1spring.zipkin.base-url=http://172.30.12.197:9411/### 网关定义zuul.routes.api-ribbon.path=/ribbon/**zuul.routes.api-ribbon.serviceId=CONSUMER-RIBBONzuul.routes.api-feign.path=/feign/**zuul.routes.api-feign.serviceId=CONSUMER-FEIGN application-node1.properties 节点1配置示例 123server.port=5301logging.file=logs/gateway1.loglogging.level.root=INFO application-node2.properties 节点2配置示例 123server.port=5302logging.file=logs/gateway2.loglogging.level.root=INFO 服务启动12[root@localhost target ]$ java -jar zuul-server-1.0-SNAPSHOT.jar --spring.profiles.active=node1 &amp;[root@localhost target ]$ java -jar zuul-server-1.0-SNAPSHOT.jar --spring.profiles.active=node2 &amp; 注册查看访问注册中心:http://admin:pwd123@node1.test.com:8001 服务访问原始服务访问1234[root@localhost ~ ]$ curl http://172.30.12.197:8080/api/invoke/select/5测试数据5[root@localhost ~ ]$ curl http://172.30.12.197:9090/api/invoke/select/7测试数据7 通过网关访问1234[root@localhost ~ ]$ curl http://172.30.12.197:5301/ribbon/api/invoke/select/5测试数据5 [root@localhost ~ ]$ curl http://172.30.12.197:5302/feign/api/invoke/select/7 测试数据7 查看调用链]]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式高可用配置中心]]></title>
    <url>%2F2016%2F09%2F03%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FSpring%2F%E5%88%86%E5%B8%83%E5%BC%8F%E9%AB%98%E5%8F%AF%E7%94%A8%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%2F</url>
    <content type="text"><![CDATA[在使用spring cloud 构建分布式系统的过程中,为了完成多个服务的配置统一管理,使用了spring cloud config作为配置中心,管理所有微服务的系统配置。在分布式系统中,配置中心是一个独立的服务部件,作用是专门为其他服务提供系统启动所需的配置信息,保证系统正确启动。高可用配置中心的基本思路是将我们的配置服务交给注册中心, 依赖注册中心的多节点服务保证注册的高可用. 注册中心注册中心地址 1http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node1.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/ GIT仓库地址说明 仓库地址: https://gitee.com/test/spring-eureka 默认分之: master 配置文件: config-property 文件结构123config-property├── application-develop.properties└── application-product.properties 注册配置中心示例展示如何搭建高可用的配置中心 工程结构123456789101112config-provider├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── cloud│ │ └── config│ │ └── provider│ │ └── Application.java│ └── resources│ └── application.properties Maven配置1234567891011121314151617181920212223&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 主类示例 @EnableConfigServer启用配置中心功能 @EnableDiscoveryClient将示例注册到注册中心 12345678910@EnableDiscoveryClient@EnableConfigServer@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 配置说明12345678910spring.application.name=config-centerserver.port=7501spring.cloud.config.server.git.uri=https://gitee.com/test/spring-eurekaspring.cloud.config.server.git.search-paths=config-propertyspring.cloud.config.server.git.username=testspring.cloud.config.server.git.password=testpwdeureka.client.service-url.defaultZone=http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node1.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/ 参数说明 spring.cloud.config.server.git.uri: 远程git仓库的位置 spring.cloud.config.server.git.search-paths: git仓库下配置文件存放的相对路径 spring.cloud.config.server.git.username: git地址的访问账号 spring.cloud.config.server.git.password: git地址的访问密码 eureka.client.service-url.defaultZone: 注册中心地址 启动实例12[root@localhost target ]$ java -jar config-provider-1.0-SNAPSHOT.jar --server.port=7501 &amp;[root@localhost target ]$ java -jar config-provider-1.0-SNAPSHOT.jar --server.port=7502 &amp; 注册效果 查看配置开发环境12345678910111213141516171819202122[root@localhost target ]$ curl http://192.168.0.106:7502/application/develop/master | python -m json.tool&#123; "label": "master", "name": "application", "profiles": [ "develop" ], "propertySources": [ &#123; "name": "https://gitee.com/test/spring-eureka/config-property/application-develop.properties", "source": &#123; "environment": "develop", "spring.datasource.driver-class-name": "com.mysql.jdbc.Driver", "spring.datasource.password": "root", "spring.datasource.url": "jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;zeroDateTimeBehavior=round&amp;transformedBitIsBoolean=true&amp;autoReconnect=true", "spring.datasource.username": "root" &#125; &#125; ], "state": null, "version": "0b67d61360ad1bfc5b202476f9871a413ca6f029"&#125; 线上环境12345678910111213141516171819202122[root@localhost target ]$ curl http://192.168.0.106:7502/application/product/master | python -m json.tool&#123; "label": "master", "name": "application", "profiles": [ "product" ], "propertySources": [ &#123; "name": "https://gitee.com/test/spring-eureka/config-property/application-product.properties", "source": &#123; "environment": "product", "spring.datasource.driver-class-name": "com.mysql.jdbc.Driver", "spring.datasource.password": "root", "spring.datasource.url": "jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;zeroDateTimeBehavior=round&amp;transformedBitIsBoolean=true&amp;autoReconnect=true", "spring.datasource.username": "root" &#125; &#125; ], "state": null, "version": "0b67d61360ad1bfc5b202476f9871a413ca6f029"&#125; 读取配置文件示例展示如何实现对已经搭建好的配置中心的信息读取 工程结构1234567891011121314config-consumer├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── cloud│ │ └── config│ │ └── consumer│ │ ├── Application.java│ │ └── www│ │ └── ConfigReadController.java│ └── resources│ └── application.properties Maven依赖12345678910111213141516171819202122232425262728293031&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.retry&lt;/groupId&gt; &lt;artifactId&gt;spring-retry&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 主类示例 @EnableDiscoveryClient将示例注册到注册中心 123456789@EnableDiscoveryClient@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 注册效果启动示例,查看注册效果]]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eureka高可用注册中心]]></title>
    <url>%2F2016%2F08%2F02%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FSpring%2FEureka%E9%AB%98%E5%8F%AF%E7%94%A8%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83%2F</url>
    <content type="text"><![CDATA[在于在我们平时的生产环境中,很难保证单节点的eureka服务能提供百分百不间断的服务,如果eureka无响应了,整个项目应用都会出现问题,因此要保证eureka随时都能提供服务的情况下,最好的方式就是采用eureka的集群模式,也就是搭建eureka的高可用,在eureka的集群模式下,多个eureka server之间可以同步注册服务,因此在一个eureka宕掉的情况下仍然可以提供服务注册和服务发现的能力,从而达到注册中心的高可用. Eureka注册集群配置Maven依赖123456789101112131415161718&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Edgware.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 项目结构123456789101112131415eureka-registry├── pom.xml└── src └── main ├── java │ └── com │ └── cloud │ └── eureka │ └── registy │ └── Application.java └── resources ├── application-node1.properties ├── application-node2.properties ├── application-node3.properties └── application.properties Maven配置1234567891011121314151617181920212223242526272829303132&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.xml.bind&lt;/groupId&gt; &lt;artifactId&gt;jaxb-api&lt;/artifactId&gt; &lt;version&gt;2.3.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 主类代码@EnableEurekaServer注解即开启注册中心服务器的功能. 12345678@EnableEurekaServer@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; new SpringApplicationBuilder(Application.class).bannerMode(Banner.Mode.LOG).run(args); &#125;&#125; 配置Host伪示例机器结点 1127.0.0.1 node1.test.com node2.test.com node3.test.com 配置示例通用配置application.properties 配置文件内容(共用) 123456spring.application.name=registry-center### 注册中心授权配置security.basic.enabled=truesecurity.user.name=adminsecurity.user.password=pwd123 节点配置application-node1.properties 配置文件内容 12345678spring.application.name=registry-node1server.port=8001eureka.instance.hostname=node1.test.comeureka.server.enable-self-preservation=trueeureka.server.eviction-interval-timer-in-ms=5000eureka.client.register-with-eureka=trueeureka.client.fetch-registry=trueeureka.client.service-url.defaultZone=http://admin:pwd123@node2.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/ application-node2.properties 配置文件内容 12345678spring.application.name=registry-node2server.port=8002eureka.instance.hostname=node2.test.comeureka.server.enable-self-preservation=trueeureka.server.eviction-interval-timer-in-ms=3000eureka.client.register-with-eureka=trueeureka.client.fetch-registry=trueeureka.client.service-url.defaultZone=http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/ application-node3.properties 配置文件内容 12345678spring.application.name=registry-node3server.port=8003eureka.instance.hostname=node3.test.comeureka.server.enable-self-preservation=trueeureka.server.eviction-interval-timer-in-ms=3000eureka.client.register-with-eureka=trueeureka.client.fetch-registry=trueeureka.client.service-url.defaultZone=http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node2.test.com:8002/eureka/ 参数说明: spring.application.name：应用名称 server.port=8003: 应用服务端口 eureka.server.enable-self-preservation: 配置Eureka自我保护机制(默认为true) eureka.server.eviction-interval-timer-in-ms: 配置向Eureka server发送心跳间隔 eureka.client.register-with-eureka: 配置是否将当前实例注册到Eureka server(默认为true) eureka.client.fetch-registry: 是否从Eureka server获取注册信息 eureka.client.service-url.defaultZone: 配置eureka server列表,多个节点可以，分隔, 默认值为(http://localhost:8761/eureka/) 启动节点123[root@localhost target ]$ java -jar eureka-registry-1.0-SNAPSHOT.jar --spring.profiles.active=node1 &amp;[root@localhost target ]$ java -jar eureka-registry-1.0-SNAPSHOT.jar --spring.profiles.active=node2 &amp;[root@localhost target ]$ java -jar eureka-registry-1.0-SNAPSHOT.jar --spring.profiles.active=node3 &amp; 说明: 第一个服务会抛出一些错误,请不用担心那是因为其余服务还没有启动.启动之后就会没有问题了. 查看示例访问node1.test.com:8081效果如下(同理可以访问node2.test.com:8082, node3.test.com:8083),因为加了授权,访问使用admin/pwd123即可 服务注册配置在设置了多节点的服务注册中心之后，我们需要简单服务配置，就能将服务注册到Eureka Server集群中; 对应的注册中心地址如下: 1eureka.client.service-url.defaultZone=http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node1.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/]]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eureka服务注册与发现]]></title>
    <url>%2F2016%2F08%2F02%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FSpring%2FEureka%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[Eureka是Netflix开发的服务发现框架，本身是一个基于REST的服务，主要用于定位运行在AWS域中的中间层服务，以达到负载均衡和中间层服务故障转移的目的。Spring Cloud将它集成在其他子项目spring-cloud-netflix中，以实现spring cloud服务发现功能。 注册中心注册中心地址 1http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node1.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/ 服务注册工程结构12345678910111213eureka-provider├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── cloud│ │ └── provider│ │ ├── Application.java│ │ └── www│ │ └── SimpleController.java│ └── resources│ └── application.properties Maven配置1234567891011121314151617181920212223&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 主类示例 @EnableDiscoveryClient开始实例自动发现并注册到注册中心 123456789@EnableDiscoveryClient@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 配置示例123456spring.application.name=server-providerserver.port=7001spring.http.encoding.charset=utf-8eureka.client.service-url.defaultZone=http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node1.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/ 服务接口123456789101112@RestController@RequestMapping(value="/api")public class SimpleController &#123; private final Logger logger = LoggerFactory.getLogger(Application.class); @GetMapping(value="/select/&#123;id&#125;", produces = MediaType.APPLICATION_JSON_UTF8_VALUE) public ResponseEntity getUserEntry(@PathVariable("id") long id) &#123; return ResponseEntity.ok("测试数据:" + id); &#125; &#125; 服务启动12[root@localhost target ]$ java -jar eureka-provider-1.0-SNAPSHOT.jar --server.port=7001 &amp;[root@localhost target ]$ java -jar eureka-provider-1.0-SNAPSHOT.jar --server.port=7002 &amp; 注册查看访问注册中心:http://admin:pwd123@node1.test.com:8001 服务消费(Ribbon)工程结构123456789101112131415eureka-consumer-ribbon├── eureka-consumer-ribbon.iml├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── cloud│ │ └── consumer│ │ └── ribbon│ │ ├── Application.java│ │ └── www│ │ └── RibbonController.java│ └── resources│ └── application.properties Maven依赖1234567891011121314151617181920212223&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-ribbon&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 消费示例主类示例@EnableDiscoveryClient注解让改应用注册为Eureka客户端应用,在主类中创建RestTemplate的实例,并通过@LoadBalanced开启客户端负载均衡. 1234567891011121314@EnableDiscoveryClient@SpringBootApplicationpublic class Application &#123; @Bean @LoadBalanced RestTemplate getRestTemplate() &#123; return new RestTemplate(); &#125; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; Api服务这里访问的PROVIDER.TEST.SIMPLESERVER是服务提供方的服务名(服务名不区分大小写) 123456789101112@RestController@RequestMapping(value="/api/invoke")public class RibbonController &#123; @Autowired RestTemplate restTemplate; @GetMapping(value="/select/&#123;id&#125;") public String remoteInvoke(@PathVariable long id) &#123; return restTemplate.getForObject("http://SERVER-PROVIDER/api/select/&#123;0&#125;", String.class, id); &#125;&#125; 配置示例12345678910server.port=8080spring.application.name=consumer-ribbon# 心跳时间，即服务续约间隔时间（缺省为30s）eureka.instance.lease-renewal-interval-in-seconds=5# 发呆时间，即服务续约到期时间（缺省为90s）eureka.instance.lease-expiration-duration-in-seconds=10### 应用注册信息eureka.client.service-url.defaultZone=http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node1.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/ 启动访问注册中心面板 访问请求查看12[root@localhost target ]$ curl -XGET http://172.30.12.197:8080/api/invoke/select/7测试数据7 服务消费(Feign)工程结构12345678910111213141516eureka-consumer-feign├── pom.xml├── src│ └── main│ ├── java│ │ └── com│ │ └── cloud│ │ └── consumer│ │ └── feign│ │ ├── Application.java│ │ ├── service│ │ │ └── SimpleFeignService.java│ │ └── www│ │ └── FeignController.java│ └── resources│ └── application.properties Maven依赖123456789101112131415161718192021222324252627&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-feign&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 消费示例主类示例@EnableDiscoveryClient注解让改应用注册为Eureka客户端应用,@EnableFeignClients开启Spring Cloud Feign支持 123456789@EnableFeignClients@EnableDiscoveryClient@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 接口调用定义调用接口,通过@FeignClient注解指定服务名来绑定服务,这里配置的SERVER-PROVIDER是服务提供方的服务名(服务名不区分大小写) 123456@FeignClient(name="SERVER-PROVIDER")public interface SimpleFeignService &#123; @GetMapping(value="/api/select/&#123;id&#125;") String getUserEntry(@PathVariable("id") long id);&#125; Api服务123456789101112@RestController@RequestMapping(value="/api/invoke")public class FeignController &#123; @Autowired private SimpleFeignService simpleFeignService; @GetMapping(value="/select/&#123;id&#125;") public String remoteInvoke(@PathVariable long id) &#123; return simpleFeignService.getUserEntry(id); &#125;&#125; 配置示例12345678910server.port=9090spring.application.name=consumer-feign# 心跳时间，即服务续约间隔时间（缺省为30s）eureka.instance.lease-renewal-interval-in-seconds=5# 发呆时间，即服务续约到期时间（缺省为90s）eureka.instance.lease-expiration-duration-in-seconds=10### 应用注册信息eureka.client.service-url.defaultZone=http://admin:pwd123@node1.test.com:8001/eureka/,http://admin:pwd123@node1.test.com:8002/eureka/,http://admin:pwd123@node3.test.com:8003/eureka/ 启动访问注册中心面板 访问请求查看12[root@localhost target ]$ curl -XGET http://172.30.12.197:9090/api/invoke/select/5测试数据5]]></content>
      <categories>
        <category>入门教程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解Java-虚拟机]]></title>
    <url>%2F2016%2F08%2F01%2F%E8%AF%AD%E8%A8%80%E8%AF%AD%E6%B3%95%2F%E7%90%86%E8%A7%A3Java-%E8%99%9A%E6%8B%9F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[JVM介绍Java虚拟机(Java Virtual Machine)简称JVM,其作用是加载与运行Class文件; JVM屏蔽了与操作系统平台相关的信息,使得Java程序只需要生成在Java虚拟机上运行的目标代码(字节码),就可在多种平台上不加修改的运行,这也是Java能够一次编译,到处运行的原因. JVM结构JVM需要管理Java运行期间的各种对象的生命周期,所以它在执行Java程序的时候会把它所管辖的内存分成若干个不同的数据区域,根据《Java虚拟机规范》规定,JVM包括下面几个运行时的内存区域: 程序计数器线程私有区域,用于指向当前线程下一条需要执行的字节码指令. 本地方法栈JVM采用本地方法堆栈来支持native方法的执行,此区域用于存储每个native方法调用的状态. 虚拟机栈线程私有区域, 虚拟机栈是一个后入先出的栈,用于描述Java方法执行的内存模型;每调用一个方法就会为每个方法生成一个栈帧,用于存储局部变量表、操作数栈、常量池引用等信息;每一个方法从调用直至执行完成的过程,就对应着一个栈帧在 Java 虚拟机栈中入栈和出栈的过程.可以通过-Xss这个虚拟机参数来指定一个程序的 Java 虚拟机栈内存大小. 该区域可能抛出以下异常： 当线程请求的栈深度超过最大值,会抛出 StackOverflowError 异常； 栈进行动态扩展时如果无法申请到足够内存,会抛出 OutOfMemoryError 异常. 堆线程共享区域,存放类实例以及数组;我们平时听到的GC(垃圾回收)就是在堆上进行的,所有对象实例都在这里分配内存.现代的垃圾收集器基本都是采用分代收集算法,主要思想是针对不同的对象采取不同的垃圾回收算法.虚拟机把 Java 堆分成以下三块： 新生代(Young Generation) 老年代(Old Generation) 永久代(Permanent Generation) 当一个对象被创建时,它首先进入新生代,之后有可能被转移到老年代中.新生代存放着大量的生命很短的对象,因此新生代在三个区域中垃圾回收的频率最高.为了更高效地进行垃圾回收,把新生代继续划分成以下三个空间： Eden(伊甸园) From Survivor(幸存者) To Survivor Java 堆不需要连续内存,并且可以动态增加其内存,增加失败会抛出 OutOfMemoryError 异常.可以通过-Xms和-Xmx两个虚拟机参数来指定一个程序的Java堆内存大小,第一个参数设置初始值,第二个参数设置最大值. 方法区线程共享区域,用于存储被虚拟机加载的类信息、final常量、静态变量、编译器即时编译后的代码等数据等数据；和Java堆一样不需要连续的内存,并且可以动态扩展,动态扩展失败一样会抛出 OutOfMemoryError 异常.对这块区域进行垃圾回收的主要目标是对常量池的回收和对类的卸载,但是一般比较难实现. JDK1.7之前,HotSpot虚拟机把它当成PermGen space(永久代)来进行垃圾回收(JDK1.7中的永久代). JDK1.8之后,取消了永久代,用metaspace(元空间)区替代(JDK1.8中的元空间). 运行时常量池运行时常量池,存放的为类中的固定的常量信息、方法和Field的引用信息(编译器生成,会在类加载后被放入这个区域)等,是方法区的一部分；其空间从方法区域中分配, 除了在编译期生成的常量,还允许动态生成; 例如: String 类的 intern(). 垃圾收集程序计数器、虚拟机栈和本地方法栈这三个区域属于线程私有的，只存在于线程的生命周期内，线程结束之后也会消失，因此不需要对这三个区域进行垃圾回收。垃圾回收主要是针对 Java 堆和方法区进行. 对新生代的对象的收集称为minor GC； 对旧生代的对象的收集称为Full GC； 程序中主动调用System.gc()强制执行的GC为Full GC。 GC介绍 内存处理器是编程人员容易出现问题的地方,忘记或者错误的内存回收导致程序或者系统的不稳定甚至崩溃,所以我们要及时的对内存中的一些无用对象清楚和回收,这个过程叫做垃圾回收,简称GC(Gabage Collection). Java中的GC功能可以自动监测对象是否超过作用域从而达到自动回收内存的目的;从而有效的防止内存泄露, GC中用于回收的方法称为收集器,由于GC需要消耗一些资源和时间,Java在对对象的生命周期特征进行分析后,按照新生代、旧生代的方式来对对象进行收集,以尽可能的缩短GC对应用造成的暂停. 垃圾回收器通常是作为一个单独的低优先级的线程运行,不可预知的情况下对内存堆中已经死亡的或者长时间没有使用的对象进行清除和回收,程序员不能实时的调用垃圾回收器对某个对象或所有对象进行垃圾回收.回收机制有分代复制垃圾回收、标记垃圾回收、增量垃圾回收等方式 基本概念 并行(Parallel): 指多条垃圾收集线程并行工作,但此时用户线程仍然处于等待状态. 并发(Concurrent):指用户线程与垃圾收集线程同时执行(但不一定是并行的,可能会交替执行),用户程序在继续运行,而垃圾收集程序运行于另一个CPU上. STW(Stop The World):在执行垃圾收集算法时,除了垃圾收集线程外,Java应用程序的其他线程都被挂起的现象(Native 代码可以执行). 对象引用类型不同的对象引用类型,GC会采用不同的方法进行回收,JVM对象的引用分为了四种类型. 强引用 默认情况下,对象采用的均为强引用(这个对象的实例没有其他对象引用,GC时才会被回收) 使用new一个新对象的方式来创建强引用. 1Object obj = new Object(); 软引用 软引用是Java中提供的一种比较适合于缓存场景的应用(只有在内存不够用的情况下才会被GC). 虚拟机在发生OutOfMemory时,肯定是没有软引用存在的. 使用 SoftReference 类来创建软引用. 123Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj);obj = null; // 使对象只被软引用关联 弱引用 弱引用与软引用类似,都是作为缓存来使用; 但与软引用不同,弱引用在进行垃圾回收时,是一定会被回收掉的,因此其生命周期只存在于一个垃圾回收周期内. 使用 WeakReference 类来实现弱引用. 123Object obj = new Object();WeakReference&lt;Object&gt; wf = new WeakReference&lt;Object&gt;(obj);obj = null; 虚引用 虚引用只是用来得知对象是否被GC, 又称为幽灵引用或者幻影引用.一个对象是否有虚引用的存在,完全不会对其生存时间构成影响,也无法通过虚引用取得一个对象实例. 使用 PhantomReference 来实现虚引用. 123Object obj = new Object();PhantomReference&lt;Object&gt; pf = new PhantomReference&lt;Object&gt;(obj);obj = null; 对象判死算法 由于程序计数器、Java虚拟机栈、本地方法栈都是线程独享,其占用的内存也是随线程生而生,随线程结束而回收.Java堆和方法区则不同,属于线程共享区域,是GC的所关注的部分;在堆中几乎存在着所有对象,GC之前需要考虑哪些对象还活着不能回收，哪些对象已经死去可以回收. 有两种算法可以判定对象是否存活: 引用计数算法给对象中添加一个引用计数器，每当一个地方应用了对象，计数器加1；当引用失效，计数器减1；当计数器为0表示该对象已死、可回收。但是它很难解决两个对象之间相互循环引用的情况(此时引用计数器永远不为 0，导致无法对它们进行回收)。 12345678910public class ReferenceCountingGC &#123; public Object instance = null; public static void main(String[] args) &#123; ReferenceCountingGC objectA = new ReferenceCountingGC(); ReferenceCountingGC objectB = new ReferenceCountingGC(); objectA.instance = objectB; objectB.instance = objectA; &#125;&#125; 可达性分析算法通过一系列称为“GC Roots”的对象作为起点，从这些节点开始向下搜索，搜索所走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连（即对象到GC Roots不可达），则证明此对象已死、可回收。 Java 中 GC Roots 一般包含以下内容： 虚拟机栈中引用的对象 本地方法栈中引用的对象 方法区中类静态属性引用的对象 方法区中的常量引用的对象 在主流的商用程序语言（如我们的Java）的主流实现中，都是通过可达性分析算法来判定对象是否存活的。 垃圾收集算法标记-清除算法最基础的算法，分标记和清除两个阶段：首先标记处所需要回收的对象，在标记完成后统一回收所有被标记的对象.它有两点不足： 效率问题，标记和清除过程都效率不高； 空间问题，标记清除之后会产生大量不连续的内存碎片（类似于我们电脑的磁盘碎片），空间碎片太多导致需要分配大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾回收动作。 复制算法为了解决效率问题，出现了”复制”算法,他将可用内存按容量划分为大小相等的两块,每次只需要使用其中一块。当一块内存用完了，将还存活的对象复制到另一块上面，然后再把刚刚用完的内存空间一次清理掉。这样就解决了内存碎片问题，但是代价就是可以用内容就缩小为原来的一半. 说明: 现在的商业虚拟机都采用这种收集算法来回收新生代，但是并不是将内存划分为大小相等的两块，而是分为一块较大的 Eden 空间和两块较小的 Survivor 空间，每次使用 Eden 空间和其中一块 Survivor。在回收时，将 Eden 和 Survivor 中还存活着的对象一次性复制到另一块 Survivor 空间上，最后清理 Eden 和使用过的那一块 Survivor。HotSpot 虚拟机的 Eden 和 Survivor 的大小比例默认为 8:1，保证了内存的利用率达到 90%。如果每次回收有多于 10% 的对象存活，那么一块 Survivor 空间就不够用了，此时需要依赖于老年代进行分配担保，也就是借用老年代的空间存储放不下的对象。 标记-整理算法复制算法在对象存活率较高时就会进行频繁的复制操作，效率将降低。因此又有了标记-整理算法，标记过程同标记-清除算法，但是在后续步骤不是直接对对象进行清理，而是让所有存活的对象都向一侧移动，然后直接清理掉端边界以外的内存。 分代收集算法当前商业虚拟机的GC都是采用分代收集算法,这种算法并没有什么新的思想,而是根据对象存活周期的不同将堆分为:新生代和老年代,方法区称为永久代(在新的版本中已经将永久代废弃,引入了元空间的概念,永久代使用的是JVM内存而元空间直接使用物理内存).这样就可以根据各个年代的特点采用不同的收集算法. 一般将 Java 堆分为新生代和老年代。 新生代使用：复制算法 老年代使用：标记 - 清理 或者 标记 - 整理 算法 垃圾收集器垃圾收集算法是方法论，垃圾收集器是具体实现。JVM规范对于垃圾收集器的应该如何实现没有任何规定，因此不同的厂商、不同版本的虚拟机所提供的垃圾收集器差别较大，这里只看HotSpot虚拟机。 JDK7/8后，HotSpot虚拟机所有收集器及组合(连线表示垃圾收集器可以配合使用). 单线程与并行（多线程）：单线程指的是垃圾收集器只使用一个线程进行收集，而并行使用多个线程。 串行与并发：串行指的是垃圾收集器与用户程序交替执行，这意味着在执行垃圾收集的时候需要停顿用户程序；并发指的是垃圾收集器和用户程序同时执行。除了 CMS 和 G1 之外，其它垃圾收集器都是以串行的方式执行。 Serial收集器Serial收集器是最基本、历史最久的收集器，曾是新生代手机的唯一选择。他是单线程的，只会使用一个CPU或一条收集线程去完成垃圾收集工作，并且它在收集的时候，必须暂停其他所有的工作线程，直到它结束，即“Stop the World”。停掉所有的用户线程。尽管如此，它仍然是虚拟机运行在client模式下的默认新生代收集器：简单而高效（与其他收集器的单个线程相比，因为没有线程切换的开销等). 工作示意图如下: ParNew收集器ParNew收集器是Serial收集器的多线程版本，除了使用了多线程之外，其他的行为(收集算法、stop the world、对象分配规则、回收策略等)同Serial收集器一样。 是许多运行在Server模式下的JVM中首选的新生代收集器，其中一个很重还要的原因就是除了Serial之外，只有他能和老年代的CMS收集器配合工作。 默认开启的线程数量与 CPU 数量相同,可以使用-XX:ParallelGCThreads参数来设置线程数. 工作示意图如下: Parallel Scavenge 收集器与 ParNew 一样是并行的多线程收集器. 它的目标是达到一个可控的吞吐量（就是CPU运行用户代码的时间与CPU总消耗时间的比值，即 吞吐量=行用户代码的时间/[行用户代码的时间+垃圾收集时间]），这样可以高效率的利用CPU时间，尽快完成程序的运算任务，适合在后台运算而不需要太多交互的任务。 提供了两个参数用于精确控制吞吐量，分别是控制最大垃圾收集停顿时间 -XX:MaxGCPauseMillis 参数以及直接设置吞吐量大小的 -XX:GCTimeRatio 参数（值为大于 0 且小于 100 的整数）。缩短停顿时间是以牺牲吞吐量和新生代空间来换取的：新生代空间变小，垃圾回收变得频繁，导致吞吐量下降。 还提供了一个参数 -XX:+UseAdaptiveSizePolicy，这是一个开关参数，打开参数后，就不需要手工指定新生代的大小（-Xmn）、Eden 和 Survivor 区的比例（-XX:SurvivorRatio）、晋升老年代对象年龄（-XX:PretenureSizeThreshold）等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量，这种方式称为 GC 自适应的调节策略（GC Ergonomics）。 Serial Old收集器Serial 收集器的老年代版本，单线程，“标记整理”算法，主要是给Client模式下的虚拟机使用。如果用在 Server 模式下，它有两大用途： 在 JDK 1.5 以及之前版本（Parallel Old 诞生以前）中与 Parallel Scavenge 收集器搭配使用。作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failure 时使用。 工作示意图如下: Parallel Old收集器是 Parallel Scavenge 收集器的老年代版本。 在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器。 参考文档 Java虚拟机规范SE8 深入理解Java虚拟机]]></content>
      <categories>
        <category>语言语法</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>JMM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper运维配置]]></title>
    <url>%2F2016%2F07%2F17%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2FZookeeper%E8%BF%90%E7%BB%B4%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Zookeeper介绍 Zookeeper 是一个基于 Google Chubby 论文实现的一款解决分布式数据一致性问题的开源实现，方便了依赖 Zookeeper 的应用实现 数据发布 / 订阅、负载均衡、服务注册与发现、分布式协调、事件通知、集群管理、Leader 选举、 分布式锁和队列 等功能 Zookeeper下载123456789101112131415161718192021[root@localhost cloud]# wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.3.6/zookeeper-3.3.6.tar.gz[root@localhost cloud]# tar -zxvd zookeeper-3.3.6.tar.gz[root@localhost cloud]# tree zookeeper-3.3.6/conf/zookeeper-3.3.6/conf/├── configuration.xsl├── log4j.properties└── zoo_sample.cfg0 directories, 3 files[root@localhost cloud]# tree zookeeper-3.3.6/bin/zookeeper-3.3.6/bin/├── README.txt├── zkCleanup.sh├── zkCli.cmd├── zkCli.sh├── zkEnv.cmd├── zkEnv.sh├── zkServer.cmd└── zkServer.sh0 directories, 8 files 配置说明: Zookeeper配置目录zookeeper的安装配置目录在zookeeper-x.x.x/conf/目录下. zookeeper脚本目录zookeeper的启停脚本目录在zookeeper-x.x.x/bin/目录下.(.sh和.cmd分布适用于unix系统和windows系统) 脚本 说明 zkCleanup 清理Zookeeper历史数据, 包括事务日志文件和快照数据文件 zkEnv 设置Zookeeper的环境变量 zkServer Zookeeper服务器的启动、停止、重启脚本 zkCli Zookeeper的命令行客户端 单机环境搭建zookeeper安装配置大致分为两个步骤: 配置服务器标识文件 和 修改配置zoo.cfg文件. 创建服务器标识文件1234567[root@localhost cloud]# pwd/export/cloud[root@localhost cloud]# cd zookeeper-3.3.6[root@localhost zookeeper-3.3.6]# mkdir data[root@localhost zookeeper-3.3.6]# echo 1 &gt; data/myid[root@localhost zookeeper-3.3.6]# cat data/myid1 示例中我们创建了一个目录/export/cloud/zookeeper-3.3.6/data, 该目录为Zookeeper的数据存储目录, 并在该目录下创建了一个myid文件, 文件写入内容1，用来标识当前实例的标识值为1. 修改zoo.cfg文件并启动zookeeper的配置文件在zookeeper-x.x.x/conf目录下,默认的示例文件名为zoo_sample.cfg 123456789101112131415161718[root@localhost cloud]# cd zookeeper-3.3.6/conf/[root@localhost conf]# cp zoo_sample.cfg zoo.cfg[root@localhost conf]# vim zoo.cfg[root@localhost conf]# cat zoo.cfg# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial# synchronization phase can takeinitLimit=10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.dataDir=/export/cloud/zookeeper-3.3.6/data# the port at which the clients will connectclientPort=2181# 配置sever1节点server.1=127.0.0.1:2888:3888 示例中只是修改了dataDir路径为我们上一步创建的数据存储目录/opt/zookeeper-3.4.9/data, 并增加了一个服务通讯标识server.1=127.0.0.1:2888:3888. 参数说明: tickTime: 用来指示服务器之间或客户端与服务器之间维护心跳机制的最小时间单元,Session最小过期时间默认为两倍的tickTime. initLimit: 集群中的Leader节点和Follower节点之间初始连接时能容忍的最多心跳数. syncLimit: 集群中的Leader节点和Follower节点之间请求和应答时能容忍的最多心跳数. dataDir: Zookeeper保存服务器存储快照文件的目录(默认目录：/tmp/zookeeper) dataLogDir: 用来存储服务器事务日志 clientPort: 客户端连接Zookeeper服务器的端口,Zookeeper会监听这个端口接受客户端的访问请求(默认:2181) 启动Zookeeper1234567[root@localhost cloud]# pwd/export/cloud[root@localhost cloud]# zookeeper-3.3.6/bin/zkServer.sh start &amp;[root@localhost cloud]# zookeeper-3.3.6/bin/zkServer.sh statusJMX enabled by defaultUsing config: /export/cloud/zookeeper-3.3.6/bin/../conf/zoo.cfgMode: standalone 访问Zookeeper123456789101112131415161718192021[root@localhost cloud]# zookeeper-3.3.6/bin/zkCli.shConnecting to localhost:2181[zk: localhost:2181(CONNECTED) 1] ls /[zookeeper][zk: localhost:2181(CONNECTED) 2] create /zkEdu 123Created /zkEdu[zk: localhost:2181(CONNECTED) 3] ls /[zookeeper, zkEdu][zk: localhost:2181(CONNECTED) 4] get /zkEdu123cZxid = 0x2ctime = Wed Jan 10 18:02:05 CST 2018mZxid = 0x2mtime = Wed Jan 10 18:02:05 CST 2018pZxid = 0x2cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 3numChildren = 0 伪集群环境搭建伪集群采用一台机器多个示例的方式搭建 数据存储目录和服务ID配置 机器 数据存储目录 服务ID标识文件 服务ID标识值 客户端通讯端口 集群内主从通讯端口 集群内Lead选举端口 127.0.0.1 zookeeper-3.3.6-node1/data zookeeper-3.3.6-node1/data/myid 1 2181 2887 2888 127.0.0.1 zookeeper-3.3.6-node2/data zookeeper-3.3.6-node2/data/myid 2 2182 2888 3888 127.0.0.1 zookeeper-3.3.6-node3/data zookeeper-3.3.6-node3/data/myid 3 2183 2889 3889 Zookeeper节点文件配置节点1 “/export/cloud/zookeeper-3.3.6-node1/conf/zoo.cfg”的配置 12345678tickTime=2000initLimit=10syncLimit=5dataDir=/export/cloud/zookeeper-3.3.6-node1/dataclientPort=2181server.1=127.0.0.1:2888:3888server.2=127.0.0.1:2888:3888server.3=127.0.0.1:2889:3889 节点2 “/export/cloud/zookeeper-3.3.6-node2/conf/zoo.cfg”的配置 12345678tickTime=2000initLimit=10syncLimit=5dataDir=/export/cloud/zookeeper-3.3.6-node2/dataclientPort=2182server.1=127.0.0.1:2888:3888server.2=127.0.0.1:2888:3888server.3=127.0.0.1:2889:3889 节点3 “/export/cloud/zookeeper-3.3.6-node3/conf/zoo.cfg”的配置 12345678tickTime=2000initLimit=10syncLimit=5dataDir=/export/cloud/zookeeper-3.3.6-node2/dataclientPort=2183server.1=127.0.0.1:2888:3888server.2=127.0.0.1:2888:3888server.3=127.0.0.1:2889:3889 集群访问分别启动三个节点,示例展示在节点2上进行集群状态监测和访问 12345[[root@localhost cloud]# zookeeper-3.3.6-node2/bin/zkServer.sh statusJMX enabled by defaultUsing config: /export/cloud/zookeeper-3.3.6-node2/bin/../conf/zoo.cfgMode: follower[root@localhost cloud]# zookeeper-3.3.6-node2/bin/zkCli.sh -server 127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183 集群环境搭建zookeeper的集群环境基本和单机搭建思路一致.各台机器上部署一份zookeeper示例, 配置各自的服务标识ID, 在zoo.cfg中配置统一的服务标识集合.假设集群中有3台机器: 172.30.12.197,172.30.12.198,172.30.12.199 数据存储目录和服务ID配置 机器 数据存储目录 服务ID标识文件 服务ID标识值 客户端通讯端口 集群内主从通讯端口 集群内Lead选举端口 172.30.12.197 zookeeper-3.3.6/data zookeeper-3.3.6/data/myid 1 2181 2888 3888 172.30.12.198 zookeeper-3.3.6/data zookeeper-3.3.6/data/myid 2 2181 2888 3888 172.30.12.199 zookeeper-3.3.6/data zookeeper-3.3.6/data/myid 3 2181 2888 3888 Zookeeper节点文件配置三台机器实例采用同样的配置 12345678tickTime=2000initLimit=10syncLimit=5dataDir=/export/cloud/zookeeper-3.3.6/dataclientPort=2181server.1=172.30.12.197:2888:3888server.2=172.30.12.198:2888:3888server.3=172.30.12.199:2888:3888 连接到集群1[root@localhost cloud]# zookeeper-3.3.6/bin/zkCli.sh -server 172.30.12.197:2181,172.30.12.198:2181,172.30.12.199:2181 Zookeeper四字短语ZooKeeper 支持某些特定的四字命令字母与其的交互。它们大多是查询命令，用来获取 ZooKeeper 服务的当前状态及相关信息。用户在客户端可以通过 telnet 或 nc 向 ZooKeeper 提交相应的命令。 命令 功能描述 使用示例 conf 输出相关服务配置详情信息 [root@localhost cloud]# echo conf &#124; nc localhost 2181 cons 列出所有连接到服务器的客户端的完全的连接 / 会话的详细信息。包括“接受 / 发送”的包数量、会话 id 、操作延迟、最后的操作执行等等信息。 [root@localhost cloud]# echo cons &#124; nc localhost 2181 dump 列出未经处理的会话和临时节点（只在leader上有效） [root@localhost cloud]# echo dump &#124; nc localhost 2181 envi 输出关于服务环境的详细信息 [root@localhost cloud]# echo envi &#124; nc localhost 2181 reqs 列出未经处理的请求 [root@localhost cloud]# echo reqs &#124; nc localhost 2181 ruok 测试服务是否处于正确状态。如果确实如此，那么服务返回“imok ”，否则不做任何相应。 [root@localhost cloud]# echo ruok &#124; nc localhost 2181 stat 输出关于性能和连接的客户端的列表 [root@localhost cloud]# echo stat &#124; nc localhost 2181 wchs 列出服务器 watch 的详细信息。 [root@localhost cloud]# echo wchs &#124; nc localhost 2181 wchc 通过 session 列出服务器 watch 的详细信息，它的输出是一个与watch 相关的会话的列表。 [root@localhost cloud]# echo wchc &#124; nc localhost 2181 wchp 通过路径列出服务器 watch 的详细信息。它输出一个与 session相关的路径。 [root@localhost cloud]# echo wchp &#124; nc localhost 2181 srvr 输出服务器的详细信息。zk版本、接收/发送包数量、连接数、模式（leader/follower）、节点总数 [root@localhost cloud]# echo srvr &#124; nc localhost 2181 srst 重置服务器统计信息 [root@localhost cloud]# echo srst &#124; nc localhost 2181 参考文档 https://yuzhouwan.com/posts/31915/]]></content>
      <categories>
        <category>运维部署</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[轻量级规则引擎EasyRules]]></title>
    <url>%2F2016%2F07%2F15%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2F%E8%BD%BB%E9%87%8F%E7%BA%A7%E8%A7%84%E5%88%99%E5%BC%95%E6%93%8EEasyRules%2F</url>
    <content type="text"><![CDATA[EasyRule概述Easy-Rules是一款轻量级的规则引擎. 框架特点 轻量级类库和容易上手 基于POJO的开发与注解的编程模型 方便且适用于java的抽象的业务模型规则 支持从简单的规则创建组合规则 官方地址: https://github.com/j-easy/easy-rules 应用示例入门示例定义一个规则,在任何情况下都输出Hello world. 1234567891011121314151617181920212223242526272829303132public class HelloRoles &#123; @Rule(name = "Hello World Rule", description = "Always say hello world") public static class HelloWorldRule &#123; @Condition public boolean when() &#123; return true; &#125; @Action public void then() throws Exception &#123; System.out.println("hello world"); &#125; &#125; public static void main(String[] args) &#123; // 创建规则 Rules rules = new Rules(); rules.register(new HelloWorldRule()); // 定义行为 Facts facts = new Facts(); // 将规则应用到行为上 RulesEngine rulesEngine = new DefaultRulesEngine(); rulesEngine.fire(rules, facts); &#125;&#125; 执行输出 12345678[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Engine parameters &#123; skipOnFirstAppliedRule = false, skipOnFirstNonTriggeredRule = false, skipOnFirstFailedRule = false, priorityThreshold = 2147483647 &#125;hello world[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Registered rules:[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Rule &#123; name = 'Hello World Rule', description = 'Always say hello world', priority = '2147483646'&#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Known facts:[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Rules evaluation started[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'Hello World Rule' triggered[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'Hello World Rule' performed successfully 通过API可以看到我们可以注册多个规则, 多个规则之间处理顺序可以通过设定优先级来定义, 规则优先级的上限值默认是Integer.MAX_VALUE(2147483647). 相同的优先级执行顺序按照规则名称的自然顺序执行. 优先级示例不同优先级代码示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class WeatherRules &#123; @Rule(name = "weather rule1", description = "simple rule1", priority = 2) public static class WeatherRule1 &#123; @Condition public boolean when(@Fact("rain") boolean rain) &#123; return rain; &#125; @Action public void then() &#123; System.out.println("It rains, with rule1 ."); &#125; &#125; @Rule(name = "weather rule2", description = "simple rule2", priority = 1) public static class WeatherRule2 &#123; @Condition public boolean when(@Fact("rain") boolean rain) &#123; return rain; &#125; @Action public void then() &#123; System.out.println("It rains, with rule2 ."); &#125; &#125; public static void main(String[] args) throws Exception&#123; Rules rules = new Rules(); rules.register(new WeatherRule1()); rules.register(new WeatherRule2()); Facts facts = new Facts(); facts.put("rain", true); RulesEngine rulesEngine = new DefaultRulesEngine(); rulesEngine.fire(rules, facts); &#125;&#125; 执行输出 12345678910111213[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Engine parameters &#123; skipOnFirstAppliedRule = false, skipOnFirstNonTriggeredRule = false, skipOnFirstFailedRule = false, priorityThreshold = 2147483647 &#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Registered rules:[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Rule &#123; name = 'weather rule2', description = 'simple rule2', priority = '1'&#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Rule &#123; name = 'weather rule1', description = 'simple rule1', priority = '2'&#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Known facts:[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Fact &#123; rain : true &#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Rules evaluation started[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule2' triggered[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule2' performed successfully[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule1' triggered[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule1' performed successfullyIt rains, with rule2 .It rains, with rule1 . 通过注册规则可以看到, 注册时是按照优先级注册的, 并不是按照我们代码的顺序注册的. 执行校验输出时也是按照优先级校验然后匹配给出结果. 根据@Condition条件命中来进行对应的规则输出(相同条件可以定制多个规则, 规则命中则执行) 相同优先级代码示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class WeatherRules &#123; @Rule(name = "weather rule1", description = "simple rule with b", priority = 1) public static class WeatherRule1 &#123; @Condition public boolean when(@Fact("rain") boolean rain) &#123; return rain; &#125; @Action public void then() &#123; System.out.println("It rains, with rule1 ."); &#125; &#125; @Rule(name = "weather rule2", description = "simple rule with a", priority = 1) public static class WeatherRule2 &#123; @Condition public boolean when(@Fact("rain") boolean rain) &#123; return rain; &#125; @Action public void then() &#123; System.out.println("It rains, with rule2 ."); &#125; &#125; public static void main(String[] args) throws Exception&#123; Rules rules = new Rules(); rules.register(new WeatherRule2()); rules.register(new WeatherRule1()); Facts facts = new Facts(); facts.put("rain", true); RulesEngine rulesEngine = new DefaultRulesEngine(); rulesEngine.fire(rules, facts); &#125;&#125; 执行输出 12345678910111213[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Engine parameters &#123; skipOnFirstAppliedRule = false, skipOnFirstNonTriggeredRule = false, skipOnFirstFailedRule = false, priorityThreshold = 2147483647 &#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Registered rules:[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Rule &#123; name = 'weather rule1', description = 'simple rule with b', priority = '1'&#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Rule &#123; name = 'weather rule2', description = 'simple rule with a', priority = '1'&#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Known facts:[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Fact &#123; rain : true &#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Rules evaluation started[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule1' triggered[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule1' performed successfully[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule2' triggeredIt rains, with rule1 .[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule2' performed successfullyIt rains, with rule2 . 可以看到, 注册顺序在优先级相同的时候是按照规则名称的自然顺序注册, 校验的时候按照注册顺序校验. 这种场景我们也可以通过下面方式实现,同一事件条件绑定多个依赖逻辑. 123456789101112131415161718192021222324252627282930313233343536public class WeatherRules &#123; @Rule(name = "weather rule Composite", description = "simple rule with order action", priority = 1) public static class WeatherRuleComposite&#123; @Condition public boolean when(@Fact("rain") boolean rain) &#123; return rain; &#125; @Action(order = 1) public void then1() &#123; System.out.println("It rains, with rule1 ."); &#125; @Action(order = 2) public void then2() &#123; System.out.println("It rains, with rule2 ."); &#125; &#125; public static void main(String[] args) throws Exception&#123; Rules rules = new Rules(); rules.register(new WeatherRuleComposite()); Facts facts = new Facts(); facts.put("rain", true); RulesEngine rulesEngine = new DefaultRulesEngine(); rulesEngine.fire(rules, facts); &#125;&#125; 执行输出 12345678910[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Engine parameters &#123; skipOnFirstAppliedRule = false, skipOnFirstNonTriggeredRule = false, skipOnFirstFailedRule = false, priorityThreshold = 2147483647 &#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Registered rules:[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Rule &#123; name = 'weather rule Composite', description = 'simple rule with order action', priority = '1'&#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Known facts:[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Fact &#123; rain : true &#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Rules evaluation started[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule Composite' triggered[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule Composite' performed successfullyIt rains, with rule1 .It rains, with rule2 . 硬编码示例上面的示例代码也可以通过硬编码方式实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class ApiRules &#123; public static Rule ruleComposite()&#123; Rule rule = new RuleBuilder() .name("weather rule Composite") .description("simple rule with order action") .priority(1) .when(new Condition() &#123; @Override public boolean evaluate(Facts facts) &#123; if((Boolean) facts.get("rain") == true)&#123; return true; &#125; return false; &#125; &#125;) .then(new Action() &#123; @Override public void execute(Facts facts) throws Exception &#123; if((Boolean) facts.get("rain") == true)&#123; System.out.println("It rains, with rule1 ."); &#125; &#125; &#125;) .then(new Action() &#123; @Override public void execute(Facts facts) throws Exception &#123; if((Boolean) facts.get("rain") == true)&#123; System.out.println("It rains, with rule2 ."); &#125; &#125; &#125;) .build(); return rule; &#125; public static void main(String[] args) throws Exception&#123; Rules rules = new Rules(); rules.register(ruleComposite()); Facts facts = new Facts(); facts.put("rain", true); RulesEngine rulesEngine = new DefaultRulesEngine(); rulesEngine.fire(rules, facts); &#125;&#125; 多规则示例代码示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class WeatherRules &#123; @Rule(name = "weather rule1", description = "simple rule1", priority = 1) public static class WeatherRule1 &#123; @Condition public boolean when(@Fact("rain") boolean rain) &#123; return rain; &#125; @Action public void then() &#123; System.out.println("It rains, with rule1 ."); &#125; &#125; @Rule(name = "weather rule2", description = "simple rule2", priority = 3) public static class WeatherRule2 &#123; @Condition public boolean when(@Fact("rain") boolean rain) &#123; return rain; &#125; @Action public void then() &#123; System.out.println("It rains, with rule2 ."); &#125; &#125; @Rule(name = "weather rule3", description = "simple rule3", priority = 2) public static class WeatherRule3 &#123; @Condition public boolean when(@Fact("sun") boolean sun) &#123; return sun; &#125; @Action public void then() &#123; System.out.println("It sun, with rule3 ."); &#125; &#125; public static void main(String[] args) throws Exception&#123; Rules rules = new Rules(); rules.register(new WeatherRule1()); rules.register(new WeatherRule2()); rules.register(new WeatherRule3()); Facts facts = new Facts(); facts.put("rain", true); facts.put("sun", true); RulesEngine rulesEngine = new DefaultRulesEngine(); rulesEngine.fire(rules, facts); &#125;&#125; 执行输出 123456789101112131415161718It rains, with rule1 .[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Engine parameters &#123; skipOnFirstAppliedRule = false, skipOnFirstNonTriggeredRule = false, skipOnFirstFailedRule = false, priorityThreshold = 2147483647 &#125;It sun, with rule3 .[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Registered rules:[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Rule &#123; name = 'weather rule1', description = 'simple rule1', priority = '1'&#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Rule &#123; name = 'weather rule3', description = 'simple rule3', priority = '2'&#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Rule &#123; name = 'weather rule2', description = 'simple rule2', priority = '3'&#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Known facts:[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Fact &#123; rain : true &#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Fact &#123; sun : true &#125;[main] INFO org.jeasy.rules.core.DefaultRulesEngineListener - Rules evaluation started[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule1' triggered[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule1' performed successfully[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule3' triggered[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule3' performed successfully[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule2' triggered[main] INFO org.jeasy.rules.core.DefaultRuleListener - Rule 'weather rule2' performed successfullyIt rains, with rule2 . 执行按照规则注册的优先级, 规则1&gt;规则3&gt;规则2, 进行场景条件判断, rain=true时输出rains相关打印、sun=true时输出 sun相关打印. 参考文档 https://www.cnblogs.com/jiaan-geng/p/4939293.html http://maxwoods.net/wordpress/archives/1668 https://www.jianshu.com/p/adbf35291a04]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>EasyRules</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring基本应用示例]]></title>
    <url>%2F2016%2F07%2F15%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2FSpring%E5%9F%BA%E6%9C%AC%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[消息转换器 在Spring中org.springframework.http.converter.HttpMessageConverter规范中定义了Http请求和响应的消息转换规范, 我们知道SpringMvc可以接收不同的消息形式,也可以将不同的消息形式响应回去(最常见的是json);这些消息所蕴含的”有效信息”是一致的,那么各种不同的消息转换器,都会生成同样的转换结果. 至于各种消息间解析细节的不同,就被屏蔽在不同的HttpMessageConverter实现类中. SpringMVC中使用FastJson作为转换器通过SpringMvc中message-converts配置FastJson作为转换器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;!-- 默认的注解映射的支持，org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping --&gt;&lt;mvc:annotation-driven content-negotiation-manager="contentNegotiationManager"&gt; &lt;mvc:message-converters register-defaults="true"&gt; &lt;!-- 将Jackson2HttpMessageConverter的默认格式化输出为true --&gt; &lt;!-- 配置Fastjson支持 --&gt; &lt;bean class="com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter"&gt; &lt;property name="supportedMediaTypes"&gt; &lt;list&gt; &lt;value&gt;text/html;charset=UTF-8&lt;/value&gt; &lt;value&gt;application/json&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="features"&gt; &lt;list&gt; &lt;!-- 输出key时是否使用双引号 --&gt; &lt;value&gt;QuoteFieldNames&lt;/value&gt; &lt;!-- 是否输出值为null的字段 --&gt; &lt;!-- &lt;value&gt;WriteMapNullValue&lt;/value&gt; --&gt; &lt;!-- 数值字段如果为null,输出为0,而非null --&gt; &lt;value&gt;WriteNullNumberAsZero&lt;/value&gt; &lt;!-- List字段如果为null,输出为[],而非null --&gt; &lt;value&gt;WriteNullListAsEmpty&lt;/value&gt; &lt;!-- 字符类型字段如果为null,输出为"",而非null --&gt; &lt;value&gt;WriteNullStringAsEmpty&lt;/value&gt; &lt;!-- Boolean字段如果为null,输出为false,而非null --&gt; &lt;value&gt;WriteNullBooleanAsFalse&lt;/value&gt; &lt;!-- null String不输出 --&gt; &lt;value&gt;WriteNullStringAsEmpty&lt;/value&gt; &lt;!-- null String也要输出 --&gt; &lt;!-- &lt;value&gt;WriteMapNullValue&lt;/value&gt; --&gt; &lt;!-- Date的日期转换器 --&gt; &lt;value&gt;WriteDateUseDateFormat&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/mvc:message-converters&gt;&lt;/mvc:annotation-driven&gt;&lt;!-- REST中根据URL后缀自动判定Content-Type及相应的View --&gt;&lt;bean id="contentNegotiationManager" class="org.springframework.web.accept.ContentNegotiationManagerFactoryBean"&gt; &lt;property name="mediaTypes" &gt; &lt;map&gt; &lt;entry key="json" value="application/json"/&gt; &lt;/map&gt; &lt;/property&gt; &lt;!-- 这里是否忽略掉accept header，默认就是false --&gt; &lt;property name="ignoreAcceptHeader" value="true"/&gt; &lt;property name="favorPathExtension" value="true"/&gt;&lt;/bean&gt; Fastjson轻量级属性转换定义示例对象模型做fastjson轻量注解配置, 更多配置参考https://github.com/alibaba/fastjson/wiki/JSONField 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class ModelTest implements Serializable&#123; // 使用ordinal指定字段 @JSONField(ordinal = 1) private Long id; @JSONField(ordinal = 2) private String name; @JSONField(ordinal = 3) private Integer age; // 使用serialize/deserialize指定字段不序列化 @JSONField(deserialize = false, serialize = false) private String remark; // 配置date序列化和反序列使用yyyyMMdd日期格式 @JSONField(format="yyyy-MM-dd HH:mm:ss") private Date crateTime; // 配置属性序列化使用的名称 @JSONField(name = "updateTime", format="yyyy-MM-dd HH:mm:ss") private Date modifyTime; @JSONField(ordinal = 0) private DeleteEnum deleteEnum; public enum DeleteEnum &#123; DISABLE(1, "禁用"), ENABLE(2, "启用"); private int value; private String depict; DeleteEnum(int value, String depict) &#123; this.value = value; this.depict = depict; &#125; public static DeleteEnum findByValue(int value) &#123; switch (value) &#123; case 1: return DISABLE; case 2: return ENABLE; default: return null; &#125; &#125; &#125;// getter and setter&#125; 定义示例Controller示例完成序列化数据到前端和提交JSON数据转换成对象模型 1234567891011121314151617181920212223242526272829303132333435@Controller@RequestMapping(value = "/test")public class TestWebController &#123; private static final Logger LOG = LoggerFactory.getLogger(TestWebController.class); // 序列化对象到视图 @ResponseBody @RequestMapping(value = &#123;"/bean/data"&#125;, method = &#123;RequestMethod.GET, RequestMethod.POST&#125;) public Object toBody()&#123; ModelTest modelTest = new ModelTest(); modelTest.setAge(11); modelTest.setCrateTime(new Date()); modelTest.setModifyTime(new Date()); modelTest.setId(1L); modelTest.setName("测试Fastjson"); modelTest.setRemark("备注"); modelTest.setDeleteEnum(ModelTest.DeleteEnum.ENABLE); return ImmutableMap.&lt;String, String&gt;builder() .put("code", "0") .put("data", JSON.toJSONString(modelTest)).build(); &#125; // 解析JSON对象到实体模型 @ResponseBody @RequestMapping(value = &#123;"/bean/put"&#125;, method = &#123;RequestMethod.GET, RequestMethod.POST&#125;) public Object getBody(@RequestBody ModelTest modelTest)&#123; System.out.println(JSON.toJSONString(modelTest)); return ImmutableMap.&lt;String, String&gt;builder() .put("code", "0") .put("data", "ok").build(); &#125;&#125; 测试对象序列化到前端展示1234567891011@Test public void fastJSONTest01() throws Exception &#123; MvcResult mvcResult = this.mockMvc.perform(MockMvcRequestBuilders.post("/test/bean/data") .cookie(new Cookie("token", "F3AF5F1D14F534F677XF3A00E352C")) // 登录授权 .accept(MediaType.parseMediaType("application/json;charset=UTF-8"))) .andExpect(handler().handlerType(TestWebController.class)) // 验证执行的控制器类型 .andExpect(status().isOk()) // 验证执行状态 .andDo(print()) // 打印交互信息 .andReturn(); System.out.println(mvcResult.getResponse().getContentAsString()); &#125; 执行结果: 1234&#123; "code": "0", "data": "&#123;"crateTime":"2016-12-2808: 02: 50","deleteEnum":"ENABLE","updateTime":"2016-12-2808: 02: 50","id":1,"name":"测试Fastjson","age":11&#125;"&#125; 测试提交数据转换成模型1234567891011121314151617181920212223@Test public void fastJSONTest02() throws Exception &#123; ModelTest modelTest = new ModelTest(); modelTest.setAge(11); modelTest.setCrateTime(new Date()); modelTest.setModifyTime(new Date()); modelTest.setId(1L); modelTest.setName("测试Fastjson"); modelTest.setRemark("备注"); modelTest.setDeleteEnum(ModelTest.DeleteEnum.ENABLE); MvcResult mvcResult = this.mockMvc.perform(MockMvcRequestBuilders.post("/test/bean/put") .content(JSON.toJSONString(modelTest)) .contentType(MediaType.APPLICATION_JSON) .cookie(new Cookie("token", "F3AF5F1D14F534F677XF3A00E352C")) // 登录授权 .accept(MediaType.parseMediaType("application/json;charset=UTF-8"))) .andExpect(handler().handlerType(TestWebController.class)) // 验证执行的控制器类型 .andExpect(status().isOk()) // 验证执行状态 .andDo(print()) // 打印交互信息 .andReturn(); System.out.println(mvcResult.getResponse().getContentAsString()); &#125; 执行后台打印: 1&#123;"crateTime":"2016-12-28 08:04:33","deleteEnum":"ENABLE","updateTime":"2016-12-28 08:04:33","id":1,"name":"测试Fastjson","age":11&#125; 枚举绑定处理示例中我们看到序列化的枚举为枚举的name(). 如果我们想用枚举的寓意值进行传输过程中的映射时,可以这样做, 透传一个数值, 该数值和枚举在getter()和setter()方法上绑定即可. 修改示例的对象模型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public class ModelTest implements Serializable&#123; // 使用ordinal指定字段 @JSONField(ordinal = 1) private Long id; @JSONField(ordinal = 2) private String name; @JSONField(ordinal = 3) private Integer age; // 使用serialize/deserialize指定字段不序列化 @JSONField(deserialize = false, serialize = false) private String remark; // 配置date序列化和反序列使用yyyyMMdd日期格式 @JSONField(format="yyyy-MM-dd HH:mm:ss") private Date crateTime; // 配置属性序列化使用的名称 @JSONField(name = "updateTime", format="yyyy-MM-dd HH:mm:ss") private Date modifyTime; @JSONField(ordinal = 0) private DeleteEnum deleteEnum; private int enable; public enum DeleteEnum &#123; DISABLE(1, "禁用"), ENABLE(2, "启用"); private int value; private String depict; DeleteEnum(int value, String depict) &#123; this.value = value; this.depict = depict; &#125; public static DeleteEnum findByValue(int value) &#123; switch (value) &#123; case 1: return DISABLE; case 2: return ENABLE; default: return null; &#125; &#125; &#125; // 数值绑定枚举 public int getEnable() &#123; this.deleteEnum = DeleteEnum.findByValue(enable); return enable; &#125; // 数值绑定枚举 public void setEnable(int enable) &#123; this.enable = enable; this.deleteEnum = DeleteEnum.findByValue(enable); &#125; // 枚举映射数值 public DeleteEnum getDeleteEnum() &#123; this.enable = this.deleteEnum.value; return deleteEnum; &#125; // 枚举映射数值 public void setDeleteEnum(DeleteEnum deleteEnum) &#123; this.deleteEnum = deleteEnum; this.enable = this.deleteEnum.value; &#125;&#125; 我们保持示例中Controller类和测试用例的代码不变，执行用例. 测试对象序列化到前端展示执行结果 1234&#123; "code": "0", "data": "&#123;"crateTime":"2016-12-2808: 12: 52","deleteEnum":"ENABLE","enable":2,"updateTime":"2016-12-2808: 12: 52","id":1,"name":"测试Fastjson","age":11&#125;"&#125; 测试提交数据转换成模型 执行后台打印 1&#123;"crateTime":"2016-12-28 08:20:17","deleteEnum":"ENABLE","enable":2,"updateTime":"2016-12-28 08:20:17","id":1,"name":"测试Fastjson","age":11&#125; 修改用例模拟的实体对象, 测试数值到枚举的映射. 12345678ModelTest modelTest = new ModelTest();modelTest.setAge(11);modelTest.setCrateTime(new Date());modelTest.setModifyTime(new Date());modelTest.setId(1L);modelTest.setName("测试Fastjson");modelTest.setRemark("备注");modelTest.setEnable(1); 后台打印输出: 1&#123;"crateTime":"2016-12-28 08:21:48","deleteEnum":"DISABLE","enable":1,"updateTime":"2016-12-28 08:21:48","id":1,"name":"测试Fastjson","age":11&#125; 可以看到enable的值自动和枚举类型映射上. 使用FastJson做对象和JSON之间的转换对象转换成JSON123456789101112@Test public void fastJSONTest03() &#123; ModelTest modelTest = new ModelTest(); modelTest.setAge(11); modelTest.setCrateTime(new Date()); modelTest.setModifyTime(new Date()); modelTest.setId(1L); modelTest.setName("测试Fastjson"); modelTest.setRemark("备注"); modelTest.setEnable(1); System.out.println(JSON.toJSONString(modelTest)); &#125; 执行结果: 1&#123;"crateTime":"2016-12-28 08:30:42","deleteEnum":"DISABLE","enable":1,"updateTime":"2016-12-28 08:30:42","id":1,"name":"测试Fastjson","age":11&#125; JSON转换成对象123456789101112131415@Testpublic void fastJSONTest04() &#123; String json = "&#123;\n" + " \"id\": 1,\n" + " \"deleteEnum\": \"DISABLE\",\n" + " \"updateTime\": 1482884802549,\n" + " \"crateTime\": 1482884802549,\n" + " \"age\": 11,\n" + " \"name\": \"测试Fastjson\",\n" + " \"enable\": 1\n" + "&#125;"; ModelTest modelTest = JSONObject.parseObject(json, ModelTest.class); System.out.println(modelTest); System.out.println(modelTest.getName());&#125; 执行结果: 12ModelTest&#123;id=1, name='测试Fastjson', age=11, remark='null', crateTime=Wed Dec 28 08:26:42 CST 2016, modifyTime=Wed Dec 28 08:26:42 CST 2016, deleteEnum=DISABLE, enable=1&#125;测试Fastjson 示例中直接打印输出对象,是因为复写了toString()方法. SpringBean注解扫描组件Spring中bean注解扫描类ClassPathScanningCandidateComponentProvider, 该类构造参数如下: 123456789public ClassPathScanningCandidateComponentProvider(boolean useDefaultFilters) &#123; this(useDefaultFilters, new StandardEnvironment());&#125;public ClassPathScanningCandidateComponentProvider(boolean useDefaultFilters, Environment environment) &#123; if (useDefaultFilters) &#123; registerDefaultFilters(); &#125; this.environment = environment;&#125; 类实例化方式构造依赖, 参数意义:useDefaultFilters: 是否走默认的springBean扫描策略, spring启动时该值默认是为true, 扫描的组件是@Componentenvironment: 环境变量相关,基于spring.profiles相关配置 扩展自定义类自己扩展的注解需要被SpringBean注解扫描器扫到的话需要注解上增加@Component, 自定义注解示例自定义注解,绑定Spring注解@Component 12345678910111213141516@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Inherited@Documented@Componentpublic @interface VClass &#123; // 类名称 public String name(); // 表名称 public String tableName(); // 表注释 public String comment() default &quot;&quot;;&#125; 应用自定义注解在自定义实体类上增加自定义注解 1234567import com.wplus.plugin.htmlplus.anno.bean.VClass;@VClass(name = &quot;mock&quot;, tableName = &quot;tb_mock&quot;, comment = &quot;测试实体&quot;)public class Mock &#123; // this class used for test&#125; 使用Spring注解扫描器扫描自定义类123456789101112131415161718192021222324252627282930313233343536/*** 根据路径扫描带有VClass注解的类* @param scanPath 要扫描的包路径* @return*/public List&lt;String&gt; scanAnnoBeanName(String scanPath)&#123; if(StringUtils.isBlank(scanPath))&#123; scanPath = DEFAULT_BEAN_PATH; &#125; return scanAnnoBeanName(scanPath, VClass.class);&#125;/*** 根据路径扫描要用来映射的类* @param scanPath 要扫描的包* @return*/public List&lt;String&gt; scanAnnoBeanName(String scanPath, Class&lt;? extends Annotation&gt; annotationType)&#123; ClassPathScanningCandidateComponentProvider provider = new ClassPathScanningCandidateComponentProvider(false); provider.addIncludeFilter(new AnnotationTypeFilter(annotationType)); Set&lt;BeanDefinition&gt; sets = provider.findCandidateComponents(scanPath); if(null !=sets &amp;&amp; sets.size() &gt; 0)&#123; List&lt;String&gt; classNames = new ArrayList&lt;String&gt;(); for(BeanDefinition bean: sets)&#123; classNames.add(bean.getBeanClassName()); &#125; return classNames; &#125; return null;&#125;public static void main(String[] args) &#123; List&lt;String&gt; beans = new VanoBeanLoadApiImpl().scanAnnoBeanName(&quot;com.wplus.plugin&quot;); System.out.println(beans);&#125; 程序输出 1[com.wplus.plugin.htmlplus.demo.Mock] SpringResources资源扫描组件Spring中Resources扫描类GenericApplicationContext, 对应的资源扫描方法如下: 123public Resource getResource(String location) &#123; return this.resourceLoader != null?this.resourceLoader.getResource(location):super.getResource(location);&#125; Resources默认资源加载路径String CLASSPATH_URL_PREFIX = &quot;classpath:&quot;; SpringResources资源组件扩展定义资源加载方法,增加资源后缀匹配过滤 12345678910111213141516171819202122public List&lt;Resource&gt; scanResources(String locationPattern, final List&lt;String&gt; accepts) &#123; if(StringUtils.isBlank(locationPattern))&#123; locationPattern = DEFAULT_TEMPLATE_PATH; &#125; try &#123; GenericApplicationContext context = new GenericApplicationContext(); Resource rs[] = context.getResources(locationPattern); List&lt;Resource&gt; list = (null != rs &amp;&amp; rs.length &gt; 0)? Arrays.asList(rs): new ArrayList&lt;Resource&gt;(); // do return resource list if accepts list is empty. if(CollectionUtils.isEmpty(accepts))&#123; return list; &#125; // filter resource which file extension in accepts List&lt;Resource&gt; result = list.stream() // convert list to stream .filter(line -&gt; accepts.contains(".".concat(FilenameUtils.getExtension(line.getFilename())))) .collect(Collectors.&lt;Resource&gt;toList()); return result; &#125;catch (Exception e)&#123; LOGGER.error(e.getMessage(), e); &#125; return new ArrayList&lt;Resource&gt;();&#125; 示例使用Spring资源加载组件,扫描指定路径下的资源模板, 第二个参数用来标识扫描资源结果匹配的过滤,后缀在接收列表中的资源,会加载到结果集中. 调用示例模拟1234public static void main(String[] args) &#123; List&lt;Resource&gt; list = new TemplateLoadApiImpl().scanResources("classpath:/HTemplate/**", Arrays.asList(".html")); list.forEach(li -&gt; System.out.println(li.toString()));&#125; 方法通过扫描当前工程下资源模块HTemplate中的内容,递归遍历,收集后缀为.html结尾的资源文件.如果要扫描jar包中的资源的话,classpath统配符应为&quot;classpath*:/HTemplate/**&quot; @Aspect说明Spring除了支持Schema方式配置AOP,还支持使用@Aspect注解方式切面声明。Spring默认注解支持关闭,开启配置: 1&lt;aop:aspectj-autoproxy/&gt; 这样Spring就能发现@AspectJ风格的切面并且将切面应用到目标对象。 @Aspect依赖包12345678910&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.8.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.8.9&lt;/version&gt;&lt;/dependency&gt; @Aspect示例定义切面,切入方法调用耗时123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import com.alibaba.fastjson.JSON;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Pointcut;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.core.annotation.Order;import org.springframework.stereotype.Component;import java.util.Arrays;@Component@Aspect@Order(3)public class TraceInvokeTimelineAroundAspect &#123; private static final Logger LOG = LoggerFactory.getLogger(TraceInvokeTimelineAroundAspect.class); public final static long MONITOR_MAX_TIME = 5000; @Around( "pointRestful() || pointService() || pointMapper()" ) public Object invoke(ProceedingJoinPoint point) throws Throwable &#123; long startTime = System.currentTimeMillis(); String classMethodName = point.getSignature().getDeclaringType().getSimpleName() + "." + point.getSignature().getName(); Object result = point.proceed(); long usedTime = System.currentTimeMillis() - startTime; StringBuffer buffer = new StringBuffer(600); buffer.append("耗时:").append(usedTime + "ms").append(" "); buffer.append("优化(&gt;" + MONITOR_MAX_TIME + "ms):").append(usedTime &gt;= MONITOR_MAX_TIME).append(" "); buffer.append("接口:").append(classMethodName).append(" "); buffer.append("入参:").append(Arrays.toString(point.getArgs())).append(" "); buffer.append("返回:").append(JSON.toJSONString(result)); LOG.info(buffer.toString()); return result; &#125; /** * 声明切入点 - 控制层切入 */ @Pointcut("execution(* com.tutorial.aspectj.web.controller..*.*(..))") public void pointRestful() &#123; &#125; /** * 声明切入点 - 业务层切入 */ @Pointcut("execution(* com.tutorial.aspectj.facade.service..*.*(..))") public void pointService()&#123; &#125; /** * 声明切入点 - 数据层切入 */ @Pointcut("execution(* com.tutorial.aspectj.facade.mapper..*.*(..))") public void pointMapper() &#123; &#125; 开启@Aspect支持示例中我们用注解@Component对自定的Aspect进行了实例化,需要保证注解所在包能被Spring实例注解扫描到。要使切面在两个容器(spring&amp;springmvc)中都生效,必须两个都必须配置&lt;aop:aspectj-autoproxy/&gt;开启注解支持. Spring容器 12345&lt;context:annotation-config/&gt;&lt;context:component-scan base-package="com.tutorial.aspectj..**" use-default-filters="true"&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt;&lt;/context:component-scan&gt;&lt;aop:aspectj-autoproxy/&gt; SpringMvc容器 123456&lt;context:annotation-config/&gt;&lt;context:component-scan base-package="com.tutorial.aspectj.web.controller..**" use-default-filters="true"&gt; &lt;context:include-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt; &lt;context:include-filter type="annotation" expression="org.springframework.web.bind.annotation.ControllerAdvice"/&gt;&lt;/context:component-scan&gt;&lt;aop:aspectj-autoproxy/&gt;]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>Spring Session</tag>
        <tag>Aspect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MockMvc做接口测试]]></title>
    <url>%2F2016%2F07%2F15%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2FMockMvc%20%E6%8E%A5%E5%8F%A3%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[MockMvc实现了对Http请求的模拟，能够直接使用网络的形式，转换到Controller的调用，这样可以使得测试速度快、不依赖网络环境，而且提供了一套验证的工具，这样可以使得请求的验证统一而且很方便, 最方便的是对于客户测试,不需要启动服务器即可测试我们的Rest Api。 SpringMVC编写Rest示例Controller示例 123456789101112131415161718192021@Controller@RequestMapping(value = "/test")public class TestWebController &#123; @ResponseBody @RequestMapping(value = "/list", method = &#123;RequestMethod.GET&#125;) public List getMock(@RequestParam(name = "id", required = false, defaultValue = "l0") String id) &#123; return Arrays.asList("l1", "l2", "l3", id); &#125; @ResponseBody @RequestMapping(value = "/pust", method = &#123;RequestMethod.POST&#125;) public Object postMock(@RequestBody Object data) &#123; return ImmutableMap.&lt;String, String&gt;builder().put("data", JSON.toJSONString(data)).build(); &#125; @RequestMapping(value = "/view", method = &#123;RequestMethod.GET&#125;) public ModelAndView viewMock()&#123; return new ModelAndView("index"); &#125;&#125; MockMvc测试Rest示例 MockMvc基础交互示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import org.junit.Before;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.http.MediaType;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import org.springframework.test.context.web.WebAppConfiguration;import org.springframework.test.web.servlet.MockMvc;import org.springframework.test.web.servlet.MvcResult;import org.springframework.test.web.servlet.request.MockMvcRequestBuilders;import org.springframework.test.web.servlet.setup.MockMvcBuilders;import org.springframework.web.context.WebApplicationContext;import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.get;import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.post;import static org.springframework.test.web.servlet.result.MockMvcResultHandlers.print;import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.*;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(&#123;"classpath:context/spring-context.xml", "classpath:context/spring-mvc.xml"&#125;)@WebAppConfiguration(value = "src/main/webapp")public class TestController &#123; private MockMvc mockMvc; @Autowired private WebApplicationContext webApplicationContext; @Before public void setUp() throws Exception &#123; mockMvc = MockMvcBuilders.webAppContextSetup(webApplicationContext).build(); &#125; @Test public void test01() throws Exception &#123; MvcResult mvcResult = this.mockMvc.perform(MockMvcRequestBuilders.get("/test/list") .cookie(new Cookie("token", "F3AF5F1D14F534F677XF3A00E352C")) // 登录授权 .accept(MediaType.parseMediaType("application/json;charset=UTF-8"))) .andExpect(handler().handlerType(TestWebController.class)) // 验证执行的控制器类型 .andExpect(handler().methodName("getMock")) // 验证执行的方法名 .andExpect(status().isOk()) // 验证执行状态 .andDo(print()) // 打印交互信息 .andReturn(); System.out.println(mvcResult.getResponse().getContentAsString()); &#125;&#125;``` MockMvc可以对controller中的一次调用进行模拟;1、mockMvc.perform执行一个请求；2、MockMvcRequestBuilders.get(urlTemplate, urlMultiParams)构造一个请求3、ResultActions.andExpect添加执行完成后的断言4、ResultActions.andDo添加一个结果处理器，表示要对结果做点什么事情，比如此处使用MockMvcResultHandlers.print()输出整个响应结果信息。5、ResultActions.andReturn表示执行完成后返回相应的结果。执行结果:``` bash["l1","l2","l3","l0"] 交互信息: 12345678910111213141516171819202122232425262728293031323334MockHttpServletRequest: HTTP Method = GET Request URI = /test/list Parameters = &#123;&#125; Headers = &#123;Accept=[application/json;charset=UTF-8]&#125;Handler: Type = com.simple.example.web.TestWebController Method = public java.util.List com.simple.example.web.TestWebController.getMock(java.lang.String)Async: Async started = false Async result = nullResolved Exception: Type = nullModelAndView: View name = null View = null Model = nullFlashMap: Attributes = nullMockHttpServletResponse: Status = 200 Error message = null Headers = &#123;Content-Type=[application/json;charset=UTF-8]&#125; Content type = application/json;charset=UTF-8 Body = ["l1","l2","l3","l0"] Forwarded URL = null Redirected URL = null Cookies = [] MockMvc请求参数示例 1234567891011@Test public void test02() throws Exception &#123; MvcResult mvcResult = mockMvc.perform(get("/test/list?id=&#123;id&#125;", "模板参数") .accept(MediaType.parseMediaType("application/json;charset=UTF-8"))) .andExpect(handler().handlerType(TestWebController.class)) // 验证执行的控制器类型 .andExpect(handler().methodName("getMock")) // 验证执行的方法名 .andExpect(status().isOk()) // 验证执行状态 .andDo(print()) // 打印交互信息 .andReturn(); System.out.println(mvcResult.getResponse().getContentAsString()); &#125; 执行结果: 1["l1","l2","l3","模板参数"] MockMvc测试Post示例 12345678910111213@Testpublic void test() throws Exception &#123; MvcResult mvcResult = mockMvc.perform(post("/test/pust") .contentType(MediaType.APPLICATION_JSON) .content(JSON.toString(Arrays.asList("p1", "p2"))) .accept(MediaType.parseMediaType("application/json;charset=UTF-8"))) .andExpect(handler().handlerType(TestWebController.class)) // 验证执行的控制器类型 .andExpect(handler().methodName("getMock")) // 验证执行的方法名 .andExpect(status().isOk()) // 验证执行状态 .andDo(print()) // 打印交互信息 .andReturn(); System.out.println(mvcResult.getResponse().getContentAsString());&#125; 交互信息 12345678910111213141516171819202122232425262728293031323334MockHttpServletRequest: HTTP Method = POST Request URI = /test/pust Parameters = &#123;&#125; Headers = &#123;Content-Type=[application/json], Accept=[application/json;charset=UTF-8]&#125;Handler: Type = com.simple.example.web.TestWebController Method = public java.lang.Object com.simple.example.web.TestWebController.postMock(java.lang.Object)Async: Async started = false Async result = nullResolved Exception: Type = nullModelAndView: View name = null View = null Model = nullFlashMap: Attributes = nullMockHttpServletResponse: Status = 200 Error message = null Headers = &#123;Content-Type=[application/json;charset=UTF-8]&#125; Content type = application/json;charset=UTF-8 Body = &#123;"data":"[\"p1\",\"p2\"]"&#125; Forwarded URL = null Redirected URL = null Cookies = [] 执行结果: 1&#123;"data":"[\"p1\",\"p2\"]"&#125; 测试视图页 12345678910@Testpublic void test05() throws Exception &#123; MvcResult mvcResult = mockMvc.perform(get("/test/view")) .andExpect(view().name("index")) //验证视图页面 .andExpect(model().hasNoErrors()) //验证页面没有错误 .andExpect(status().isOk()) //验证状态码 .andDo(print()) .andReturn(); System.out.println(mvcResult.getModelAndView().getViewName());&#125; 执行结果: 1index MockMvc深入学习 http://docs.spring.io/spring-security/site/docs/current/reference/html/test-mockmvc.html http://jinnianshilongnian.iteye.com/blog/2004660]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>MockMvc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringSession介绍]]></title>
    <url>%2F2016%2F07%2F15%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2FSpringSession%E5%88%86%E5%B8%83%E5%BC%8F%E4%BC%9A%E8%AF%9D%2F</url>
    <content type="text"><![CDATA[生产环境我们的应用示例不可能是单节点部署, 通常都是多结点部署, 结点上层会进行域映射, 实例之间负载响应请求. 比如常见的Nginx + Tomcat负载均衡场景中。常用的均衡算法有IP_Hash、轮训、根据权重、随机等。不管对于哪一种负载均衡算法，由于Nginx对不同的请求分发到某一个Tomcat，Tomcat在运行的时候分别是不同的容器里，因此会出现session不同步或者丢失的问题。 解决方案IP_HASHnginx可以根据客户端IP进行负载均衡，在upstream里设置ip_hash，就可以针对同一个C类地址段中的客户端选择同一个后端服务器，除非那个后端服务器宕了才会换一个. 这样如果该类QPS高会导致该台服务器的负载升高,负载不均. nginx基于ip_hash的session管理方案 通过容器插件在容器层面扩展可共享存储的插件; 比如基于Tomcat的tomcat-redis-session-manager，基于Jetty的jetty-session-redis等等。好处是对项目来说是透明的，无需改动代码。该方案由于过于依赖容器，一旦容器升级或者更换意味着又得从新来过。并且代码不在项目中，对开发者来说维护也是个问题。 会话管理工具自己写一套会话管理的工具类，包括Session管理和Cookie管理，在需要使用会话的时候都从自己的工具类中获取，而工具类后端存储可以放到Redis中。很显然这个方案灵活性最大，但开发需要一些额外的时间。并且系统中存在两套Session方案，很容易弄错而导致取不到数据。 开源解决方案这里以开源框架Spring-Session为例，Spring-Session扩展了Servlet的会话管理(所有的request都会经过SessionRepositoryFilter，而 SessionRepositoryFilter是一个优先级最高的javax.servlet.Filter，它使用了一个SessionRepositoryRequestWrapper类接管了Http Session的创建和管理工作)，既不依赖容器，又不需要改动代码. 可插拔, 轻量级. 支持多维度存储;诸如 Redis 、Pivotal GemFire、Jdbc、Mongo 、Hazelcast等 SpringSession应用SpringMvc项目使用SpringSession maven依赖12345678910&lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session&lt;/artifactId&gt; &lt;version&gt;1.3.1.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt; &lt;version&gt;1.3.1.RELEASE&lt;/version&gt;&lt;/dependency&gt; 配置web.xml12345678&lt;filter&gt; &lt;filter-name&gt;springSessionRepositoryFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;springSessionRepositoryFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 配置redis、以及redisHttpSession存储12345678910111213141516171819202122232425262728293031323334&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd"&gt; &lt;context:annotation-config/&gt; &lt;!-- 将session放入redis --&gt; &lt;bean id="redisHttpSessionConfiguration" class="org.springframework.session.data.redis.config.annotation.web.http.RedisHttpSessionConfiguration"&gt; &lt;property name="maxInactiveIntervalInSeconds" value="1800" /&gt; &lt;/bean&gt; &lt;bean id="jedisPoolConfig" class="redis.clients.jedis.JedisPoolConfig"&gt; &lt;property name="maxTotal" value="100" /&gt; &lt;property name="maxIdle" value="10" /&gt; &lt;/bean&gt; &lt;bean id="jedisConnectionFactory" class="org.springframework.data.redis.connection.jedis.JedisConnectionFactory" destroy-method="destroy"&gt; &lt;property name="hostName" value="127.0.0.1"/&gt; &lt;property name="port" value="6379"/&gt; &lt;property name="password" value="" /&gt; &lt;property name="timeout" value="3000"/&gt; &lt;property name="usePool" value="true"/&gt; &lt;property name="poolConfig" ref="jedisPoolConfig"/&gt; &lt;/bean&gt; &lt;bean id="redisTemplate" class="org.springframework.data.redis.core.StringRedisTemplate"&gt; &lt;property name="connectionFactory" ref="jedisConnectionFactory" /&gt; &lt;/bean&gt;&lt;/beans&gt; 测试代码1234567891011121314151617181920212223242526272829303132333435@RestController@RequestMapping("/api/cloud")public class ApiSessionController extends BaseMultiController &#123; private static final Logger LOG = LoggerFactory.getLogger(ApiSessionController.class); @Autowired protected HttpSession httpSession; @GetMapping("/session/put") public APIResult sessionPut()&#123; httpSession.setAttribute("cloud", JSON.toJSONString(new User("Elonsu", "123456"))); String userString = (String)httpSession.getAttribute("cloud"); LOG.info("[session][set]:" + userString); return APIResult.success(true); &#125; @GetMapping("/session/get") public APIResult sessionGet()&#123; String userString = (String)httpSession.getAttribute("cloud"); LOG.info("[session][get]:" + userString); User user = JSONObject.parseObject(userString, User.class); return APIResult.success(user); &#125; @Data @Builder @NoArgsConstructor @AllArgsConstructor public static class User implements Serializable &#123; private String username; private String password; &#125;&#125; 测试输出启两个容器实例,端口分别使用8080和8081进行访问 实例1访问结果 实例2访问结果 查看redis中存储的session 应用SpringSessionmaven依赖1234567891011&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session&lt;/artifactId&gt; &lt;version&gt;1.3.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-redis&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 启动主类增加注解启动类上增加注解@EnableRedisHttpSession 12345678910111213@Configuration@SpringBootApplication@EnableAutoConfiguration@EnableRedisHttpSessionpublic class Application extends WebMvcStrap &#123; protected final static Logger LOG = LoggerFactory.getLogger(Application.class); public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 应用配置文件配置应用配置文件application.properties 增加如下配置 1234567# spring redisspring.redis.host=127.0.0.1spring.redis.port=6379spring.redis.password=# spring sessionspring.session.store-type=redisserver.session.timeout=5 官方文档 Spring官方https://docs.spring.io/spring-session/docs/current/reference/html5/#httpsession]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>Spring Session</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringAsync异步编程]]></title>
    <url>%2F2016%2F07%2F01%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2FSpringAsync%20%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Spring为任务调度与异步方法执行提供了注解支持。通过在方法上设置@Async注解，可使得方法被异步调用。也就是说调用者会在调用时立即返回，而被调用方法的实际执行是交给Spring的TaskExecutor来完成。 Async示例定义接口123456public interface IDemoService &#123; public String syncServiceInvoke() throws Exception; public String asynServiceInvoke() throws Exception;&#125; 接口实现123456789101112131415161718192021222324252627282930313233@Servicepublic class IDemoServiceImpl implements IDemoService&#123; // 异步操作必须和被调用的线程独立开 @Resource private AsynDemoServiceProxy asynDemoServiceProxy; public String syncServiceInvoke() throws Exception &#123; long start = System.currentTimeMillis(); String str1 = asynDemoServiceProxy.asynMethod().get(); String str = syncMethod(); String str2 = asynDemoServiceProxy.asynMethod().get(); return "同步调用: 方法1:" + str1 + "方法2:" + str + "方法3:" + str2 + ",总共耗时:" + (System.currentTimeMillis() - start) + "ms"; &#125; public String asynServiceInvoke() throws Exception &#123; long start = System.currentTimeMillis(); Future&lt;String&gt; str1 = asynDemoServiceProxy.asynMethod(); String str = syncMethod(); Future&lt;String&gt; str2 = asynDemoServiceProxy.asynMethod(); return "异步调用: 方法1:" + str1.get() + "方法2:" + str + "方法3:" + str2.get() + ",总共耗时:" + (System.currentTimeMillis() - start) + "ms"; &#125; public String syncMethod()&#123; try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return "耗时:2s"; &#125;&#125; 示例中分别实现了两个方法: 一个同步调用, 一个异步调用(AsynDemoServiceProxy类中定义了方法的异步实现,这里在并行结果等待场景下,异步逻辑不能和当前主线程处于同一线程中, 负责异步会失效), 代码如下 123456789101112131415@Component@EnableAsyncpublic class AsynDemoServiceProxy &#123; @Async public Future&lt;String&gt; asynMethod()&#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return new AsyncResult&lt;&gt;("耗时:3s"); &#125;&#125; RestAPI定义123456789101112131415161718@Controller@RequestMapping(value = "/api/demo")public class AsyncDemoController extends BaseMultiController &#123; private static final Logger LOG = LoggerFactory.getLogger(LeopardDemoController.class); @ResponseBody @RequestMapping(value = "/invoke/sync", method = RequestMethod.GET) public APIResult sync() throws Exception&#123; return new APIResult(ResultEnum.SUCCESS, demoService.syncServiceInvoke()); &#125; @ResponseBody @RequestMapping(value = "/invoke/asyn", method = RequestMethod.GET) public APIResult asyn() throws Exception&#123; return new APIResult(ResultEnum.SUCCESS, demoService.asynServiceInvoke()); &#125;&#125; 开启@Async支持SpringMvc处理器xml中配置如下内容 123&lt;!-- 支持 @Async 注解 --&gt;&lt;task:annotation-driven executor="myExecutor"/&gt;&lt;task:executor id="myExecutor" pool-size="5-20" queue-capacity="100"/&gt; 示例测试#### 同步调用测试 12345678910111213@Testpublic void sync() throws Exception &#123; String uri = "/api/demo/invoke/sync"; MvcResult mvcResult = this.mockMvc.perform(MockMvcRequestBuilders.get(uri) .cookie(new Cookie("token", "61ADD4B8ED86C7E162")) .contentType(MediaType.APPLICATION_JSON_UTF8) .accept(MediaType.APPLICATION_JSON_UTF8)) .andExpect(handler().handlerType(AsyncDemoController.class)) .andExpect(status().isOk()) .andDo(print()) .andReturn(); println(mvcResult.getResponse().getContentAsString());&#125; 程序执行结果(约8ms): 1234567891011121314151617181920212223242526272829303132333435MockHttpServletRequest: HTTP Method = GET Request URI = /api/demo/invoke/sync Parameters = &#123;&#125; Headers = &#123;Content-Type=[application/json;charset=UTF-8], Accept=[application/json;charset=UTF-8]&#125;Handler: Type = com.tutorial.spring.async.api.web.controller.AsyncDemoController Method = public com.tutorial.spring.async.api.result.APIResult com.tutorial.spring.async.api.web.controller.AsyncDemoController.sync() throws java.lang.ExceptionAsync: Async started = false Async result = nullResolved Exception: Type = nullModelAndView: View name = null View = null Model = nullFlashMap: Attributes = nullMockHttpServletResponse: Status = 200 Error message = null Headers = &#123;M-Appkey=[com.tutorial.spring.async.api], M-SpanName=[AsyncDemoController.sync], M-Host=[172.30.12.197], Content-Type=[application/json;charset=UTF-8], Content-Length=[126]&#125; Content type = application/json;charset=UTF-8 Body = &#123;"data":"同步调用: 方法1:耗时:3s方法2:耗时:2s方法3:耗时:3s,总共耗时:8019ms","message":"成功","status":0&#125; Forwarded URL = null Redirected URL = null Cookies = []&#123;"data":"同步调用: 方法1:耗时:3s方法2:耗时:2s方法3:耗时:3s,总共耗时:8019ms","message":"成功","status":0&#125; #### 异步调用测试 12345678910111213@Testpublic void async() throws Exception &#123; String uri = "/api/demo/invoke/asyn"; MvcResult mvcResult = this.mockMvc.perform(MockMvcRequestBuilders.get(uri, 1, 1, 15) .cookie(new Cookie("token", "61ADD4B8ED86C7E162")) .contentType(MediaType.APPLICATION_JSON_UTF8) .accept(MediaType.APPLICATION_JSON_UTF8)) .andExpect(handler().handlerType(AsyncDemoController.class)) .andExpect(status().isOk()) .andDo(print()) .andReturn(); println(mvcResult.getResponse().getContentAsString());&#125; 程序执行结果(约5ms):1234567891011121314151617181920212223242526272829303132333435MockHttpServletRequest: HTTP Method = GET Request URI = /api/demo/invoke/asyn Parameters = &#123;&#125; Headers = &#123;Content-Type=[application/json;charset=UTF-8], Accept=[application/json;charset=UTF-8]&#125;Handler: Type = com.tutorial.spring.async.api.web.controller.AsyncDemoController Method = public com.tutorial.spring.async.api.result.APIResult com.tutorial.spring.async.api.web.controller.AsyncDemoController.asyn() throws java.lang.ExceptionAsync: Async started = false Async result = nullResolved Exception: Type = nullModelAndView: View name = null View = null Model = nullFlashMap: Attributes = nullMockHttpServletResponse: Status = 200 Error message = null Headers = &#123;M-Appkey=[com.tutorial.spring.async.api], M-SpanName=[AsyncDemoController.asyn], M-Host=[172.30.12.197], Content-Type=[application/json;charset=UTF-8], Content-Length=[126]&#125; Content type = application/json;charset=UTF-8 Body = &#123;"data":"异步调用: 方法1:耗时:3s方法2:耗时:2s方法3:耗时:3s,总共耗时:5011ms","message":"成功","status":0&#125; Forwarded URL = null Redirected URL = null Cookies = []&#123;"data":"异步调用: 方法1:耗时:3s方法2:耗时:2s方法3:耗时:3s,总共耗时:5011ms","message":"成功","status":0&#125; 可以明显的看到, 其中串行的一个3ms的耗时被优化.相比于我们自己去定义一个线程体, 然后调用,这样的方式更优雅简单,也利于维护和调整. 问题解决同一Bean中,异步等待场景,调用失效]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>Async</tag>
        <tag>异步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lombok簡化模板代碼]]></title>
    <url>%2F2016%2F06%2F19%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2FLombok%20%E7%AE%80%E5%8C%96%E6%A8%A1%E6%9D%BF%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[Lombok可以做到对已有Java代码在字节码层级的改变，它的目的在于让程序员少写一些“样板代码”;所谓样板代码是那些没有营养,却又不得不写的代码;像实体中定义的getter、setter、equals、hashcode、toString等方法). 官方地址：https://projectlombok.org/ 工程依赖：https://projectlombok.org/mavenrepo/index.html Intelli IDEA安装Lombok插件1.【Intelli IDEA】- 【Preferences】-【plugins】 搜索lombok，点击install. Lombok常用注解 @Getter/ @Setter:注解在属性上；为属性提供 getting 和 setting 方法 @ToString：实现toString()方法 @EqualsAndHashCode：实现equals()方法和hashCode()方法 @Data ：注解在类上；提供类所有属性的 getting 和 setting 方法，此外还提供了equals、canEqual、hashCode、toString 方法 @Log4j ：注解在类上；为类提供一个 属性名为log 的 log4j 日志对象 @NoArgsConstructor：注解在类上；为类提供一个无参的构造方法 @AllArgsConstructor：注解在类上；为类提供一个全参的构造方法 @Builder:注解在类上：实现一种Builder Patterns（生成器模式）的功能 @SneakyThrows:注解到方法上：可以实现在try…catch到异常后进行的throw操作 @Synchronized:注解到方法上：实现同步代码片段的功能 更多注解说明: 官方文档 Lombok使用常用的模板代码注解化示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Setter@Getter@ToString@EqualsAndHashCode@Builder@NoArgsConstructor@AllArgsConstructorclass BasicModel implements Serializable&#123; private Long id; private String title; private Double score; private Date createTime; public static void main(String[] args) &#123; Date now = new Date(); // 通用模式构建对象 BasicModel bm1 = new BasicModel(); bm1.setCreateTime(now); bm1.setId(1L); bm1.setScore(11D); bm1.setTitle("Lombok示例1"); // Builder模式构建对象 BasicModel bm2 = BasicModel.builder() .id(1L).score(11D).title("Lombok示例1").createTime(now) .build(); // 全参构造 // 需要注意的是Lombok提供的全参构造是依赖属性在对象中定义顺序的, BasicModel bm3 = new BasicModel(2L, "Lombok示例1", 11D, now); // toString使用 System.out.println("bm1 = " + bm1); System.out.println("bm2 = " + bm2); System.out.println("bm3 = " + bm3); // equals()使用 System.out.println("bm1.equals(bm2) = " + bm1.equals(bm2)); System.out.println("bm1.equals(bm3) = " + bm1.equals(bm3)); &#125;&#125; 示例执行结果 12345bm1 = BasicModel(id=1, title=Lombok示例1, score=11.0, createTime=Sun Nov 27 17:14:17 CST 2016)bm2 = BasicModel(id=1, title=Lombok示例1, score=11.0, createTime=Sun Nov 27 17:14:17 CST 2016)bm3 = BasicModel(id=2, title=Lombok示例1, score=11.0, createTime=Sun Nov 27 17:14:17 CST 2016)bm1.equals(bm2) = truebm1.equals(bm3) = false 上面的注解可以简化为如下 1234567891011@Data@Builder@NoArgsConstructor@AllArgsConstructorclass BasicModel implements Serializable&#123; private Long id; private String title; private Double score; private Date createTime;&#125; Lombok问题方案问题: 父子类集成关系中中全参构造, 以及Builder模式构建对象时,父类属性不可见。方案: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354@Data@NoArgsConstructor@AllArgsConstructorclass BasicModel implements Serializable&#123; private Long id; private String title; private Double score; private Date createTime;&#125;@Data@NoArgsConstructor@AllArgsConstructor@ToString(callSuper = true)class ArticleModel extends BasicModel&#123; private String author; private String images; // 增减该构造方案, 此时子类中有三个构造,@NoArgsConstructor、@AllArgsConstructor、当前构造 @Builder public ArticleModel(Long id, String title, Double score, Date createTime, String author, String images) &#123; super(id, title, score, createTime); this.author = author; this.images = images; &#125; public static void main(String[] args) &#123; Date now = new Date(); // Builder模式构建对象 ArticleModel am1 = ArticleModel.builder() .id(1L).score(11D).title("Lombok示例1").createTime(now) .author("Elon.su").images("图片地址") .build(); // 常见的模板对象创建 ArticleModel am2 = new ArticleModel("Elon.su", "图片地址"); am2.setId(1L); am2.setTitle("Lombok示例1"); am2.setCreateTime(now); // toString使用 System.out.println("am1 = " + am1); System.out.println("am2 = " + am2); // equals()使用 System.out.println("am1.equals(am2) = " + am1.equals(am2)); &#125;&#125; 示例执行结果 123am1 = ArticleModel(super=BasicModel(id=1, title=Lombok示例1, score=11.0, createTime=Sun Nov 27 17:41:53 CST 2016), author=Elon.su, images=图片地址)am2 = ArticleModel(super=BasicModel(id=1, title=Lombok示例1, score=null, createTime=Sun Nov 27 17:41:53 CST 2016), author=Elon.su, images=图片地址)am1.equals(am2) = true]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Lombok</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式文件系统FastDFS]]></title>
    <url>%2F2016%2F06%2F01%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[分布式文件系统：Distributed file system, DFS，又叫做网络文件系统：Network File System。一种允许文件通过网络在多台主机上分享的文件系统，可让多机器上的多用户分享文件和存储空间。 FastDFS是用c语言编写的一款开源的分布式文件系统，充分考虑了冗余备份、负载均衡、线性扩容等机制，并注重高可用、高性能等指标，功能包括：文件存储、文件同步、文件访问（文件上传、文件下载）等，解决了大容量存储和负载均衡的问题。特别适合中小文件（建议范围：4KB &lt; file_size &lt;500MB），对以文件为载体的在线服务，如相册网站、视频网站等。 FastDFS 架构FastDFS架构包括Tracker server和Storage server。客户端请求Tracker server进行文件上传、下载，通过Tracker server调度最终由Storage server完成文件上传和下载。 跟踪服务器Tracker Server主要做调度工作，起到均衡的作用；负责管理所有的 storage server和 group，每个 storage 在启动后会连接 Tracker，告知自己所属 group 等信息，并保持周期性心跳。tracker根据storage的心跳信息，建立group==&gt;[storage serverlist]的映射表。 Tracker需要管理的元信息很少，会全部存储在内存中；另外tracker上的元信息都是由storage汇报的信息生成的，本身不需要持久化任何数据，这样使得tracker非常容易扩展，直接增加tracker机器即可扩展为tracker cluster来服务，cluster里每个tracker之间是完全对等的，所有的tracker都接受stroage的心跳信息，生成元数据信息来提供读写服务。 存储服务器Storage Server主要提供容量和备份服务；以 group 为单位，每个 group 内可以有多台 storage server，数据互为备份。以group为单位组织存储能方便的进行应用隔离、负载均衡、副本数定制（group内storage server数量即为该group的副本数），比如将不同应用数据存到不同的group就能隔离应用数据，同时还可根据应用的访问特性来将应用分配到不同的group来做负载均衡；缺点是group的容量受单机存储容量的限制，同时当group内有机器坏掉时，数据恢复只能依赖group内地其他机器，使得恢复时间会很长。 group内每个storage的存储依赖于本地文件系统，storage可配置多个数据存储目录，比如有10块磁盘，分别挂载在/data/disk1-/data/disk10，则可将这10个目录都配置为storage的数据存储目录。storage接受到写文件请求时，会根据配置好的规则选择其中一个存储目录来存储文件。为了避免单个目录下的文件数太多，在storage第一次启动时，会在每个数据存储目录里创建2级子目录，每级256个，总共65536个文件，新写的文件会以hash的方式被路由到其中某个子目录下，然后将文件数据作为本地文件存储到该目录中。 FastDFS的存储策略为了支持大容量，存储节点（服务器）采用了分卷（或分组）的组织方式。存储系统由一个或多个卷组成，卷与卷之间的文件是相互独立的，所有卷的文件容量累加就是整个存储系统中的文件容量。一个卷可以由一台或多台存储服务器组成，一个卷下的存储服务器中的文件都是相同的，卷中的多台存储服务器起到了冗余备份和负载均衡的作用。 在卷中增加服务器时，同步已有的文件由系统自动完成，同步完成后，系统自动将新增服务器切换到线上提供服务。当存储空间不足或即将耗尽时，可以动态添加卷。只需要增加一台或多台服务器，并将它们配置为一个新的卷，这样就扩大了存储系统的容量。 FastDFS的上传过程FastDFS向使用者提供基本文件访问接口，比如upload、download、append、delete等，以客户端库的方式提供给用户使用。 Storage Server会定期的向Tracker Server发送自己的存储信息。当Tracker Server Cluster中的Tracker Server不止一个时，各个Tracker之间的关系是对等的，所以客户端上传时可以选择任意一个Tracker。 当Tracker收到客户端上传文件的请求时，会为该文件分配一个可以存储文件的group，当选定了group后就要决定给客户端分配group中的哪一个storage server。当分配好storage server后，客户端向storage发送写文件请求，storage将会为文件分配一个数据存储目录。然后为文件分配一个fileid，最后根据以上的信息生成文件名存储文件。 选择tracker server当集群中不止一个tracker server时，由于tracker之间是完全对等的关系，客户端在upload文件时可以任意选择一个trakcer。 选择存储的group当tracker接收到upload file的请求时，会为该文件分配一个可以存储该文件的group，支持如下选择group的规则： 1. Round robin，所有的group间轮询 2. Specified group，指定某一个确定的group 3. Load balance，剩余存储空间多多group优先 选择storage server当选定group后，tracker会在group内选择一个storage server给客户端，支持如下选择storage的规则： 1. Round robin，在group内的所有storage间轮询 2. First server ordered by ip，按ip排序 3. First server ordered by priority，按优先级排序（优先级在storage上配置） 选择storage path当分配好storage server后，客户端将向storage发送写文件请求，storage将会为文件分配一个数据存储目录，支持如下规则： 1. Round robin，多个存储目录间轮询 2. 剩余存储空间最多的优先 生成Fileid选定存储目录之后，storage会为文件生一个Fileid，由storage server ip、文件创建时间、文件大小、文件crc32和一个随机数拼接而成，然后将这个二进制串进行base64编码，转换为可打印的字符串。 选择两级目录当选定存储目录之后，storage会为文件分配一个fileid，每个存储目录下有两级256*256的子目录，storage会按文件fileid进行两次hash（猜测），路由到其中一个子目录，然后将文件以fileid为文件名存储到该子目录下。 生成文件名当文件存储到某个子目录后，即认为该文件存储成功，接下来会为该文件生成一个文件名，文件名由group、存储目录、两级子目录、fileid、文件后缀名（由客户端指定，主要用于区分文件类型）拼接而成。 FastDFS的文件同步写文件时，客户端将文件写至group内一个storage server即认为写文件成功，storage server写完文件后，会由后台线程将文件同步至同group内其他的storage server。 每个storage写文件后，同时会写一份binlog，binlog里不包含文件数据，只包含文件名等元信息，这份binlog用于后台同步，storage会记录向group内其他storage同步的进度，以便重启后能接上次的进度继续同步；进度以时间戳的方式进行记录，所以最好能保证集群内所有server的时钟保持同步。 storage的同步进度会作为元数据的一部分汇报到tracker上，tracke在选择读storage的时候会以同步进度作为参考。 比如一个group内有A、B、C三个storage server，A向C同步到进度为T1 (T1以前写的文件都已经同步到B上了），B向C同步到时间戳为T2（T2 &gt; T1)，tracker接收到这些同步进度信息时，就会进行整理，将最小的那个做为C的同步时间戳，本例中T1即为C的同步时间戳为T1（即所有T1以前写的数据都已经同步到C上了）；同理，根据上述规则，tracker会为A、B生成一个同步时间戳。 FastDFS的文件下载客户端uploadfile成功后，会拿到一个storage生成的文件名，接下来客户端根据这个文件名即可访问到该文件。 跟upload file一样，在downloadfile时客户端可以选择任意tracker server。tracker发送download请求给某个tracker，必须带上文件名信息，tracke从文件名中解析出文件的group、大小、创建时间等信息，然后为该请求选择一个storage用来服务读请求。 FastDFS性能方案 FastDFS 安装 软件包 版本 FastDFS v5.05 libfastcommon v1.0.7 下载安装libfastcommon 下载 1wget https://github.com/happyfish100/libfastcommon/archive/V1.0.7.tar.gz 解压 12tar -xvf V1.0.7.tar.gzcd libfastcommon-1.0.7 编译、安装 12./make.sh./make.sh install 创建软链接 1234ln -s /usr/lib64/libfastcommon.so /usr/local/lib/libfastcommon.soln -s /usr/lib64/libfastcommon.so /usr/lib/libfastcommon.soln -s /usr/lib64/libfdfsclient.so /usr/local/lib/libfdfsclient.soln -s /usr/lib64/libfdfsclient.so /usr/lib/libfdfsclient.so 下载安装FastDFS 下载FastDFS 1wget https://github.com/happyfish100/fastdfs/archive/V5.05.tar.gz 解压 12tar -xvf V5.05.tar.gzcd fastdfs-5.05 编译、安装 12./make.sh./make.sh install 配置 Tracker 服务上述安装成功后，在/etc/目录下会有一个fdfs的目录，进入它。会看到三个.sample后缀的文件，这是作者给我们的示例文件，我们需要把其中的tracker.conf.sample文件改为tracker.conf配置文件并修改它： 12cp tracker.conf.sample tracker.confvi tracker.conf 编辑tracker.conf 1234567891011# 配置文件是否不生效，false 为生效disabled=false# 提供服务的端口port=22122# Tracker 数据和日志目录地址base_path=//home/data/fastdfs# HTTP 服务端口http.server_port=80 创建tracker基础数据目录，即base_path对应的目录 1mkdir -p /home/data/fastdfs 使用ln -s 建立软链接 123ln -s /usr/bin/fdfs_trackerd /usr/local/binln -s /usr/bin/stop.sh /usr/local/binln -s /usr/bin/restart.sh /usr/local/bin 启动服务 1service fdfs_trackerd start 查看监听 1netstat -unltp|grep fdfs 如果看到22122端口正常被监听后，这时候说明Tracker服务启动成功啦！ tracker server 目录及文件结构Tracker服务启动成功后，会在base_path下创建data、logs两个目录。目录结构如下： 123456$&#123;base_path&#125; |__data | |__storage_groups.dat：存储分组信息 | |__storage_servers.dat：存储服务器列表 |__logs | |__trackerd.log： tracker server 日志文件 配置 Storage 服务进入 /etc/fdfs 目录，复制 FastDFS 存储器样例配置文件 storage.conf.sample，并重命名为 storage.conf 123# cd /etc/fdfs# cp storage.conf.sample storage.conf# vi storage.conf 编辑storage.conf 123456789101112131415161718192021222324252627282930313233# 配置文件是否不生效，false 为生效disabled=false# 指定此 storage server 所在 组(卷)group_name=group1# storage server 服务端口port=23000# 心跳间隔时间，单位为秒 (这里是指主动向 tracker server 发送心跳)heart_beat_interval=30# Storage 数据和日志目录地址(根目录必须存在，子目录会自动生成)base_path=/home/data/fastdfs/storage# 存放文件时 storage server 支持多个路径。这里配置存放文件的基路径数目，通常只配一个目录。store_path_count=1# 逐一配置 store_path_count 个路径，索引号基于 0。# 如果不配置 store_path0，那它就和 base_path 对应的路径一样。store_path0=/home/data/fastdfs/storage# FastDFS 存储文件时，采用了两级目录。这里配置存放文件的目录个数。 # 如果本参数只为 N（如： 256），那么 storage server 在初次运行时，会在 store_path 下自动创建 N * N 个存放文件的子目录。subdir_count_per_path=256# tracker_server 的列表 ，会主动连接 tracker_server# 有多个 tracker server 时，每个 tracker server 写一行tracker_server=192.168.1.190:22122# 允许系统同步的时间段 (默认是全天) 。一般用于避免高峰同步产生一些问题而设定。sync_start_time=00:00sync_end_time=23:59 使用ln -s 建立软链接 1ln -s /usr/bin/fdfs_storaged /usr/local/bin 启动服务 1service fdfs_storaged start 查看监听 1netstat -unltp|grep fdfs 启动Storage前确保Tracker是启动的。初次启动成功，会在 /home/data/fastdfs/storage 目录下创建 data、 logs 两个目录。如果看到23000端口正常被监听后，这时候说明Storage服务启动成功啦！ 查看Storage和Tracker是否在通信 1/usr/bin/fdfs_monitor /etc/fdfs/storage.conf FastDFS 配置 Nginx 模块 软件包 版本 openresty v1.13.6.1 fastdfs-nginx-module v1.1.6 FastDFS 通过 Tracker 服务器，将文件放在 Storage 服务器存储， 但是同组存储服务器之间需要进行文件复制，有同步延迟的问题。 假设 Tracker 服务器将文件上传到了 192.168.1.190，上传成功后文件 ID已经返回给客户端。此时 FastDFS 存储集群机制会将这个文件同步到同组存192.168.1.190，在文件还没有复制完成的情况下，客户端如果用这个文件 ID 在 192.168.1.190 上取文件,就会出现文件无法访问的错误。而 fastdfs-nginx-module 可以重定向文件链接到源服务器取文件，避免客户端由于复制延迟导致的文件无法访问错误。 下载 安装 Nginx 和 fastdfs-nginx-module： 推荐您使用yum安装以下的开发库: 1yum install readline-devel pcre-devel openssl-devel -y 下载最新版本并解压： 1234567wget https://openresty.org/download/openresty-1.13.6.1.tar.gztar -xvf openresty-1.13.6.1.tar.gzwget https://github.com/happyfish100/fastdfs-nginx-module/archive/master.zipunzip master.zip 配置 nginx 安装，加入fastdfs-nginx-module模块： 1./configure --add-module=../fastdfs-nginx-module-master/src/ 编译、安装： 1make &amp;&amp; make install 查看Nginx的模块： 1/usr/local/openresty/nginx/sbin/nginx -v 有下面这个就说明添加模块成功 复制 fastdfs-nginx-module 源码中的配置文件到/etc/fdfs 目录， 并修改： 1cp /fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs/ 1234567891011121314# 连接超时时间connect_timeout=10# Tracker Servertracker_server=192.168.1.190:22122# StorageServer 默认端口storage_server_port=23000# 如果文件ID的uri中包含/group**，则要设置为trueurl_have_group_name = true# Storage 配置的store_path0路径，必须和storage.conf中的一致store_path0=/home/data/fastdfs/storage 复制 FastDFS 的部分配置文件到/etc/fdfs 目录： 12cp /fastdfs-nginx-module/src/http.conf /etc/fdfs/cp /fastdfs-nginx-module/src/mime.types /etc/fdfs/ 配置nginx，修改nginx.conf： 123location ~/group([0-9])/M00 &#123; ngx_fastdfs_module;&#125; 启动Nginx： 12[root@iz2ze7tgu9zb2gr6av1tysz sbin]# ./nginxngx_http_fastdfs_set pid=9236 测试上传： 12[root@iz2ze7tgu9zb2gr6av1tysz fdfs]# /usr/bin/fdfs_upload_file /etc/fdfs/client.conf /etc/fdfs/4.jpggroup1/M00/00/00/rBD8EFqVACuAI9mcAAC_ornlYSU088.jpg 部署结构图： JAVA 客户端集成pom.xml引入： 123456&lt;!-- fastdfs --&gt;&lt;dependency&gt; &lt;groupId&gt;org.csource&lt;/groupId&gt; &lt;artifactId&gt;fastdfs-client-java&lt;/artifactId&gt; &lt;version&gt;1.27&lt;/version&gt;&lt;/dependency&gt; fdfs_client.conf配置： 123456789101112#连接tracker服务器超时时长connect_timeout = 2 #socket连接超时时长network_timeout = 30#文件内容编码 charset = UTF-8 #tracker服务器端口http.tracker_http_port = 8080http.anti_steal_token = nohttp.secret_key = FastDFS1234567890#tracker服务器IP和端口（可以写多个）tracker_server = 192.168.1.190:22122 FastDFSClient上传类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133public class FastDFSClient&#123; private static final String CONFIG_FILENAME = "D:\\itstyle\\src\\main\\resources\\fdfs_client.conf"; private static final String GROUP_NAME = "market1"; private TrackerClient trackerClient = null; private TrackerServer trackerServer = null; private StorageServer storageServer = null; private StorageClient storageClient = null; static&#123; try &#123; ClientGlobal.init(CONFIG_FILENAME); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (MyException e) &#123; e.printStackTrace(); &#125; &#125; public FastDFSClient() throws Exception &#123; trackerClient = new TrackerClient(ClientGlobal.g_tracker_group); trackerServer = trackerClient.getConnection(); storageServer = trackerClient.getStoreStorage(trackerServer);; storageClient = new StorageClient(trackerServer, storageServer); &#125; /** * 上传文件 * @param file 文件对象 * @param fileName 文件名 * @return */ public String[] uploadFile(File file, String fileName) &#123; return uploadFile(file,fileName,null); &#125; /** * 上传文件 * @param file 文件对象 * @param fileName 文件名 * @param metaList 文件元数据 * @return */ public String[] uploadFile(File file, String fileName, Map&lt;String,String&gt; metaList) &#123; try &#123; byte[] buff = IOUtils.toByteArray(new FileInputStream(file)); NameValuePair[] nameValuePairs = null; if (metaList != null) &#123; nameValuePairs = new NameValuePair[metaList.size()]; int index = 0; for (Iterator&lt;Map.Entry&lt;String,String&gt;&gt; iterator = metaList.entrySet().iterator(); iterator.hasNext();) &#123; Map.Entry&lt;String,String&gt; entry = iterator.next(); String name = entry.getKey(); String value = entry.getValue(); nameValuePairs[index++] = new NameValuePair(name,value); &#125; &#125; return storageClient.upload_file(GROUP_NAME,buff,fileName,nameValuePairs); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 获取文件元数据 * @param fileId 文件ID * @return */ public Map&lt;String,String&gt; getFileMetadata(String groupname,String fileId) &#123; try &#123; NameValuePair[] metaList = storageClient.get_metadata(groupname,fileId); if (metaList != null) &#123; HashMap&lt;String,String&gt; map = new HashMap&lt;String, String&gt;(); for (NameValuePair metaItem : metaList) &#123; map.put(metaItem.getName(),metaItem.getValue()); &#125; return map; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 删除文件 * @param fileId 文件ID * @return 删除失败返回-1，否则返回0 */ public int deleteFile(String groupname,String fileId) &#123; try &#123; return storageClient.delete_file(groupname,fileId); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return -1; &#125; /** * 下载文件 * @param fileId 文件ID（上传文件成功后返回的ID） * @param outFile 文件下载保存位置 * @return */ public int downloadFile(String groupName,String fileId, File outFile) &#123; FileOutputStream fos = null; try &#123; byte[] content = storageClient.download_file(groupName,fileId); fos = new FileOutputStream(outFile); InputStream ips = new ByteArrayInputStream(content); IOUtils.copy(ips,fos); return 0; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if (fos != null) &#123; try &#123; fos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return -1; &#125; public static void main(String[] args) throws Exception &#123; FastDFSClient client = new FastDFSClient(); File file = new File("D:\\23456.png"); String[] result = client.uploadFile(file, "png"); System.out.println(result.length); System.out.println(result[0]); System.out.println(result[1]); &#125;&#125; 执行main方法测试返回： 1232group1M00/00/00/rBD8EFqTrNyAWyAkAAKCRJfpzAQ227.png 地址：https://blog.52itstyle.com/archives/2430/]]></content>
      <categories>
        <category>运维部署</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java8 常用工具示例]]></title>
    <url>%2F2016%2F05%2F27%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2FJava8%20%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[本篇采用示例的形式展示Java8的常见特性应用. 线程写法Java8之前 123456new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("自定义线程"); &#125;&#125;).start(); Java8写法 123new Thread(()-&gt; &#123; System.out.println("自定义线程");&#125;).start(); 比较器写法Java8之前 123456789List&lt;String&gt; list = Arrays.asList("bb", "a", "ccc");Collections.sort(list, new Comparator&lt;String&gt;() &#123; @Override public int compare(String o1, String o2) &#123; return o1.length() - o2.length(); &#125;&#125;);// [a, bb, ccc]System.out.println(list); Java8写法 123Collections.sort(list, (o1, o2) -&gt; o1.length() - o2.length());// [a, bb, ccc]System.out.println(list); 遍历写法Java8之前 1234List&lt;String&gt; list = Arrays.asList("bb", "a", "ccc");for (String li : list) &#123; System.out.println(li);&#125; Java8写法 123List&lt;String&gt; list = Arrays.asList("bb", "a", "ccc");list.forEach(li -&gt; System.out.println(li));list.forEach(System.out::println); 计算过滤12345List&lt;Integer&gt; numbers = Arrays.asList(20,22,1,2,1,3,3,2,4,8,16);// 过滤集合中的偶数,去重、排序List&lt;Integer&gt; filters = numbers.stream().filter(i -&gt; i % 2 == 0).distinct().sorted().collect(Collectors.toList());// 248162022filters.forEach(System.out::print); 对列表每个元素应用函数12345List&lt;String&gt; list = Arrays.asList("USA", "Japan", "France", "Germany", "Italy","Canada");// 将字符串换成大写并用点号链接起来String str = list.stream().map(x -&gt; x.toUpperCase()).collect(Collectors.joining("、"));// USA、JAPAN、FRANCE、GERMANY、ITALY、CANADASystem.out.println(str); lambda表达式中的Map Reduce1234List&lt;Integer&gt; costBeforeTax = Arrays.asList(100, 200, 300, 400);double bill = costBeforeTax.stream().map((cost) -&gt; cost + .10*cost).reduce((sum, cost) -&gt; sum + cost).get();// 1100.0System.out.println("集合中的元素每个增大10%后的结果总和: " + bill); 集合元素计算123456List&lt;Integer&gt; primes = Arrays.asList(2, 3, 5, 7, 11, 13, 17, 19, 23, 29);IntSummaryStatistics stats = primes.stream().mapToInt((x) -&gt; x).summaryStatistics();System.out.println("集合中最大值 : " + stats.getMax());System.out.println("集合中最小值 : " + stats.getMin());System.out.println("集合元素总数 : " + stats.getSum());System.out.println("集合元素均值 : " + stats.getAverage()); 集合元素分组123456789101112131415161718192021222324252627public static void main(String[] args) &#123; List&lt;Person&gt; list = Arrays.asList(new Person(10, "Elon"), new Person(12, "Dennisit"), new Person(10, "Alone")); // [&#123;"age":10,"name":"Elon"&#125;, &#123;"age":12,"name":"Dennisit"&#125;, &#123;"age":10,"name":"Alone"&#125;] System.out.println(list); Map&lt;Integer, List&lt;Person&gt;&gt; groupByAge = list.stream().collect(Collectors.groupingBy(Person::getAge, Collectors.toList())); // &#123;10=[&#123;"age":10,"name":"Elon"&#125;, &#123;"age":10,"name":"Alone"&#125;], 12=[&#123;"age":12,"name":"Dennisit"&#125;]&#125; System.out.println(groupByAge); List&lt;String&gt; names = list.stream().map(e-&gt;e.getName()).collect(Collectors.toList()); // [Elon, Dennisit, Alone] System.out.println(names); List&lt;Integer&gt; ages = list.stream().map(e-&gt;e.getAge()).distinct().collect(Collectors.toList()); // [10, 12] System.out.println(ages);&#125;@Data@Builder@NoArgsConstructor@AllArgsConstructorstatic class Person&#123; private int age; private String name; public String toString()&#123; return JSON.toJSONString(this); &#125;&#125; 集合对象排序123456789public static void main(String[] args) &#123; List&lt;Person&gt; list = Arrays.asList(new Person(10, "Elon"), new Person(12, "Dennisit"), new Person(10, "Alone")); // 按照年龄降序 List&lt;Person&gt; sort = list.stream() .sorted(Comparator.comparing(Person::getAge).reversed()) .collect(Collectors.toList()); // [&#123;"age":12,"name":"Dennisit"&#125;, &#123;"age":10,"name":"Elon"&#125;, &#123;"age":10,"name":"Alone"&#125;] System.out.println(sort);&#125; 函数式接口1234567// 函数接口定义Predicate&lt;String&gt; startWithJ = (n) -&gt; n.startsWith("J");Predicate&lt;String&gt; containedA = (n) -&gt; n.contains("a");List&lt;String&gt; languages = Arrays.asList("Java", "Scala", "C++", "Haskell", "Lisp");// 使用逻辑函数合并Predicate 执行结果: Java,Scala,Haskell,languages.stream().filter(startWithJ.or(containedA)).forEach(li -&gt; System.out.print(li + ",")); 函数接口应用 我们在用全文索引的时候需要将数据先存储到索引然后进行搜索,索引数据一般操作有全量索引、增量索引、实施索引, 索引创建方式可能有databus、基于消息机制、或者基于定时任务等等. 通常我们把DB数据转换为搜索引擎的索引的时候,分两步: 筛选出待索引的数据 将目标数据进行索引 针对这个行为我们来定义行为规约, 函数接口定义时使用@FunctionalInterface 筛选数据行为123456789101112131415/** * 筛选数据行为规约 */@FunctionalInterfacepublic interface DataCriteria&lt;T&gt; &#123; /** * 分页加载数据 * @param page 页码 * @param size 页量 * @return 加载的数据集合 */ public List&lt;T&gt; loader(int page, int size);&#125; 索引数据行为规约1234567891011121314/** * 索引数据行为规约 */@FunctionalInterfacepublic interface IndexCriteria &#123; /** * 数据索引行为 * @param list 待索引的数据 * @return */ public void index(List&lt;?&gt; list);&#125; 全量索引行为抽象123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * 每次批量处理的数据一次最大500个 */public static final Integer DEFAULT_BATCH_SIZE = 500;/** * 私有化构造 */private IndexAction()&#123;&#125;/** * 执行全量索引 * @param dataCriteria 数据加载行为 * @param indexCriteria 索引数据行为 */public static void fullyIndex(DataCriteria dataCriteria, IndexCriteria indexCriteria) &#123; fullyIndex(DEFAULT_BATCH_SIZE, dataCriteria, indexCriteria);&#125;/** * 执行全量索引 * @param size 每个批次索引的数据大小 * @param dataCriteria 数据加载行为 * @param indexCriteria 索引数据行为 */public static void fullyIndex(int size, DataCriteria dataCriteria, IndexCriteria indexCriteria) &#123; // 游标页从第一页开始 int current = 1; // 约束批量处理的大小 size = NumberUtils.restrainNum(size, 0, DEFAULT_BATCH_SIZE); while(true)&#123; int cursor = cursor(current, size, dataCriteria, indexCriteria); if (cursor == current) &#123; LOG.info("[创建索引] 创建完成, &#123;&#125;*&#123;&#125;数据", size, cursor); break; &#125; current = cursor; &#125;&#125;/** * 游标扫描处理 * @param page 游标页 * @param size 每个批次索引的数据大小 * @param dataCriteria 数据加载行为 * @param indexCriteria 索引数据行为 * @return */private static int cursor(int page, int size, DataCriteria dataCriteria, IndexCriteria indexCriteria)&#123; Assert.notNull(dataCriteria, "数据操作不能为空"); Assert.notNull(indexCriteria, "操作操作不能为空"); List list = dataCriteria.loader(page, size); if(CollectionUtils.isNotEmpty(list))&#123; indexCriteria.index(list); ++ page; &#125; return page;&#125; 上面我们做了简单的全量索引行为抽象, 从第一页开始分页加载数据进行索引,直到我们加载到的数据为空,本次索引行为结束 特定索引行为抽象123456789101112/** * 指定数据索引 * @param list 待索引数据 * @param indexCriteria 索引行为 */public static void pointIndex(List&lt;?&gt; list, IndexCriteria indexCriteria)&#123; if(CollectionUtils.isEmpty(list))&#123; return; &#125; Assert.notNull(indexCriteria, "操作操作不能为空"); indexCriteria.index(list);&#125; 该抽象比较简单,直接进行指定的数据索引即可. 规约使用上面我们定义了规约, 接下来直接展示如何优雅的基于规约装载数据 12345678910111213141516171819202122232425/** * 全量索引 */@Overridepublic void fullyIndex() &#123; IndexAction.fullyIndex( // 分页加载数据 (p, s) -&gt; &#123; return merchantService.selectList(p, s); &#125;, // 数据进行索引 (x)-&gt; merchantEsRepository.indexList((List&lt;MerchantIndex&gt;)x) );&#125;/** * 特定索引 * @param ids 主键编号集合 * @throws Exception */@Overridepublic void pointIndex(List&lt;Integer&gt; ids) throws Exception&#123; List&lt;MerchantIndex&gt; list = merchantService.selectList(ids); IndexAction.pointIndex(list, (x) -&gt; merchantEsRepository.indexList((List&lt;MerchantIndex&gt;) x));&#125; 示例中分别展示了全量索引和特定数据索引的规约应用,通过函数接口功能打包.我们把全量和增量的行为就隐藏在了规约行为中,使业务代码更简洁优雅. 接口方法1234567891011121314151617public static void main(String[] args) &#123; MathOperation addition = (int a, int b) -&gt; a + b; // 默认方法 addition.print(); // 9 System.out.println(addition.operation(5, 4));&#125;interface MathOperation &#123; // 接口方法 int operation(int a, int b); default void print()&#123; System.out.println("默认方法"); &#125;&#125; Java8日期操作12345678910111213141516171819202122232425262728/创建日期LocalDate date = LocalDate.of(2017,1,21); //2017-01-21int year = date.getYear() //2017Month month = date.getMonth(); //JANUARYint day = date.getDayOfMonth(); //21DayOfWeek dow = date.getDayOfWeek(); //SATURDAYint len = date.lengthOfMonth(); //31(days in January)boolean leap = date.isLeapYear(); //false(not a leap year)//时间的解析和格式化LocalDate date = LocalDate.parse(&quot;2017-01-21&quot;);LocalTime time = LocalTime.parse(&quot;13:45:20&quot;);LocalDateTime now = LocalDateTime.now();now.format(DateTimeFormatter.BASIC_ISO_DATE);//合并日期和时间LocalDateTime dt1 = LocalDateTime.of(2017, Month.JANUARY, 21, 18, 7);LocalDateTime dt2 = LocalDateTime.of(localDate, time);LocalDateTime dt3 = localDate.atTime(13,45,20);LocalDateTime dt4 = localDate.atTime(time);LocalDateTime dt5 = time.atDate(localDate);//操作日期LocalDate date1 = LocalDate.of(2014,3,18); //2014-3-18LocalDate date2 = date1.plusWeeks(1); //2014-3-25LocalDate date3 = date2.minusYears(3); //2011-3-25LocalDate date4 = date3.plus(6, ChronoUnit.MONTHS); //2011-09-25 日期格式化java.util.date和java.time.LocalDateTime格式化 应用示例1234567891011121314151617181920212223242526272829/*** 格式化日期* @param date 待格式化的日期* @param pattern 格式化正则* @return 格式化结果串*/public static String format(Date date, String pattern)&#123; return new SimpleDateFormat(pattern).format(date);&#125;/*** 格式化日期* @param localDateTime 待格式化的日期* @param pattern 格式化正式* @return 格式化结果串*/public static String format(LocalDateTime localDateTime, String pattern)&#123; return localDateTime.format(DateTimeFormatter.ofPattern(pattern));&#125;/*** 格式化日期* @param localDate 待格式化的日期* @param pattern 格式化正则, 这里使用的类型 &#123;@link LocalDate&#125;, 所以正则只能设定到天* @return 格式化结果串*/public static String format(LocalDate localDate, String pattern)&#123; return localDate.format(DateTimeFormatter.ofPattern(pattern));&#125; 示例测试123456// 2017-08-28 15:45:02System.out.println(format(new Date(), "yyyy-MM-dd HH:mm:ss"));// 2017-08-28 15:45:02System.out.println(format((LocalDateTime.now()), "yyyy-MM-dd HH:mm:ss"));// 2017-08-28System.out.println(format((LocalDateTime.now().toLocalDate()), "yyyy-MM-dd")); 日期转换java.util.date和java.time.LocalDateTime互相转换 应用示例123456789101112131415161718/*** 将 &#123;@link LocalDateTime&#125; 转换成 &#123;@link Date&#125;* @param localDateTime &#123;@link LocalDateTime&#125; 待转换的日期* @return 转换成Date结果*/public static Date from(LocalDateTime localDateTime)&#123; Instant instant = localDateTime.atZone(ZoneId.systemDefault()).toInstant(); return Date.from(instant);&#125;/*** 将 &#123;@link Date&#125; 转换成 &#123;@link LocalDateTime&#125;* @param date &#123;@link Date&#125; 待转换的日期* @return 转换成 &#123;@link LocalDateTime&#125; 结果*/public static LocalDateTime from(Date date)&#123; return LocalDateTime.ofInstant(date.toInstant(), ZoneId.systemDefault());&#125; 示例测试123456String patternTime = "yyyy-MM-dd HH:mm:ss";Date now = new Date();// 2017-08-28 14:47:10System.out.println(format(from(now), patternTime));// 2017-08-28 14:47:10System.out.println(format(from(LocalDateTime.now()), patternTime)); 日期区间集合计算两端日期之间内的日期天数集合 示例代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/*** 获取&#123;@link Date&#125;在开始时间和结束时间内的日期时间段&#123;@link Date&#125;集合* @param start 开始时间* @param end 结束时间* @return 时间天数集合*/public static List&lt;Date&gt; dateZones(Date start, Date end)&#123; return dateZones(from(start), from(end));&#125;/*** 获取 &#123;@link LocalDate&#125; 在开始时间和结束时间内的日期时间段 &#123;@link LocalDate&#125; 集合* @param start 开始时间* @param end 结束时间* @return 时间集合*/public static List&lt;Date&gt; dateZones(LocalDate start, LocalDate end)&#123; return Stream.iterate(start, x -&gt; x.plusDays(1)) .limit(ChronoUnit.DAYS.between(start, end) + 1) .map(e -&gt; Date.from(e.atStartOfDay().atZone(ZoneId.systemDefault()).toInstant())) .collect(Collectors.toList());&#125; /*** 获取&#123;@link LocalDateTime&#125; 在开始时间和结束时间内的日期时间段&#123;@link Date&#125;集合* @param start 开始时间* @param end 结束时间* @return 时间天数集合*/public static List&lt;Date&gt; dateZones(LocalDateTime start, LocalDateTime end)&#123; // 用起始时间作为流的源头，按照每次加一天的方式创建一个无限流 return Stream.iterate(start.toLocalDate(), x -&gt; x.plusDays(1)) // 截断无限流，长度为起始时间和结束时间的差+1个 .limit(ChronoUnit.DAYS.between(start, end) + 1) // 由于最后要的是字符串，所以map转换一下 .map(e -&gt; Date.from(e.atStartOfDay().atZone(ZoneId.systemDefault()).toInstant())) // 把流收集为List .collect(Collectors.toList());&#125;/*** 获取&#123;@link Date&#125;在开始时间和结束时间内的日期时间段&#123;@link LocalDate&#125;集合* @param start 开始时间* @param end 结束时间* @return 时间集合*/public static List&lt;LocalDate&gt; localDateZones(Date start, Date end)&#123; return localDateZones(from(start), from(end));&#125;/*** 获取 &#123;@link LocalDate&#125; 在开始时间和结束时间内的日期时间段 &#123;@link LocalDate&#125; 集合* @param start 开始时间* @param end 结束时间* @return 时间集合*/public static List&lt;LocalDate&gt; localDateZones(LocalDate start, LocalDate end)&#123; return Stream.iterate(start, x -&gt; x.plusDays(1)) .limit(ChronoUnit.DAYS.between(start, end) + 1) .collect(Collectors.toList());&#125;/*** 获取 &#123;@link LocalDateTime&#125; 在开始时间和结束时间内的日期时间段 &#123;@link LocalDate&#125; 集合* @param start 开始时间* @param end 结束时间* @return 时间集合*/public static List&lt;LocalDate&gt; localDateZones(LocalDateTime start, LocalDateTime end)&#123; // 用起始时间作为流的源头，按照每次加一天的方式创建一个无限流 return Stream.iterate(start.toLocalDate(), x -&gt; x.plusDays(1)) // 截断无限流，长度为起始时间和结束时间的差+1个 .limit(ChronoUnit.DAYS.between(start, end) + 1) .map(e -&gt; e.atStartOfDay().toLocalDate()) // 把流收集为List .collect(Collectors.toList());&#125; 示例测试12345678910111213141516171819202122232425262728String patternDate = "yyyy-MM-dd";List&lt;Date&gt; dateList = Arrays.asList(new Date(2017-1900, 11, 30), new Date(2018-1900, 0, 3));// 2017-12-30System.out.println("开始时间:" + format(dateList.get(0), patternDate) + ", 结束时间:" + format(dateList.get(1), patternDate));// [2017-12-30, 2017-12-31, 2018-01-01, 2018-01-02, 2018-01-03]System.out.println(dateZones(dateList.get(0), dateList.get(1)).stream().map(x -&gt; format(x, patternDate)).collect(Collectors.toList()));// [2017-12-30, 2017-12-31, 2018-01-01, 2018-01-02, 2018-01-03]System.out.println(localDateZones(dateList.get(0), dateList.get(1)).stream().map(x -&gt; format(x, patternDate)).collect(Collectors.toList()));LocalDateTime now = LocalDateTime.of(2017, Month.DECEMBER, 30, 0, 0, 0);// 2017-12-30System.out.println(format(now, patternDate));// [2017-12-30, 2017-12-31, 2018-01-01, 2018-01-02, 2018-01-03]System.out.println(dateZones(now, now.plus(4, ChronoUnit.DAYS)) .stream().map(x -&gt; format(x, patternDate)).collect(Collectors.toList()));// [2017-12-30, 2017-12-31, 2018-01-01, 2018-01-02, 2018-01-03]System.out.println(localDateZones(now, now.plus(4, ChronoUnit.DAYS)) .stream().map(x -&gt; format(x, patternDate)).collect(Collectors.toList()));// [2017-12-30, 2017-12-31, 2018-01-01, 2018-01-02, 2018-01-03]System.out.println(localDateZones(now.toLocalDate(), now.toLocalDate().plus(4, ChronoUnit.DAYS)) .stream().map(x -&gt; format(x, patternDate)).collect(Collectors.toList())); 日期加减示例代码123456789101112131415161718192021222324252627282930String patternDate = "yyyy-MM-dd HH:mm:ss";LocalDateTime now = LocalDateTime.of(2017, Month.DECEMBER, 30, 0, 0, 0);// 当前时间: 2017-12-30 00:00:00System.out.println("当前时间: " + format(now, patternDate));// 30秒前: 2017-12-29 23:59:30System.out.println("30秒前: " + format(now.plus(-30, ChronoUnit.SECONDS), patternDate));// 5分钟后: 2017-12-30 00:05:00System.out.println("5分钟后: " + format(now.plus(5, ChronoUnit.MINUTES), patternDate));// 2天前: 2017-12-28 00:00:00System.out.println("2天前: " + format(now.plus(-2, ChronoUnit.DAYS), patternDate));// 2天后: 2018-01-01 00:00:00System.out.println("2天后: " + format(now.plus(2, ChronoUnit.DAYS), patternDate));// 1周后: 2018-01-06 00:00:00System.out.println("1周后: " + format(now.plusWeeks(1), patternDate)); // 1月前: 2017-11-30 00:00:00System.out.println("1月前: " + format(now.plus(-1, ChronoUnit.MONTHS), patternDate));// 1月后: 2018-01-30 00:00:00System.out.println("1月后: " + format(now.plus(1, ChronoUnit.MONTHS), patternDate));// 1年后: 2018-12-30 00:00:00System.out.println("1年后: " + format(now.plus(1, ChronoUnit.YEARS), patternDate)); 日期推算示例代码12345678910111213141516171819202122232425262728293031String patternDate = "yyyy-MM-dd";LocalDateTime now = LocalDateTime.of(2017, Month.DECEMBER, 30, 0, 0, 0);// 当前时间: 2017-12-30System.out.println("当前时间: " + format(now, patternDate) + " ,是否闰年: " + now.toLocalDate().isLeapYear());// 当前月份: 十二月System.out.println("当前月份: " + Month.from(now).getDisplayName(TextStyle.FULL, Locale.CHINA));// 当前星期: 星期六System.out.println("当前星期: " + DayOfWeek.from(now).getDisplayName(TextStyle.FULL, Locale.CHINA));// 需要注意:java8提供的获取的本周第一天和本周最后一天是西方的界定方式, 第一天是周末, 最后一天是周六, 和中国的不太一样// 本周初第一天:2017-12-24System.out.println("本周初第一天: " + format(now.with(WeekFields.of(Locale.CHINA).dayOfWeek(),1L), patternDate));// 本周最后一天:2017-12-30System.out.println("本周最后一天: " + format(now.with(WeekFields.of(Locale.CHINA).dayOfWeek(),7L), patternDate));// 本月初第一天:2017-12-01System.out.println("本月初第一天: " + format(now.with(TemporalAdjusters.firstDayOfMonth()), patternDate));// 本月最后一天:2017-12-31System.out.println("本月最后一天: " + format(now.with(TemporalAdjusters.lastDayOfMonth()), patternDate));// 本年最后一天:2017-01-01System.out.println("本年最后一天: " + format(now.with(TemporalAdjusters.firstDayOfYear()), patternDate));// 本年最后一天:2017-12-31System.out.println("本年最后一天: " + format(now.with(TemporalAdjusters.lastDayOfYear()), patternDate)); Optional类说明 Optional类的Javadoc描述如下:这是一个可以为null的容器对象。如果值存在则isPresent()方法会返回true，调用get()方法会返回该对象。 Optional基础对象处理示例代码 123456789101112131415161718192021// 有则返回, 无则由函数产生System.out.println(Optional.ofNullable(null).orElseGet(() -&gt; Arrays.asList(1,2,3)));// 元素存在输出true,反之输出falseSystem.out.println(Optional.ofNullable("Elon").isPresent());// 不为空时输出元素,反之输出“默认值”System.out.println(Optional.ofNullable("Elon").orElse("默认值"));// 元素存在输出元素,反之抛出异常System.out.println(Optional.ofNullable("Elon").orElseThrow(IllegalArgumentException::new));// 元素存在输出true,反之输出falseSystem.out.println(Optional.ofNullable(null).isPresent());// 不为空时输出元素,反之输出“默认值”System.out.println(Optional.ofNullable(null).orElse("默认值"));// 元素存在输出元素,反之抛出异常System.out.println(Optional.ofNullable(null).orElseThrow(IllegalArgumentException::new)); 示例输出 12345678910[1, 2, 3]trueElonElonfalse默认值java.lang.IllegalArgumentException at java.util.Optional.orElseThrow(Optional.java:290) ... Optional基础集合处理示例代码 123456789101112131415List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 2, 5, 3);// 集合不为空的时候进行遍历去重,反之输出空集合System.out.println(Optional.ofNullable(list).orElse(Lists.newArrayList()).stream().distinct().collect(Collectors.toList()));// 集合不为空的时候进行遍历去重,反之抛出异常System.out.println(Optional.ofNullable(list).orElseThrow(IllegalArgumentException::new).stream().distinct().collect(Collectors.toList()));list = null;// 集合不为空的时候进行遍历去重,反之输出空集合System.out.println(Optional.ofNullable(list).orElse(Lists.newArrayList()).stream().distinct().collect(Collectors.toList()));// 集合不为空的时候进行遍历去重,反之抛出异常 IllegalArgumentExceptionSystem.out.println(Optional.ofNullable(list).orElseThrow(IllegalArgumentException::new).stream().distinct().collect(Collectors.toList())); 示例输出 1234567[1, 2, 3, 5][1, 2, 3, 5][]java.lang.IllegalArgumentException at java.util.Optional.orElseThrow(Optional.java:290) ... Optional复杂集合处理示例代码 1234567891011121314151617181920212223242526@Data@NoArgsConstructor@AllArgsConstructor@ToStringclass Person&#123; private String name; private int age; public String toString()&#123; return JSON.toJSONString(this); &#125;&#125;@Testpublic void list()&#123; Person person = null; System.out.println(Optional.ofNullable(person).map(x -&gt; x.getName()).orElse("默认姓名")); System.out.println(Optional.ofNullable(person).map(x -&gt; x.getAge()).orElse(18)); List&lt;Person&gt; list = Arrays.asList(new Person("p1", 10), new Person("p2", 15)); System.out.println("最大年龄用户:" + Optional.ofNullable(list).orElseThrow(NullPointerException:: new).stream().collect(Collectors.maxBy(Comparator.comparingInt(Person::getAge))).get()); System.out.println("最大年龄数值:" + Optional.ofNullable(list).orElseThrow(NullPointerException:: new).stream().collect(Collectors.maxBy(Comparator.comparingInt(Person::getAge))).get().getAge()); System.out.println("年龄均值数值:" + Optional.ofNullable(list).orElseThrow(NullPointerException:: new).stream().collect(Collectors.averagingInt(Person::getAge)));&#125; 示例输出 12345默认姓名18最大年龄用户:&#123;"age":15,"name":"p2"&#125;最大年龄数值:15年龄均值数值:12.5 Nashorn JavaScript引擎123456ScriptEngineManager manager = new ScriptEngineManager();ScriptEngine engine = manager.getEngineByName("JavaScript");// jdk.nashorn.api.scripting.NashornScriptEngineSystem.out.println( engine.getClass().getName() );// Result:2.0System.out.println("Result:" + engine.eval("function f() &#123; return 1; &#125;; f() + 1;")); Base64支持12345final String text = "Base64 finally in Java 8!";final String encoded = Base64.getEncoder().encodeToString(text.getBytes(StandardCharsets.UTF_8));System.out.println(encoded);final String decoded = new String(Base64.getDecoder().decode(encoded), StandardCharsets.UTF_8);System.out.println(decoded); Java8新特性示例图]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Guava 常用操作示例]]></title>
    <url>%2F2016%2F05%2F27%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2FGuava%20%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[本篇采用示例的形式展示Guava的常见工具应用. Guava集合排序示例代码1234567891011121314List&lt;Integer&gt; list = Arrays.asList(1, 2, 6, 7, 11, 9, 10, 4);System.out.println("原始集合:" + list);// 有空元素放置到最后System.out.println("自然正序:" + Ordering.natural().nullsLast().sortedCopy(list));// 按照自然排序然后翻转, 有空元素排到最后System.out.println("自然逆序:" + Ordering.natural().reverse().nullsLast().sortedCopy(list));// 获取集合最大元素System.out.println("最小元素:" + Ordering.natural().max(list));// 获取集合元素中最大的3个元素System.out.println("最大元素:" + Ordering.natural().greatestOf(list, 3));// 获取集合最小元素System.out.println("最小元素:" + Ordering.natural().min(list));// 获取集合元素中最小的3个元素System.out.println("最小元素:" + Ordering.natural().leastOf(list, 3)); 执行输出1234567原始集合:[1, 2, 6, 7, 11, 9, 10, 4]自然正序:[1, 2, 4, 6, 7, 9, 10, 11]自然逆序:[11, 10, 9, 7, 6, 4, 2, 1]最小元素:11最大元素:[11, 10, 9]最小元素:1最小元素:[1, 2, 4] 示例代码12345678910111213141516171819202122232425262728293031323334public static void main(String[] args) &#123; List&lt;User&gt; list = Arrays.asList(new User(1, "Abc"), new User(3, "BAb"), new User(2,"zbc"), new User(4,"fac")); System.out.println("原始集合:" + list); Ordering&lt;User&gt; natureAgeAscOrdering = Ordering.natural().nullsFirst().onResultOf(new Function&lt;User, Integer&gt;() &#123; public Integer apply(User input) &#123; return input.getAge(); &#125; &#125;); System.out.println("年龄升序:" + natureAgeAscOrdering.sortedCopy(list)); Ordering&lt;User&gt; natureNameAscOrdering = Ordering.natural().nullsFirst().onResultOf(new Function&lt;User, String&gt;() &#123; public String apply(User input) &#123; return input.getName(); &#125; &#125;); System.out.println("姓名升序:" + natureNameAscOrdering.sortedCopy(list));&#125; @Data@NoArgsConstructor@AllArgsConstructorpublic static class User&#123; private int age; private String name; @Override public String toString() &#123; return "User&#123;" + "age=" + age + ", name='" + name + '\'' + '&#125;'; &#125;&#125; 执行输出123原始集合:[User&#123;age=1, name='Abc'&#125;, User&#123;age=3, name='BAb'&#125;, User&#123;age=2, name='zbc'&#125;, User&#123;age=4, name='fac'&#125;]年龄升序:[User&#123;age=1, name='Abc'&#125;, User&#123;age=2, name='zbc'&#125;, User&#123;age=3, name='BAb'&#125;, User&#123;age=4, name='fac'&#125;]姓名升序:[User&#123;age=1, name='Abc'&#125;, User&#123;age=3, name='BAb'&#125;, User&#123;age=4, name='fac'&#125;, User&#123;age=2, name='zbc'&#125;] Guava集合过滤&amp;转换示例代码1234567891011121314List&lt;String&gt; list = Lists.newArrayList("a", "A", "1", "B", "as");Collection&lt;String&gt; filter = Collections2.filter(list, new Predicate&lt;String&gt;() &#123; public boolean apply(String s) &#123; return CharMatcher.JAVA_UPPER_CASE.matchesAllOf(s); &#125;&#125;);System.out.println("集合过滤:" + filter); Collection&lt;String&gt; transform = Collections2.transform(list, new Function&lt;String, String&gt;() &#123; public String apply(String s) &#123; return s.toUpperCase(); &#125;&#125;);System.out.println("集合转换:" + transform); 执行输出12集合过滤:[A, B]集合转换:[A, A, 1, B, AS] Guava集合切分示例代码12345List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7);List&lt;List&lt;Integer&gt;&gt; segmentList = Lists.partition(list, 4);for(List&lt;Integer&gt; segment: segmentList)&#123; System.out.println(segment);&#125; 执行输出12[1, 2, 3, 4][5, 6, 7] Guava不可变集合当你不希望修改一个集合类，或者想做一个常量集合类的时候，使用immutable集合类就是一个最佳的编程实践。 示例代码1234567891011121314151617181920ImmutableList&lt;Integer&gt; immutableList = ImmutableList.&lt;Integer&gt;builder() .add(1) .add(1, 2) .addAll(Arrays.asList(2, 3, 5, 5, 6)) .build();System.out.println("immutableList:\t" + immutableList); ImmutableSet&lt;Integer&gt; immutableSet = ImmutableSet.&lt;Integer&gt;builder() .add(1) .add(1, 2) .addAll(Arrays.asList(2, 3, 5, 5, 6)) .build();System.out.println("immutableSet:\t" + immutableSet); Map&lt;Integer, String&gt; map = Maps.newHashMap();map.put(1, "val1");map.put(2, "val2");map.put(2, "val3");ImmutableMap&lt;Integer, String&gt; immutableMap = ImmutableMap.copyOf(map);System.out.println("immutableMap:\t" + immutableMap); 执行输出123immutableList: [1, 1, 2, 2, 3, 5, 5, 6]immutableSet: [1, 2, 3, 5, 6]immutableMap: &#123;1=val1, 2=val3&#125; Guava缓存示例代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public static void main(String[] args) &#123; TestCache testCache = new TestCache(); String cacheKey = testCache.buildKey("dateKey"); System.out.println(testCache.getIfPresent(cacheKey)); System.out.println(testCache.getCache(cacheKey)); testCache.putCache(cacheKey, Arrays.asList(1L,2L,3L,4L,5L)); System.out.println(testCache.getIfPresent(cacheKey)); System.out.println(testCache.getCache(cacheKey));&#125; static class TestCache &#123; public static LoadingCache&lt;String, List&lt;Long&gt;&gt; loadingCache = null; public TestCache()&#123; init(); &#125; public void init()&#123; loadingCache = CacheBuilder.newBuilder() .expireAfterWrite(5, TimeUnit.MINUTES) // 5分钟自动过期 .build(new CacheLoader&lt;String, List&lt;Long&gt;&gt;() &#123; @Override public List&lt;Long&gt; load(String key) throws Exception &#123; return Lists.newArrayList(); // 默认数据不存在的获取方法 &#125; &#125;); &#125; //获取数据，如果不存在返回null public List&lt;Long&gt; getIfPresent(String key)&#123; return loadingCache.getIfPresent(key); &#125; //获取数据，如果数据不存在则通过cacheLoader获取数据，缓存并返回 public List&lt;Long&gt; getCache(String key)&#123; try &#123; return loadingCache.get(key); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null; &#125; //直接向缓存put数据 public void putCache(String key, List&lt;Long&gt; value)&#123; if(CollectionUtils.isEmpty(value))&#123; return; &#125; loadingCache.put(key, value); &#125; // 构建缓存Key public String buildKey(String cacheKey)&#123; return cacheKey; &#125;&#125; 执行输出1234null[][1, 2, 3, 4, 5][1, 2, 3, 4, 5]]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Guava</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解Java-Object类]]></title>
    <url>%2F2016%2F03%2F27%2F%E8%AF%AD%E8%A8%80%E8%AF%AD%E6%B3%95%2F%E7%90%86%E8%A7%A3Java-Object%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[Java中的Object有哪些公用方法1．clone方法保护方法, 创建并返回此对象的一个副本, 用于实现对象的浅复制, 只有实现了Cloneable接口才可以调用该方法，否则抛出CloneNotSupportedException异常;JAVA里除了8种基本类型传参数是值传递，其他的类对象传参数都是引用传递，我们有时候不希望在方法里将参数改变，这时就需要在类中复写clone方法. 2．getClass方法final方法，返回一个对象的运行时类。 3．toString方法返回该对象的字符串表示, 该方法用得比较多，一般子类都有覆盖。 4．finalize方法该方法用于释放资源。当垃圾回收器确定不存在对该对象的更多引用时，由对象的垃圾回收器调用此方法。子类重写 finalize 方法，以配置系统资源或执行其他清除。 在启用某个对象的 finalize 方法后，将不会执行进一步操作，直到 Java 虚拟机再次确定尚未终止的任何线程无法再通过任何方法访问此对象，其中包括由准备终止的其他对象或类执行的可能操作，在执行该操作时，对象可能被丢弃。 对于任何给定对象，Java 虚拟机最多只调用一次 finalize 方法。 5．equals方法该方法是非常重要的一个方法。一般equals和==是不一样的，但是在Object中两者是一样的。子类一般都要重写这个方法。 6．hashCode方法该方法用于哈希查找，可以减少在查找中使用equals的次数，重写了equals方法一般都要重写hashCode方法。这个方法在一些具有哈希功能的Collection中用到。 一般必须满足obj1.equals(obj2)==true。可以推出obj1.hashCode()==obj2.hashCode()，但是hashCode相等不一定就满足equals。不过为了提高效率，应该尽量使上面两个条件接近等价。 如果不重写hashCode(),在HashSet中添加两个equals的对象，会将两个对象都加入进去。 7．wait方法wait方法就是使当前线程等待该对象的锁，当前线程必须是该对象的拥有者，也就是具有该对象的锁。wait()方法一直等待，直到获得锁或者被中断。wait(long timeout)设定一个超时间隔，如果在规定时间内没有获得锁就返回。 调用该方法后当前线程进入睡眠状态，直到以下事件发生。 （1）其他线程调用了该对象的notify方法。 （2）其他线程调用了该对象的notifyAll方法。 （3）其他线程调用了interrupt中断该线程。 （4）时间间隔到了。 此时该线程就可以被调度了，如果是被中断的话就抛出一个InterruptedException异常。 8．notify方法该方法唤醒在该对象上等待的某个线程。 9．notifyAll方法该方法唤醒在该对象上等待的所有线程。 面试延伸 Java是值传递还是引用传递 Java是指传递, 对于基本类型传递的是值, 对于引用类型传递的是指针的地址 值传递：方法调用时，实际参数把它的值传递给对应的形式参数，方法执行中形式参数值的改变不影响实际参数的值。 引用传递：也称为传地址, 方法调用时实际参数的引用(地址,而不是参数的值)被传递给方法中相对应的形式参数,在方法执行中对形式参数的操作实际上就是对实际参数的操作,方法执行中形式参数值的改变将会影响实际参数的值. 阐述final、finally、finalize的区别 final：修饰符(关键字)有三种用法：(1)修饰类：表示该类不能被继承；(2)修饰方法：表示方法不能被重写；(3)修饰变量：表示变量只能一次赋值以后值不能被修改（常量） finally：通常放在try…catch…的后面构造总是执行代码块(try{}里的return语句，其后finally{}里的代码会方法返回给调用者前执行)，这就意味着程序无论正常执行还是发生异常，这里的代码只要JVM不关闭都能执行，可以将释放外部资源的代码写在finally块中。 finalize：Object类中定义的方法，Java中允许使用finalize()方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作(如关闭连接、关闭文件)。这个方法一般不会显示的调用, 通常是由垃圾收集器在销毁对象时调用的，通过重写finalize()方法可以整理系统资源或者执行其他清理工作。 说明: http://www.iteye.com/topic/484934 equals与==的区别 == 比较的是变量(栈)内存中存放的对象的(堆)内存地址，用来判断两个对象的地址是否相同，即是否是指相同一个对象。比较的是真正意义上的指针操作. equals 比较的是两个对象的内容是否相等，由于所有的类都是继承自java.lang.Object类的，所以适用于所有对象，而equals()可以返回true或者false主要取决于重写equals方法的实现逻辑. 代码示例 123456789101112131415public static void main(String[] args) &#123; String a = new String("ab"); // a 为一个引用 String b = new String("ab"); // b为另一个引用,对象的内容一样 String aa = "ab"; // 放在常量池中 String bb = "ab"; // 从常量池中查找 if (aa == bb) // true System.out.println("aa==bb"); if (a == b) // false，非同一对象 System.out.println("a==b"); if (a.equals(b)) // true System.out.println("aEQb"); if (42 == 42.0) &#123; // true System.out.println("true"); &#125;&#125; 对象值相同(x.equals(y)为true)，但却可以有不同的hash值? 如果两个对象满足equals为true，既 x.equals(y)==true, 那么他的哈希码(hash code)必然相同. 如果两个对象的hashCode相同, 它们并不一定相同. 如何解决Hash冲突 通过构造性能良好的哈希函数，可以减少冲突，但一般不可能完全避免冲突，因此解决冲突是哈希法的另一个关键问题。创建哈希表和查找哈希表都会遇到冲突，两种情况下解决冲突的方法应该一致。下面以创建哈希表为例，说明解决冲突的方法。常用的解决冲突方法有以下四种: 开放定址法又称为开放散列法 基本思想是: 当关键字key的哈希地址p=H(key)出现冲突时，以p为基础,产生另一个哈希地址p1,如果p1仍然冲突,再以p为基础,产生另一个哈希地址p2, …;直到找出一个不冲突的哈希地址pi ，将相应元素存入其中. 这种方法有一个通用的再散列函数形式: Hi = ( H(key) + di ) % m 其中 i=1，2，…，n 说明: H(key)为哈希函数, m为表长, di称为增量序列, 增量序列的取值方式不同，相应的再散列方式也不同, 主要有以下三种： 线性探测再散列di i=1，2，3，…，m-1 这种方法的特点是: 冲突发生时，顺序查看表中下一单元，直到找出一个空单元或查遍全表。 二次探测再散列di = 12，-12，22，-22，…，k2，-k2 (k&lt;=m/2) 这种方法的特点是: 冲突发生时，在表的左右进行跳跃式探测，比较灵活。 伪随机探测再散列di = 伪随机数序列. 示例说明例如: 已知哈希表长度m=11，哈希函数为：H(key) = key % 11，则H（47）=3，H（26）=4，H（60）=5，假设下一个关键字为69，则H（69）=3，与47冲突. 用线性探测再散列处理冲突: 下一个哈希地址为H1=(3 + 1) % 11 = 4，仍然冲突，再找下一个哈希地址为H2= (3 + 2) % 11 = 5，还是冲突，继续找下一个哈希地址为H3=（3 + 3）% 11 = 6，此时不再冲突，将69填入5号单元。 用二次探测再散列处理冲突: 下一个哈希地址为H1=(3 + 12)% 11 = 4，仍然冲突，再找下一个哈希地址为H2= (3 - 12) % 11 = 2，此时不再冲突，将69填入2号单元。 用伪随机探测再散列处理冲突: 设伪随机数序列为: 2，5，9，……..，则下一个哈希地址为H1=（3 + 2）% 11 = 5，仍然冲突，再找下一个哈希地址为H2=（3 + 5）% 11 = 8，此时不再冲突，将69填入8号单元。 再哈希法基本思想: 同时构造多个不同的哈希函数: Hi=RH1(key) i=1, 2, …, k 当哈希地址Hi=RH1(key)发生冲突时，再计算Hi=RH2(key)…, 直到冲突不再产生。这种方法不易产生聚集，但增加了计算时间. 链地址法基本思想: 将所有哈希地址相同的记录都链接在同一单链表中; 并将单链表的头指针存在哈希表的第i个单元中，因而查找、插入和删除主要在同义词链中进行. 链地址法适用于经常进行插入和删除的情况. 建立公共溢出区基本思想: 将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表 参考 http://www.cnblogs.com/zjc950516/p/7877511.html https://blog.csdn.net/qq_33098049/article/details/82664061 https://www.cnblogs.com/wuchaodzxx/p/7396599.html https://blog.csdn.net/yeiweilan/article/details/73412438 https://www.cnblogs.com/qqzy168/p/3246057.html https://jingyan.baidu.com/article/4f34706e0a1ba9e387b56dab.html https://www.cnblogs.com/moongeek/p/7631447.html https://www.jianshu.com/p/25e243850bd2?appinstall=0]]></content>
      <categories>
        <category>语言语法</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解Java-面向对象]]></title>
    <url>%2F2016%2F03%2F27%2F%E8%AF%AD%E8%A8%80%E8%AF%AD%E6%B3%95%2F%E7%90%86%E8%A7%A3Java-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[面向对象三大特征和七大原则 三大特征封装封装就是对属性和方法的载体类,只能通过其提供的接口(方法)来访问,而把实现细节隐藏起来.也就是说,具体实现对程序员来说是透明的,封装的好处在于对类内部的改变,不会影响到其他代码 封装的做法: 私有属性(private修饰符修饰属性)、提供public的读(getXX)写(setXX)方法、在构造中调用方法.所有的非常量属性基本都需要封装. 封装的好处：隐藏类的实现细节、对所有用户提供统一的接口、增强执行效果、易于维护和扩展 继承继承是一种关系,逻辑上满足子类is a 父类的关系才使用继承. 子类继承父类的属性和非私有方法.不能继承父类的构造,继承使用关键字extends,类单继承,接口多继承. 在构造子类对象时,依次调用父类的构造(子类默认调用父类的无参构造.可以使用super(参数列表)来调用指定的父类的含参构造)到Object为止.再调用子类自身的; 子类调用父类的构造时,父类的构造只能调用一个且必须写在子类构造的第一句. 多态多态性是指允许不同类的对象对同一消息作出响应。多态性包括参数化多态性和包含多态性。多态性语言具有灵活、抽象、行为共享、代码共享的优势，很好的解决了应用程序函数同名问题; 多态的类型有下面4种: 基本类型的多态: 拆箱、装箱.本质上是基本类型之间的自动类型转换,Java语言中将8种数据类型都分别封装了一个类,这些类中封装了各基本数据类型的属性和基本运算. 基本类型自动转换为对应的封装类的操作叫做自动装箱操作,反之叫自动拆箱操作,自动装箱操作有一个自动装箱池(范围为-128~127).只要自动装箱的数在自动装箱池范围内,则直接去池中找数据. 方法的多态: 重载、重写. 重写(overriding): 父类继承过来的方法对子类不合适时子类可以改变该方法的实现,这种操作叫做方法的重写/覆盖(继承是重写的前提条件);重写要求:1、返回值、方法名和参数相同((5.0以后允许返回子类类型))；2、子类异常不能超出父类异常；3、子类访问级别不能低于父类访问级别. 重载(overloading): 重载是在同一个类中存在两个或两个以上的同名方法，但是参数不同(参数个数不同、类型不同、顺序不同&lt;(int,String)和(String,int)是不一样的&gt;),方法体也不相同. 返回值类型可以相同可以不相同.最常用的重载例子便是构造函数。 类或者接口的多态: 父类的引用指向子类的对象父类的引用指向子类的对象(Person p = new Student())就发生了多态, 该场景下： 只能使用父类中方定义的属性和方法 子类中定义的不能直接使用 子类复写了父类的方法,此时调用情况根据方法是否static而不同 [static(调用父类),非static(调用子类)]. 如果想使用子类中定义的方法,可以强制类型转换(判断是否可以转换,用instance of运算符来判断对象的类型) 程序示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class A &#123; int a = 1; static int b = 20; int c = 3; double d = 2.0; void show() &#123; System.out.println("Class A: a=" + a + "\td=" + d + "\tc=" + c + "\tb=" + b); &#125; void common()&#123; System.out.println("Class A: method common()"); &#125; static void execute()&#123; System.out.println("Class A: method excute()"); &#125;&#125;class B extends A &#123; float a = 3.0f; static int b = 30; int c = 5; String d = "Java program."; void show() &#123; super.show(); System.out.println("Class B: a=" + a + "\td=" + d + "\tc=" + c + "\tb=" +b); &#125; void common()&#123; System.out.println("Class B: method common()"); &#125; static void execute()&#123; System.out.println("Class B: method execute()"); &#125; public static void main(String[] args) &#123; A a = new B(); a.show(); System.out.println("----------------------"); a.common(); System.out.println("----------------------"); a.execute(); &#125;&#125; 执行输出 123456Class A: a=1 d=2.0 c=3 b=20Class B: a=3.0 d=Java program. c=5 b=30----------------------Class B: method common()----------------------Class A: method excute() 传参时的多态基本类型的多态与类类型的多态混合使用, 这里会涉及常见面试题: Java中的参数传递是传传递还是传引用？解答: Java语言的方法调用只支持参数的值传递, 当一个对象实例作为一个参数被传递到方法中时，参数的值就是对该对象的引用。对象的属性可以在被调用过程中被改变，但对对象引用的改变是不会影响到调用者的。 程序示例 1234567891011121314151617181920212223242526272829public class Test &#123; public static void invoke(int num, Person person)&#123; num = 222; person.setAge(20); person.setName("李四"); System.out.println(num + "," + person.getName() + "," + person.getAge()); &#125; public static void main(String[] args) &#123; int num = 111; Person person = new Person("张三", 10); invoke(num, person); System.out.println(num + "," + person.getName() + "," + person.getAge()); &#125; @Data static class Person&#123; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; private String name; private int age; &#125;&#125; 程序输出 12222,李四,20111,李四,20 七大原则 单一职责原则(一个类只做它该做的事情)、 开闭原则(更改性封闭，扩展性开放)、 依赖倒转原则(面向接口编程)、 里氏替换原则(任何时候都可以用子类型替换掉父类型)、 接口隔离原则(接口要小而专，绝不能大而全)、 合成复用原则(优先使用聚合或合成关系复用代码)、 迪米特原则(对象与对象之间应该使用尽可能少的方法来关联) 说明: https://www.cnblogs.com/Smile-123/p/5385663.html]]></content>
      <categories>
        <category>语言语法</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解Java-类与变量]]></title>
    <url>%2F2016%2F03%2F27%2F%E8%AF%AD%E8%A8%80%E8%AF%AD%E6%B3%95%2F%E7%90%86%E8%A7%A3Java-%E7%B1%BB%E4%B8%8E%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[Java程序初始化顺序Java语言中当实例化对象时,对象所在的类的所有成员变量首先要进行实例化, 只有当所有类成员实例化后,才会调用对象所在类的构造函数创建对象.Java程序的初始化一般遵循3个原则(优先级依次递减): 1). 静态对象(变量)优先于非静态对象(变量)初始化. 2). 分类优先于子类进行初始化. 3). 按照成员变量的定义顺序进行初始化.即使变量定义散布于方法定义之中, 他们依然在任何方法(包括构造函数)被调用之前先初始化. 程序示例 123456789101112131415161718192021222324252627282930class B &#123; static &#123; System.out.println("B with static "); &#125; &#123; System.out.println("B with code block"); &#125; public B()&#123; System.out.println("B with construct"); &#125;&#125;class A extends B &#123; static &#123; System.out.println("A with static"); &#125; &#123; System.out.println("A with code block"); &#125; public A()&#123; System.out.println("A with construct"); &#125;&#125; 场景1用例: 12345678public class Test &#123; public static void main(String[] args) &#123; new B(); System.out.println("------"); new B(); &#125;&#125; 场景1输出: 123456B with static B with code blockB with construct------B with code blockB with construct 场景2用例: 123456public class Test &#123; public static void main(String[] args) &#123; new A(); &#125;&#125; 场景2输出: 123456B with static A with staticB with code blockB with constructA with code blockA with construct 场景3用例: 12345678public class Test &#123; public static void main(String[] args) &#123; new A(); System.out.println("------"); new A(); &#125;&#125; 场景3输出: 1234567891011B with static A with staticB with code blockB with constructA with code blockA with construct------B with code blockB with constructA with code blockA with construct Java语言的变量类型在Java语言中,变量的类型只要有3种: 成员变量、静态变量和局部变量. 成员变量类成员变量的作用范围与类的实例化对象的作用范围相同, 当类被实例化时, 成员变量就会在内存中分配空间并初始化，直到这个被实例化对象的生命周期结束时成员变量的生命周期才结束.类成员变量作用域有4种,访问权限范围由大到小依次为: public &gt; protected &gt; default &gt; private. public: 表明该成员变量或方法对所有类或对象都是可见的，所有类或对象都可以直接访问. protected: 表明该成员变量或方法对自己及其子类是可见的，除此之外的其他类或对象都没有访问权限. default: 表明该成员变量或方法只有自己和与其位于同一包内的类可见, 若父类与子类位于不同的包内,则无访问权限. private: 表明该成员变量或方法是私有的，只有当前类对其具有访问权限，除此之外 的其他类或者对象都没有访问权限. 注意: 这些修饰符只能修饰成员变量，不能用来修饰局部变量, private与protected不能用来修饰类(只有 public、 abstract或 final 能用来修饰类). 静态变量被static修饰的成员变量称为静态变量或全局变量,与成员变量不同的是,静态变量不依赖于特定的实例，而是被所有实例所共享; 也就是说: 只要一个类被加载, JVM就会给类的静态变量分配存储空间. 因此就可以通过类名和变量名来访问静态变量. 局部变量局部变量的作用域与可见性为它所在的花括号内.]]></content>
      <categories>
        <category>语言语法</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解Java-理论基础]]></title>
    <url>%2F2016%2F03%2F27%2F%E8%AF%AD%E8%A8%80%E8%AF%AD%E6%B3%95%2F%E7%90%86%E8%A7%A3Java-%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[思维导图系列 JVM体系 基础体系 地址附录 https://www.processon.com/view/link/5c401ef2e4b08a7683ab26a9]]></content>
      <categories>
        <category>语言语法</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解Java-IO模型]]></title>
    <url>%2F2016%2F03%2F27%2F%E8%AF%AD%E8%A8%80%E8%AF%AD%E6%B3%95%2F%E7%90%86%E8%A7%A3Java-IO%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[在高性能的IO体系设计中，有几个名词概念需要先做个概述. 同步和异步、阻塞和非阻塞同步和异步是针对应用程序与内核的交互而言的,阻塞和非阻塞是针对于进程在访问数据的时候，根据IO操作的就绪状态来采取的不同方式，说白了是一种读取或者写入操作函数的实现方式，阻塞方式下读取或者写入函数将一直等待，而非阻塞方式下，读取或者写入函数会立即返回一个状态值.简而言之: 同步和异步是目的，阻塞和非阻塞是实现方式。 同步: 指的是用户进程触发IO操作并等待或者轮询的去查看IO操作是否就绪. 异步: 指用户进程触发IO操作以后便开始做自己的事情，而当IO操作已经完成的时候会得到IO完成的通知(异步的特点就是通知)告诉朋友自己合适衣服的尺寸，大小，颜色，让朋友委托去卖，然后自己可以去干别的事. 阻塞: 所谓阻塞方式的意思是指, 当试图对该文件描述符进行读写时, 如果当时没有东西可读,或者暂时不可写, 程序就进入等待 状态, 直到有东西可读或者可写为止. 比如:去公交站充值，发现这个时候，充值员不在(可能上厕所去了)，然后我们就在这里等待，一直等到充值员回来为止. 非阻塞: 非阻塞状态下,如果没有东西可读或者不可写, 读写函数马上返回而不会等待. 比如: 银行里取款办业务时，领取一张小票，领取完后我们自己可以玩玩手机，或者与别人聊聊天，当轮我们时，银行的喇叭会通知，这时候我们就可以去了. Java中的IO模型 Java中主要的三种IO模型: 阻塞IO（BIO）、非阻塞IO（NIO）和 异步IO（AIO）. Java中提供的IO有关的API，在文件处理的时候，其实依赖操作系统层面的IO操作实现的。比如在Linux 2.6以后，Java中NIO和AIO都是通过epoll来实现的，而在Windows上，AIO是通过IOCP来实现的. 同步阻塞IO(JAVA BIO)同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善. 同步非阻塞IO(Java NIO)同步非阻塞，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。用户进程也需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问. 异步阻塞IO(Java NIO)此种方式下是指应用发起一个IO操作以后，不等待内核IO操作的完成，等内核完成IO操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问IO是否完成，那么为什么说是阻塞的呢？因为此时是通过select系统调用来完成的，而select函数本身的实现方式是阻塞的，而采用select函数有个好处就是它可以同时监听多个文件句柄(如果从UNP的角度看，select属于同步操作。因为select之后，进程还需要读写数据)从而提高系统的并发性！ 异步非阻塞IO(Java AIO(NIO.2))在此种模式下，用户进程只需要发起一个IO操作然后立即返回，等IO操作真正的完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的IO读写操作，因为真正的IO读取或者写入操作已经由内核完成了. BIO、NIO、AIO应用场景 BIO方式适用于连接数目比较小且固定的架构,这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解. NIO方式适用于连接数目多且连接比较短(轻操作)的架构,比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持. AIO方式使用于连接数目多且连接比较长(重操作)的架构,比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持. Linux中的IO模型 在Linux(UNIX)中共有五种IO模型: 阻塞IO模型、非阻塞IO模型、信号驱动IO模型、IO复用模型 和 异步IO模型. 阻塞IO模型阻塞 I/O 是最简单的 I/O 模型，一般表现为进程或线程等待某个条件，如果条件不满足，则一直等下去。条件满足，则进行下一步操作. 应用进程通过系统调用 recvfrom 接收数据，但由于内核还未准备好数据报，应用进程就会阻塞住，直到内核准备好数据报，recvfrom 完成数据报复制工作，应用进程才能结束阻塞状态。 非阻塞IO模型非阻塞的IO模型。应用进程与内核交互，目的未达到之前，不再一味的等着，而是直接返回。然后通过轮询的方式，不停的去问内核数据准备有没有准备好。如果某一次轮询发现数据已经准备好了，那就把数据拷贝到用户空间中。 应用进程通过 recvfrom 调用不停的去和内核交互，直到内核准备好数据。如果没有准备好，内核会返回error，应用进程在得到error后，过一段时间再发送recvfrom请求。在两次发送请求的时间段，进程可以先做别的事情. 信号驱动IO模型应用进程在读取文件时通知内核，如果某个 socket 的某个事件发生时，请向我发一个信号。在收到信号后，信号对应的处理函数会进行后续处理。 应用进程预先向内核注册一个信号处理函数，然后用户进程返回，并且不阻塞，当内核数据准备就绪时会发送一个信号给进程，用户进程便在信号处理函数中开始把数据拷贝的用户空间中。 IO复用模型多个进程的IO可以注册到同一个管道上，这个管道会统一和内核进行交互。当管道中的某一个请求需要的数据准备好之后，进程再把对应的数据拷贝到用户空间中。 IO多路转接是多了一个select函数，多个进程的IO可以注册到同一个select上，当用户进程调用该select，select会监听所有注册好的IO，如果所有被监听的IO需要的数据都没有准备好时，select调用进程会阻塞。当任意一个IO所需的数据准备好之后，select调用就会返回，然后进程在通过recvfrom来进行数据拷贝。 这里的IO复用模型，并没有向内核注册信号处理函数，所以，他并不是非阻塞的。进程在发出select后，要等到select监听的所有IO操作中至少有一个需要的数据准备好，才会有返回，并且也需要再次发送请求去进行文件的拷贝。 异步IO模型异步IO模型。应用进程把IO请求传给内核后，完全由内核去操作文件拷贝。内核完成相关操作后，会发信号告诉应用进程本次IO已经完成. 用户进程发起aio_read操作之后，给内核传递描述符、缓冲区指针、缓冲区大小等，告诉内核当整个操作完成时，如何通知进程，然后就立刻去做其他事情了。当内核收到aio_read后，会立刻返回，然后内核开始等待数据准备，数据准备好以后，直接把数据拷贝到用户控件，然后再通知进程本次IO已经完成。 5种IO模型对比 说明: 阻塞IO模型、非阻塞IO模型、IO复用模型和信号驱动IO模型都是同步的IO模型。原因是因为，无论以上那种模型，真正的数据拷贝过程，都是同步进行的。 虽然信号驱动IO模型，内核是在数据准备好之后通知进程，然后进程再通过recvfrom操作进行数据拷贝。我们可以认为数据准备阶段是异步的，但是，数据拷贝操作是同步的。所以，整个IO过程也不能认为是异步的。 参考 https://mp.weixin.qq.com/s/fVpb1R0o2knXh3_uZeaQBA https://www.cnblogs.com/xingzc/p/5796413.html http://cmsblogs.com/?cat=183]]></content>
      <categories>
        <category>语言语法</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集合对象纵向維度聚合]]></title>
    <url>%2F2016%2F01%2F11%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2F%E9%9B%86%E5%90%88%E5%AF%B9%E8%B1%A1%E7%BA%B5%E5%90%91%E7%B6%AD%E5%BA%A6%E8%81%9A%E5%90%88%2F</url>
    <content type="text"><![CDATA[业务开发中经常会遇到列项聚合的问题,所以常用的一些思路整理出来. 集合纵向聚合系统中可能存在不同的业务流程聚合出来的(K,V)结构数据, 然后需要汇总, 可以采用Java8进行流式处理 逻辑抽象12345678910111213141516/** * 进行map集合中相同元素的value值聚合 * @param map1 基础map * @param map2 扩展map * @return */public static &lt;K, V&gt; Map&lt;K, List&lt;V&gt;&gt; keyMerge(Map&lt;K, List&lt;V&gt;&gt; map1, Map&lt;K, List&lt;V&gt;&gt; map2)&#123; Map&lt;K, List&lt;V&gt;&gt; base = Optional.ofNullable(map1).orElse(Maps.newHashMap()); Map&lt;K, List&lt;V&gt;&gt; ext = Optional.ofNullable(map2).orElse(Maps.newHashMap()); ext.keySet().forEach( key -&gt; base.merge(key, ext.get(key), (v1,v2) -&gt; Arrays.asList(v1, v2).stream().flatMap(Collection::stream).collect(Collectors.toList()) ) ); return base;&#125; 测试示例1234567891011@Testpublic void keyMerge() &#123; Map&lt;String,List&lt;Integer&gt;&gt; m1 = Maps.newHashMap(); m1.put("L1", Arrays.asList(1, 2, 3)); Map&lt;String,List&lt;Integer&gt;&gt; m2 = Maps.newHashMap(); m2.put("L1", Arrays.asList(3, 4, 5)); m2.put("L2", Arrays.asList(1, 1, 2, 2)); System.out.println("根据Key值聚合结果:" + JSON.toJSONString(keyMerge(m1, m2)));&#125; 执行结果1根据Key值聚合结果:&#123;"L1":[1,2,3,3,4,5],"L2":[1,1,2,2]&#125; 对象纵项聚合我们的业务系统可能纵向扩好几个表，但是各个表之间又有纵向key关联, 为了不进行太多JOIN方式的聚合, 异步可以进行单维度查询,然后最后结果并行聚合 纵向装配方案定义 定义对象唯一规范 123456789public interface Atomicity &#123; /** * 唯一标识定义 * @return */ String uniquely();&#125; 定义可纵向装配行为 12345678public interface Assemble &lt;D extends Atomicity&gt; extends Atomicity &#123; /** * 可纵向装配操作 */ D assemble(D d);&#125; 定义纵向装配逻辑 12345678910111213141516171819202122232425262728293031public class Assembly &#123; private Assembly()&#123;&#125; /** * 列项追加逻辑，将assemblyList上的元素追加到baseDataList上, * 如果baseDataList为空,则返回空集合。 * 如果待追加的元素不存在于目标集合中则忽略 * @param baseDataList 目标集合元素 * @param assembleList 追加集合元素 * @param &lt;K&gt; * @param &lt;V&gt; * @return 返回目标集合中填充后的元素对象 */ public static &lt;K extends Atomicity, V extends Assemble&lt;K&gt;&gt; List&lt;K&gt; assem(List&lt;K&gt; baseDataList, List&lt;V&gt; assembleList) &#123; Map&lt;String, K&gt; baseMap = Optional.ofNullable(baseDataList).orElse(Lists.newArrayList()) .stream().collect(Collectors.toMap(K::uniquely, Function.identity())); Optional.ofNullable(assembleList).orElse(Lists.newArrayList()) .stream().forEach(x -&gt;&#123; K k = baseMap.get(x.uniquely()); if(null != k) &#123; x.assemble(k); &#125; &#125; ); return baseDataList; &#125;&#125; 纵向装配使用示例 对象实现纵向装配行为 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465@Datastatic class AssemMock implements Assemble&lt;AssemMock&gt;&#123; private Integer id; private String name; private Integer age; public AssemMock() &#123; &#125; public AssemMock(Integer id) &#123; this.id = id; &#125; public AssemMock(Integer id, String name) &#123; this.id = id; this.name = name; &#125; public AssemMock(Integer id, Integer age) &#123; this.id = id; this.age = age; &#125; public AssemMock(Integer id, String name, Integer age) &#123; this.id = id; this.name = name; this.age = age; &#125; /** * 定义填充依赖的唯一因子 * @return */ @Override public String uniquely() &#123; return String.valueOf(id); &#125; /** * 定义填充依赖的附属逻辑 * @param assemMock * @return */ @Override public AssemMock assemble(AssemMock assemMock) &#123; if(null != getId())&#123; assemMock.setId(getId()); &#125; if(null != getName())&#123; assemMock.setName(getName()); &#125; if(null != getAge())&#123; assemMock.setAge(getAge()); &#125; return assemMock; &#125; @Override public String toString()&#123; return JSON.toJSONString(this); &#125;&#125; 对象纵向装配行为测试 12345678910111213141516171819202122232425public static void main(String[] args) &#123; List&lt;AssemMock&gt; targets = Arrays.asList( new AssemMock(1), new AssemMock(2), new AssemMock(3) ); List&lt;AssemMock&gt; assem1 = Arrays.asList( new AssemMock(1, 21), new AssemMock(2, 20), new AssemMock(3, 19), new AssemMock(4, 22) ); List&lt;AssemMock&gt; assem2 = Arrays.asList( new AssemMock(1, "dennisit"), new AssemMock(2, "elonsu"), new AssemMock(3, "gosling"), new AssemMock(4, "andi") ); System.out.println("目标集合为空:" + Assembly.assem(Lists.newArrayList(), targets)); System.out.println("追加集合为空:" + Assembly.assem(targets, Lists.newArrayList())); // 因为目标集合中不存在唯一属性为4的对象, 所以待追加集合中的属性为4的对象被忽略 System.out.println("列项追加对象:" + Assembly.assem(targets, assem1)); System.out.println("列项追加对象:" + Assembly.assem(targets, assem2));&#125; 执行结果 1234目标集合为空:[]追加集合为空:[&#123;"id":1&#125;, &#123;"id":2&#125;, &#123;"id":3&#125;]列项追加对象:[&#123;"age":21,"id":1&#125;, &#123;"age":20,"id":2&#125;, &#123;"age":19,"id":3&#125;]列项追加对象:[&#123;"age":21,"id":1,"name":"dennisit"&#125;, &#123;"age":20,"id":2,"name":"elonsu"&#125;, &#123;"age":19,"id":3,"name":"gosling"&#125;] 可以看到,数据根据我们定义的唯一性规范进行了列项装配.实际开发中对于不同的列数据可以进行异步处理. 然后展示的时候聚合异步处理的结果]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RestTemplate使用說明]]></title>
    <url>%2F2015%2F12%2F07%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2FRestTemplate%20%E5%BA%94%E7%94%A8%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[RestTemplate介绍 RestTemplate是Spring提供的用于访问Rest服务的客户端，同时RestTemplate提供了多种便捷访问远程Http服务的方法，能够大大提高客户端的编写效率。 RestTemplate初探通过实例展示RestTemplate的使用 1234567891011121314151617181920212223242526272829@Testpublic void test01()&#123; RestTemplate restTemplate = new RestTemplate(); String url = "http://www.baidu.com?id=&#123;id&#125;&amp;name=&#123;name&#125;&amp;num=&#123;age&#125;"; ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(url, String.class, 1, "Elon", 20); if(responseEntity.getStatusCode() == HttpStatus.OK)&#123; System.out.println(responseEntity.getBody()); &#125;&#125;@Testpublic void test02()&#123; RestTemplate restTemplate = new RestTemplate(); String url = "http://localhost:8080/api/v1/org/&#123;department&#125;/list"); HttpEntity&lt;String&gt; formEntity = new HttpEntity&lt;String&gt;(getHttpHeaders()); ResponseEntity&lt;String&gt; responseEntity = restTemplate.exchange(url, HttpMethod.GET, formEntity, String.class, 2); if(responseEntity.getStatusCode() == HttpStatus.OK)&#123; System.out.println(responseEntity.getBody()); &#125;&#125;// Mock登录授权信息public static HttpHeaders getHttpHeaders()&#123; HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_JSON_UTF8); headers.setAccept(Arrays.asList(MediaType.APPLICATION_JSON_UTF8)); headers.add("Cookie", "token=123244545"); return headers;&#125; 示例参数: String url: 该方法第一个参数标识请求的url Class responseType: 该参数标识该次HTTP请求的结果映射成的对象类型, Object… urlVariables: 该参数用来为url中的参数赋值,支持多个值传递. RestTemplate的交互说明上面示例中的交互可以分为三个步骤: url和参数聚合 HTTP请求交互 HTTP结果映射 对应的SpringRest中的如下方法. 12345@Overridepublic &lt;T&gt; T execute(String url, HttpMethod method, RequestCallback requestCallback, ResponseExtractor&lt;T&gt; responseExtractor, Object... urlVariables) throws RestClientException &#123; URI expanded = getUriTemplateHandler().expand(url, urlVariables); return doExecute(expanded, method, requestCallback, responseExtractor);&#125; URL和参数聚合RestTemplate的源码中使用getUriTemplateHandler().expand(url, urlVariables)来完善完成URL中占位符参数的聚合。示例: 1234567@Testpublic void url()&#123; String url = "http://www.baidu.com?id=&#123;id&#125;&amp;name=&#123;name&#125;&amp;num=&#123;age&#125;"; URI expanded = restTemplate.getUriTemplateHandler().expand(url, 1, "Elon", 20); // 输出结果： http://www.baidu.com?id=1&amp;name=Elon&amp;num=20 System.out.println(expanded.toString());&#125; RestTemplate中HTTP交互过程http交互规范的定义了解该块前先看看Spring中对ClientHttpRequest和对应的工厂接口的定义 ClientHttpRequest接口的定义 ClientHttpRequest接口的定义很简单,一个钩子方法 123public interface ClientHttpRequest extends HttpRequest, HttpOutputMessage &#123; ClientHttpResponse execute() throws IOException;&#125; ClientHttpRequest接口默认提供了如下的扩展类 ClientHttpRequestFactory接口的定义 ClientHttpRequestFactory接口的定义如下： 123public interface ClientHttpRequestFactory &#123; ClientHttpRequest createRequest(URI uri, HttpMethod httpMethod) throws IOException;&#125; ClientHttpRequestFactory接口提供的实现类如下: HttpAccessor的代理实现 Spring中定义了抽象类HttpAccessor, 提供了默认的ClientHttpRequestFactory实现,ClientHttpRequestFactory默认值为SimpleClientHttpRequestFactory(). 对应的HttpAccessor实现的代码:12345678910111213141516171819202122public abstract class HttpAccessor &#123; protected final Log logger = LogFactory.getLog(getClass()); private ClientHttpRequestFactory requestFactory = new SimpleClientHttpRequestFactory(); public void setRequestFactory(ClientHttpRequestFactory requestFactory) &#123; Assert.notNull(requestFactory, "'requestFactory' must not be null"); this.requestFactory = requestFactory; &#125; public ClientHttpRequestFactory getRequestFactory() &#123; return this.requestFactory; &#125; protected ClientHttpRequest createRequest(URI url, HttpMethod method) throws IOException &#123; ClientHttpRequest request = getRequestFactory().createRequest(url, method); if (logger.isDebugEnabled()) &#123; logger.debug("Created " + method.name() + " request for \"" + url + "\""); &#125; return request; &#125;&#125; doExecute中的钩子使用URL构建完成后RestTemplate调用的是一个doExecute方法,该方法对应的代码如下12345678910111213141516171819202122232425262728protected &lt;T&gt; T doExecute(URI url, HttpMethod method, RequestCallback requestCallback, ResponseExtractor&lt;T&gt; responseExtractor) throws RestClientException &#123; Assert.notNull(url, "'url' must not be null"); Assert.notNull(method, "'method' must not be null"); ClientHttpResponse response = null; try &#123; ClientHttpRequest request = createRequest(url, method); if (requestCallback != null) &#123; requestCallback.doWithRequest(request); &#125; response = request.execute(); handleResponse(url, method, response); if (responseExtractor != null) &#123; return responseExtractor.extractData(response); &#125; else &#123; return null; &#125; &#125; catch (IOException ex) &#123; throw new ResourceAccessException("I/O error on " + method.name() + " request for \"" + url + "\": " + ex.getMessage(), ex); &#125; finally &#123; if (response != null) &#123; response.close(); &#125; &#125;&#125; 这里的createRequest(url, method)方法集成自抽象类HttpAccessor.这样,在构建完成后,HTTP交互过程交给定义的ClientHttpRequest的规范实现. HTTP交互结果处理HTTP交互接口返回通过ResponseExtractor&lt;T&gt;该规范来处理.ResponseExtractor接口的定义:123public interface ResponseExtractor&lt;T&gt; &#123; T extractData(ClientHttpResponse response) throws IOException;&#125; ResponseExtractor接口默认提供的实现: 示例中我们的接口调用的返回结果实现使用的是ResponseEntityResponseExtractor&lt;T&gt; 1234567@Overridepublic &lt;T&gt; ResponseEntity&lt;T&gt; getForEntity(String url, Class&lt;T&gt; responseType, Object... urlVariables) throws RestClientException &#123; RequestCallback requestCallback = acceptHeaderRequestCallback(responseType); ResponseExtractor&lt;ResponseEntity&lt;T&gt;&gt; responseExtractor = responseEntityExtractor(responseType); return execute(url, HttpMethod.GET, requestCallback, responseExtractor, urlVariables);&#125; 具体的实现可以参看RestTemplate，不再做过多介绍. RestTemplate类关系图RestTemplate类关系图 这里RestTemplate使用的常见的Rest请求的接口都定义在RestOperations中，对应的规范定义如下: GET 规范1234567891011121314// GET&lt;T&gt; T getForObject(String url, Class&lt;T&gt; responseType, Object... uriVariables) throws RestClientException; &lt;T&gt; T getForObject(String url, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException; &lt;T&gt; T getForObject(URI url, Class&lt;T&gt; responseType) throws RestClientException; &lt;T&gt; ResponseEntity&lt;T&gt; getForEntity(String url, Class&lt;T&gt; responseType, Object... uriVariables) throws RestClientException; &lt;T&gt; ResponseEntity&lt;T&gt; getForEntity(String url, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException;&lt;T&gt; ResponseEntity&lt;T&gt; getForEntity(URI url, Class&lt;T&gt; responseType) throws RestClientException; HEAD 规范1234567// HEAD HttpHeaders headForHeaders(String url, Object... uriVariables) throws RestClientException;HttpHeaders headForHeaders(String url, Map&lt;String, ?&gt; uriVariables) throws RestClientException; HttpHeaders headForHeaders(URI url) throws RestClientException; POST 规范1234567891011121314151617181920212223// POSTURI postForLocation(String url, Object request, Object... uriVariables) throws RestClientException; URI postForLocation(String url, Object request, Map&lt;String, ?&gt; uriVariables) throws RestClientException; URI postForLocation(URI url, Object request) throws RestClientException;&lt;T&gt; T postForObject(String url, Object request, Class&lt;T&gt; responseType, Object... uriVariables) throws RestClientException; &lt;T&gt; T postForObject(String url, Object request, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException; &lt;T&gt; T postForObject(URI url, Object request, Class&lt;T&gt; responseType) throws RestClientException; &lt;T&gt; ResponseEntity&lt;T&gt; postForEntity(String url, Object request, Class&lt;T&gt; responseType, Object... uriVariables) throws RestClientException; &lt;T&gt; ResponseEntity&lt;T&gt; postForEntity(String url, Object request, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException; &lt;T&gt; ResponseEntity&lt;T&gt; postForEntity(URI url, Object request, Class&lt;T&gt; responseType) throws RestClientException; PUT 规范1234567// PUTvoid put(String url, Object request, Object... uriVariables) throws RestClientException;void put(String url, Object request, Map&lt;String, ?&gt; uriVariables) throws RestClientException; void put(URI url, Object request) throws RestClientException; DELETE 规范1234567// DELETEvoid delete(String url, Object... uriVariables) throws RestClientException; void delete(String url, Map&lt;String, ?&gt; uriVariables) throws RestClientException;void delete(URI url) throws RestClientException; OPTIONS 规范1234567// OPTIONS Set&lt;HttpMethod&gt; optionsForAllow(String url, Object... uriVariables) throws RestClientException;Set&lt;HttpMethod&gt; optionsForAllow(String url, Map&lt;String, ?&gt; uriVariables) throws RestClientException; Set&lt;HttpMethod&gt; optionsForAllow(URI url) throws RestClientException; 在提供通用REST(GET、POST、HEAD、PUT、DELETE、OPTIONS)规范操作的基础上还提供了exchange()和execute()相关的规范操作接口定义. RestTemplate异步模块AsyncRestTemplate是提供的异步操作RestTemplate的组件快.使用方式比较简单. 1234567891011121314@Testpublic void test()&#123; AsyncRestTemplate asyncRestTemplate = new AsyncRestTemplate(); String url = "http://www.baidu.com?id=&#123;id&#125;&amp;name=&#123;name&#125;&amp;num=&#123;age&#125;"; ListenableFuture&lt;ResponseEntity&lt;String&gt;&gt; listenableFuture = asyncRestTemplate.getForEntity(url, String.class, 1, "Elon", 20); try &#123; ResponseEntity&lt;String&gt; responseEntity = listenableFuture.get(); if(responseEntity.getStatusCode() == HttpStatus.OK)&#123; System.out.println(responseEntity.getBody()); &#125; &#125; catch (Exception e) &#123; LOG.error(e.getLocalizedMessage(), e); &#125;&#125; 参考文档官方文档]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>RestTemplate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[稳定性保障优雅重试]]></title>
    <url>%2F2015%2F10%2F01%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2F%E7%A8%B3%E5%AE%9A%E6%80%A7%E4%BF%9D%E9%9A%9C%E4%BC%98%E9%9B%85%E9%87%8D%E8%AF%95%2F</url>
    <content type="text"><![CDATA[业务开发中为了保证衔接模块的偶尔不确定性,需要做一些重试保障机制. 为了让我们的重试代码更优雅简单, 这里介绍两个方案:Guava-Retry和Spring-Retry Guava-Retrying Guava Retrying 是一个灵活方便的重试组件，包含了多种的重试策略，而且扩展起来非常容易. maven依赖12345&lt;dependency&gt; &lt;groupId&gt;com.github.rholder&lt;/groupId&gt; &lt;artifactId&gt;guava-retrying&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt; 应用示例示例展示目标接口在返回true时进行逻辑重试, 重试次数为3次, 重试时间间隔为每间隔2s执行一次重试. 12345678910111213141516171819202122232425262728293031public static void main(String[] args) &#123; Retryer&lt;Boolean&gt; retryer = RetryerBuilder.&lt;Boolean&gt; newBuilder() .retryIfException() .retryIfResult(Predicates.equalTo(true)) .withBlockStrategy(BlockStrategies.threadSleepStrategy()) .withStopStrategy(StopStrategies.stopAfterAttempt(3)) .withWaitStrategy(WaitStrategies.fixedWait(2, TimeUnit.SECONDS)) .build(); boolean withRetry = delayRetry(retryer); System.out.println("[RETRY-RESULT]: " + withRetry);&#125;public static boolean delayRetry(Retryer&lt;Boolean&gt; retryer)&#123; boolean result = false; try &#123; result = retryer.call(new Callable&lt;Boolean&gt;() &#123; @Override public Boolean call() throws Exception &#123; try &#123; System.out.println("[TIME-STAMP]:" + System.currentTimeMillis()); return true; &#125; catch (Exception e) &#123; throw new Exception(e); &#125; &#125; &#125;); &#125; catch (Exception e) &#123; log.error(e.getLocalizedMessage(), e); &#125; return result;&#125; 示例输出 1234[TIME-STAMP]:1521125204339[TIME-STAMP]:1521125206340[TIME-STAMP]:1521125208342[RETRY-RESULT]: false 代码解读 RetryerBuilder用于构造重试实例, 用于设置重试源(可以支持多个重试源)、重试次数、重试超时时间以及等待时间间隔等. retryIfException(): 设置异常重试源 retryIfResult(Predicates.equalTo(true)): 设置自定义段元重试源, call方法返回true重试. withStopStrategy(StopStrategies.stopAfterAttempt(3)): 设置重试3次 withWaitStrategy(WaitStrategies.fixedWait(2, TimeUnit.SECONDS)): 重试等待策略, 间隔2s重试一次. 策略说明任务阻塞策略 (BlockStrategies)通俗的讲就是当前任务执行完,下次任务还没开始这段时间做什么, 默认策略为 BlockStrategies.THREAD_SLEEP_STRATEGY 也就是调用 Thread.sleep(sleepTime). 停止重试策略 (StopStrategy) stopAfterDelay(): 设定一个最长允许的执行时间; 比如设定最长执行10s, 无论任务执行多少次, 只要重试的时候超出了最长时间, 则任务终止并返回重试异常RetryException. neverStop(): 一直重试直到成功. stopAfterAttempt(): 设定最大重试次数,超出最大重试次数则停止重试并返回重试异常. 重试间隔策略 (WaitStrategies) noWait(): 不等待策略 代码示例: 异常直接重试4次123456Retryer&lt;Boolean&gt; retryer = RetryerBuilder.&lt;Boolean&gt;newBuilder() .retryIfException().retryIfResult(Predicates.equalTo(true)) .withBlockStrategy(BlockStrategies.threadSleepStrategy()) .withStopStrategy(StopStrategies.stopAfterAttempt(4)) .withWaitStrategy(WaitStrategies.noWait()) .build(); 执行输出: 12345[TIME-STAMP]:1521196411381[TIME-STAMP]:1521196411381[TIME-STAMP]:1521196411381[TIME-STAMP]:1521196411381[RETRY-RESULT]: false exceptionWait(): 异常时长等待策略 fixedWait(): 固定等待时长策略 代码示例: 每隔2秒执行一次重试 123456Retryer&lt;Boolean&gt; retryer = RetryerBuilder.&lt;Boolean&gt;newBuilder() .retryIfException().retryIfResult(Predicates.equalTo(true)) .withBlockStrategy(BlockStrategies.threadSleepStrategy()) .withStopStrategy(StopStrategies.stopAfterAttempt(3)) .withWaitStrategy(WaitStrategies.fixedWait(2, TimeUnit.SECONDS)) .build(); 执行输出 1234[TIME-STAMP]:1521195170114[TIME-STAMP]:1521195172119[TIME-STAMP]:1521195174122[RETRY-RESULT]: false randomWait(): 随机等待时长策略(可以提供一个最小和最大时长,等待时长为其区间随机值) 代码示例: 随机间隔0~2秒重试 123456Retryer&lt;Boolean&gt; retryer = RetryerBuilder.&lt;Boolean&gt;newBuilder() .retryIfException().retryIfResult(Predicates.equalTo(true)) .withBlockStrategy(BlockStrategies.threadSleepStrategy()) .withStopStrategy(StopStrategies.stopAfterAttempt(3)) .withWaitStrategy(WaitStrategies.randomWait(2, TimeUnit.SECONDS)) .build(); 执行输出: 1234[TIME-STAMP]:1521601715654[TIME-STAMP]:1521601717496[TIME-STAMP]:1521601718763[RETRY-RESULT]: false incrementingWait(): 递增等待时长策略(提供一个初始值和步长,等待时间随重试次数增加而增加) 代码示例: 首次间隔1s,以后每次增加3s重试. 时间维度为: initialSleepTime + increment * (attemptNumber - 1) 1234567Retryer&lt;Boolean&gt; retryer = RetryerBuilder.&lt;Boolean&gt;newBuilder() .retryIfException().retryIfResult(Predicates.equalTo(true)) .withBlockStrategy(BlockStrategies.threadSleepStrategy()) .withStopStrategy(StopStrategies.stopAfterAttempt(4)) // initialSleepTime:第一次到第二次尝试的间隔, increment: 每增加一次尝试,需要增加的时间间隔 .withWaitStrategy(WaitStrategies.incrementingWait(1, TimeUnit.SECONDS, 3, TimeUnit.SECONDS)) .build(); 12345[TIME-STAMP]:1521194872168[TIME-STAMP]:1521194873172[TIME-STAMP]:1521194877173[TIME-STAMP]:1521194884175[RETRY-RESULT]: false fibonacciWait(): 斐波那契数列时长间隔策略 fibonacciWait(long multiplier,long maximumTime,TimeUnit maximumTimeUnit); multiplier单位固定是ms, maximumTime最大等待时间. 代码示例: 采用斐波那契数列时长进行重试 1234567Retryer&lt;Boolean&gt; retryer = RetryerBuilder.&lt;Boolean&gt;newBuilder() .retryIfException().retryIfResult(Predicates.equalTo(true)) .withBlockStrategy(BlockStrategies.threadSleepStrategy()) .withStopStrategy(StopStrategies.stopAfterAttempt(4)) // 斐波那契数列 .withWaitStrategy(WaitStrategies.fibonacciWait(5, TimeUnit.SECONDS)) .build(); 执行输出 12345[TIME-STAMP]:1521195661799[TIME-STAMP]:1521195661801[TIME-STAMP]:1521195661802[TIME-STAMP]:1521195661805[RETRY-RESULT]: false exponentialWait(): 按照指数递增(2的n次方)来等待, 各个参数含义与fibonacciWait相同. 代码示例 123456Retryer&lt;Boolean&gt; retryer = RetryerBuilder.&lt;Boolean&gt;newBuilder() .retryIfException().retryIfResult(Predicates.equalTo(true)) .withBlockStrategy(BlockStrategies.threadSleepStrategy()) .withStopStrategy(StopStrategies.stopAfterAttempt(4)) .withWaitStrategy(WaitStrategies.exponentialWait(100, 10, TimeUnit.SECONDS)) .build(); 执行输出 12345[TIME-STAMP]:1521196302323[TIME-STAMP]:1521196302527[TIME-STAMP]:1521196302932[TIME-STAMP]:1521196303736[RETRY-RESULT]: false Spring-Retry spring-retry非常简单,在配置类加上@EnableRetry注解启用spring-retry, 然后在需要失败重试的方法加@Retryable注解即可, spring-retry通过捕获异常来触发重试机制. Maven依赖12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.retry&lt;/groupId&gt; &lt;artifactId&gt;spring-retry&lt;/artifactId&gt; &lt;version&gt;1.2.2.RELEASE&lt;/version&gt;&lt;/dependency&gt; 应用示例硬编码方式示例代码 12345678910111213141516171819202122232425262728293031323334353637383940414243@Testpublic void springRetry() throws Exception &#123; // 构建重试模板实例 RetryTemplate retryTemplate = new RetryTemplate(); // 设置重试策略 retryTemplate.setRetryPolicy(new SimpleRetryPolicy(3, Collections.&lt;Class&lt;? extends Throwable&gt;, Boolean&gt; singletonMap(Exception.class, true))); // 设置退避策略 FixedBackOffPolicy fixedBackOffPolicy = new FixedBackOffPolicy(); fixedBackOffPolicy.setBackOffPeriod(100); retryTemplate.setBackOffPolicy(fixedBackOffPolicy); boolean withRetry = retryTemplate.execute( // 重试行为 new RetryCallback&lt;Boolean, Exception&gt;() &#123; @Override public Boolean doWithRetry(RetryContext retryContext) throws Exception &#123; System.out.println("[TIME-STAMP]:" + System.currentTimeMillis() + ", retry:" + retryContext.getRetryCount()); return sample(5); &#125; &#125;, // 多次重试无效后执行逻辑 new RecoveryCallback&lt;Boolean&gt;() &#123; @Override public Boolean recover(RetryContext retryContext) throws Exception &#123; System.out.println("[TIME-STAMP]:" + System.currentTimeMillis() + ", recover:" + retryContext.getRetryCount()); return false; &#125; &#125; ); System.out.println("[RETRY-RESULT]: " + withRetry);&#125;private boolean sample(int id) throws Exception &#123; if(id &lt; 10)&#123; throw new RuntimeException(String.valueOf(id)); &#125; return true;&#125; 程序输出 12345[TIME-STAMP]:1521207239963, retry:0[TIME-STAMP]:1521207240065, retry:1[TIME-STAMP]:1521207240166, retry:2[TIME-STAMP]:1521207240166, recover:3[RETRY-RESULT]: false 上面示例中我们的sample()方法入参为5时输出结果如上图, 当参数设置大于10时输出结果如下. 12[TIME-STAMP]:1521207481718, retry:0[RETRY-RESULT]: true 重试策略 NeverRetryPolicy: 执行一次待执行操作,如果出现异常不进行重试. AlwaysRetryPolicy: 异常后一直重试直到成功. SimpleRetryPolicy: 对指定的异常进行若干次重试,默认情况下对Exception异常及其子类重试3次(默认策略). CircuitBreakerRetryPolicy: 有个内部类CircuitBreakerRetryContext, 断路器重试上下文。提供过载保护的策略, 如果在时间间隔openTimeout内，直接短路，不允许重试，只有超过间隔的才能重试. CompositeRetryPolicy: 用户指定一组策略,随后根据optimistic选项来确认如何重试. ExceptionClassifierRetryPolicy: 根据产生的异常选择重试策略 ExpressionRetryPolicy: 扩展自SimpleRetryPolicy, 在父类canRetry的基础上加上对lastThrowable的的表达式判断,符合特地表达式的异常才能重试. TimeoutRetryPolicy: 在执行execute方法时从open操作开始到调用TimeoutRetryPolicy的canRetry方法这之间所经过的时间,这段时间未超过TimeoutRetryPolicy定义的超时时间,那么执行操作,否则抛出异常. 退避策略 NoBackOffPolicy: 实现了空方法,因此采用次策略,重试不会等待。这也是RetryTemplate采用的默认退避(backOff)策略 FixedBackOffPolicy: 在等待一段固定的时间后再进行重试(默认为1秒). UniformRandomBackOffPolicy: 均匀随机退避策略,等待时间为:最小退避时间 + [0,最大退避时间 - 最小退避时间)间的一个随机数,如果最大退避时间等于最小退避时间那么等待时间为0 ExponentialBackOffPolicy: 指数退避策略,每次等待时间为:等待时间 = 等待时间 * N ，即每次等待时间为上一次的N倍。如果等待时间超过最大等待时间，那么以后的等待时间为最大等待时间 ExponentialRandomBackOffPolicy: 指数随机策略 如果每次有重试需求的时候都写一个RetryTemplate太臃肿了,SpringRetry也提供了使用注解方式进行重试操作. 注解方式应用示例示例代码 123456789101112131415161718192021@Slf4j@Service@EnableRetrypublic class SpringRetry &#123; @Retryable(value = &#123;Exception.class&#125;, maxAttempts = 3, backoff = @Backoff(delay = 2000, multiplier = 1.5)) public String withRetry(long id)&#123; System.out.println("[TIME-STAMP]:" + System.currentTimeMillis() + ", id=" + id); if(id &lt; 10)&#123; throw new IllegalArgumentException(String.valueOf(id)); &#125; return String.valueOf(id); &#125; @Recover public String withRecover(Exception exception, long id)&#123; System.out.println("[TIME-STAMP]:" + System.currentTimeMillis() + ", id=" + id + ", withRecover"); return StringUtils.EMPTY; &#125;&#125; 执行结果 123456[TIME-STAMP]:1521207829018, id=11[TIME-STAMP]:1521207829018, id=8[TIME-STAMP]:1521207831023, id=8[TIME-STAMP]:1521207834027, id=8[TIME-STAMP]:1521207834028, id=8, withRecover 注解说明 @EnableRetry: 在需要执行重试的类上使用@EnableRetry,如果设置了proxyTargetClass=true(默认值为false)表示使用CGLIB动态代理 @Retryable: 注解需要被重试的方法 value: 指定要重试的异常(默认为空). include: 指定处理的异常类(默认为空). exclude: 指定不需要处理的异常(默认为空). maxAttempts: 最大重试次数(默认3次) backoff: 重试等待策略(默认使用@Backoff注解) @Backoff：重试回退策略(立即重试还是等待一会再重试),不设置参数时默认使用FixedBackOffPolicy,重试等待1000ms; 只设置delay()属性时,使用FixedBackOffPolicy,重试等待指定的毫秒数; 当设置delay()和maxDealy()属性时,重试等待在这两个值之间均态分布; @Recover: 用于方法上,用于@Retryable失败时的”兜底”处理方法, @Recover注释的方法第一入参为要重试的异常,其他参数与@Retryable保持一致,返回值也要一样,否则无法执行！ @CircuitBreaker：用于方法,实现熔断模式. include 指定处理的异常类。默认为空 exclude指定不需要处理的异常。默认为空 value指定要重试的异常。默认为空 maxAttempts 最大重试次数。默认3次 openTimeout 配置熔断器打开的超时时间,默认5s,当超过openTimeout之后熔断器电路变成半打开状态（只要有一次重试成功,则闭合电路） resetTimeout 配置熔断器重新闭合的超时时间,默认20s,超过这个时间断路器关闭 参考文档 http://blog.csdn.net/u011116672/article/details/77823867 https://www.jianshu.com/p/96a5003c470c]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Retry</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringEvent事件編程]]></title>
    <url>%2F2015%2F09%2F11%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2FSpringEvent%20%E4%BA%8B%E4%BB%B6%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[SpringEvent 事件驱动介绍 Spring中的事件驱动模型也叫作发布订阅模式,是观察者模式的一个典型的应用Spring事件驱动模型中存在三个角色: 事件原型、事件发布者、事件监听者. Spring事件原型 Spring事件定义通过ApplicationEvent,该类继承自Jdk的EventObject; JDK要求所有事件将继承它,并通过source得到事件源, 比如我们的AWT事件体系也是继承自它; ApplicationEvent规范定义如下: 123456789101112131415161718192021222324public abstract class ApplicationEvent extends EventObject &#123; /** use serialVersionUID from Spring 1.2 for interoperability */ private static final long serialVersionUID = 7099057708183571937L; /** System time when the event happened */ private final long timestamp; /** * Create a new ApplicationEvent. * @param source the object on which the event initially occurred (never &#123;@code null&#125;) */ public ApplicationEvent(Object source) &#123; super(source); this.timestamp = System.currentTimeMillis(); &#125; /** * Return the system time in milliseconds when the event happened. */ public final long getTimestamp() &#123; return this.timestamp; &#125;&#125; Spring事件发布者 Spring事件发布规范定义在ApplicationEventPublisher中,主要用于事件发布者发布中事件; ApplicationEventPublisher规范定义如下: 12345678910111213141516171819202122public interface ApplicationEventPublisher &#123; /** * Notify all &lt;strong&gt;matching&lt;/strong&gt; listeners registered with this * application of an application event. Events may be framework events * (such as RequestHandledEvent) or application-specific events. * @param event the event to publish * @see org.springframework.web.context.support.RequestHandledEvent */ void publishEvent(ApplicationEvent event); /** * Notify all &lt;strong&gt;matching&lt;/strong&gt; listeners registered with this * application of an event. * &lt;p&gt;If the specified &#123;@code event&#125; is not an &#123;@link ApplicationEvent&#125;, * it is wrapped in a &#123;@link PayloadApplicationEvent&#125;. * @param event the event to publish * @since 4.2 * @see PayloadApplicationEvent */ void publishEvent(Object event);&#125; ApplicationContext接口就集成了ApplicationEventPublisher,如图: 对于ApplicationContext, Spring提供了默认的实现, 在抽象类AbstractApplicationContext中, 常见的AnnotationConfigWebApplicationContext、ClassPathXmlApplicationContext、FileSystemXmlApplicationContext都有继承AbstractApplicationContext的事件行为: AbstractApplicationContext类中实现代码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * Publish the given event to all listeners. * @param event the event to publish (may be an &#123;@link ApplicationEvent&#125; * or a payload object to be turned into a &#123;@link PayloadApplicationEvent&#125;) * @param eventType the resolved event type, if known * @since 4.2 */protected void publishEvent(Object event, ResolvableType eventType) &#123; Assert.notNull(event, "Event must not be null"); if (logger.isTraceEnabled()) &#123; logger.trace("Publishing event in " + getDisplayName() + ": " + event); &#125; // Decorate event as an ApplicationEvent if necessary ApplicationEvent applicationEvent; if (event instanceof ApplicationEvent) &#123; applicationEvent = (ApplicationEvent) event; &#125; else &#123; applicationEvent = new PayloadApplicationEvent&lt;Object&gt;(this, event); if (eventType == null) &#123; eventType = ((PayloadApplicationEvent)applicationEvent).getResolvableType(); &#125; &#125; // Multicast right now if possible - or lazily once the multicaster is initialized if (this.earlyApplicationEvents != null) &#123; this.earlyApplicationEvents.add(applicationEvent); &#125; else &#123; getApplicationEventMulticaster().multicastEvent(applicationEvent, eventType); &#125; // Publish event via parent context as well... if (this.parent != null) &#123; if (this.parent instanceof AbstractApplicationContext) &#123; ((AbstractApplicationContext) this.parent).publishEvent(event, eventType); &#125; else &#123; this.parent.publishEvent(event); &#125; &#125;&#125; 在这个方法中,我们看到了一getApplicationEventMulticaster();ApplicationEventMulticaster.属于事件广播器,它的作用是把ApplicationContext发布的Event广播给所有的监听器. Spring事件监听者Spring中ApplicationListener是事件监控者规范的定义, 定义内容如下: 1234567public interface ApplicationListener&lt;E extends ApplicationEvent&gt; extends EventListener &#123; /** * Handle an application event. * @param event the event to respond to */ void onApplicationEvent(E event);&#125; ApplicationListener继承自jdk的EventListener,所有的监听器都要实现这个接口,这个接口只有一个onApplicationEvent()方法,该方法接受一个ApplicationEvent或其子类对象作为参数,在方法体中,可以通过不同对Event类的判断来进行相应的处理.当事件触发时所有的监听器都会收到消息,如果你需要对监听器的接收顺序有要求,可以实现该接口的一个子接口SmartApplicationListener,通过这个接口可以指定监听器接收事件的顺序. SmartApplicationListener的接口集成关系如下: SpringEvent 事件应用示例事件定义123456public class PlanAllocatePubEvent extends ApplicationEvent &#123; public PlanAllocatePubEvent(Long allocateId) &#123; super(allocateId); &#125;&#125; 事件发布1234567891011121314@Componentpublic class PlanAllocatePubPublisher &#123; private static final Logger LOG = LoggerFactory.getLogger(PlanAllocatePubPublisher.class); @Autowired private ApplicationEventPublisher applicationEventPublisher; public void publishEvent(long allocateId)&#123; LOG.info("[事件发布] 数据创建, 编号:" + allocateId); PlanAllocatePubEvent event = new PlanAllocatePubEvent(allocateId); applicationEventPublisher.publishEvent(event); &#125;&#125; 事件监听(无序)同步事件处理 123456789101112131415161718192021@Componentpublic class SyncPlanAllocatePubListener implements ApplicationListener&lt;PlanAllocatePubEvent&gt;&#123; private static final Logger LOG = LoggerFactory.getLogger(SyncPlanAllocatePubListener.class); @Override public void onApplicationEvent(PlanAllocatePubEvent planAllocatePubEvent) &#123; LOG.info("[同步][事件监听][开始]数据创建, 数据编号:" + planAllocatePubEvent.getSource()); doSomething(); LOG.info("[同步][事件监听][结束]数据创建, 数据编号:" + planAllocatePubEvent.getSource()); &#125; public void doSomething()&#123; try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; LOG.error(e.getLocalizedMessage(), e); &#125; &#125;&#125; 异步事件处理 1234567891011121314151617181920212223@Component@EnableAsyncpublic class AsynPlanAllocatePubListener implements ApplicationListener&lt;PlanAllocatePubEvent&gt; &#123; private static final Logger LOG = LoggerFactory.getLogger(AsynPlanAllocatePubListener.class); @Async @Override public void onApplicationEvent(PlanAllocatePubEvent planAllocatePubEvent) &#123; LOG.info("[异步][事件监听][开始]数据创建, 数据编号:" + planAllocatePubEvent.getSource()); doSomething(); LOG.info("[异步][事件监听][结束]数据创建, 数据编号:" + planAllocatePubEvent.getSource()); &#125; public void doSomething()&#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; LOG.error(e.getLocalizedMessage(), e); &#125; &#125;&#125; 事件监听(有序)示例代码: 事件监听逻辑1 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Componentpublic class Order1PlanAllocatePubListener implements SmartApplicationListener &#123; private static final Logger LOG = LoggerFactory.getLogger(Order1PlanAllocatePubListener.class); /** * supportsEventType用于指定支持的事件类型，只有支持的才调用onApplicationEvent * @param eventType * @return */ @Override public boolean supportsEventType(Class&lt;? extends ApplicationEvent&gt; eventType) &#123; return eventType == PlanAllocatePubEvent.class; &#125; /** * supportsSourceType支持的目标类型，只有支持的才调用onApplicationEvent * @param sourceType * @return */ @Override public boolean supportsSourceType(Class&lt;?&gt; sourceType) &#123; return sourceType == Long.class; &#125; @Override public void onApplicationEvent(ApplicationEvent event) &#123; LOG.info("[有序事件][事件监听][Order:1]数据创建, 编号:" + event.getSource()); doSomething(); &#125; /** * 优先级顺序，越小优先级越高 * @return */ @Override public int getOrder() &#123; return 1; &#125; public void doSomething()&#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; LOG.error(e.getLocalizedMessage(), e); &#125; &#125;&#125; 示例代码: 事件监听逻辑2 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Componentpublic class Order2PlanAllocatePubListener implements SmartApplicationListener&#123; private static final Logger LOG = LoggerFactory.getLogger(Order2PlanAllocatePubListener.class); /** * supportsEventType用于指定支持的事件类型，只有支持的才调用onApplicationEvent * @param eventType * @return */ @Override public boolean supportsEventType(Class&lt;? extends ApplicationEvent&gt; eventType) &#123; return eventType == PlanAllocatePubEvent.class; &#125; /** * supportsSourceType支持的目标类型，只有支持的才调用onApplicationEvent * @param sourceType * @return */ @Override public boolean supportsSourceType(Class&lt;?&gt; sourceType) &#123; return sourceType == Long.class; &#125; @Override public void onApplicationEvent(ApplicationEvent event) &#123; LOG.info("[有序事件][事件监听][Order:2]数据创建, 编号:" + event.getSource()); doSomething(); &#125; /** * 优先级顺序，越小优先级越高 * @return */ @Override public int getOrder() &#123; return 2; &#125; public void doSomething()&#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; LOG.error(e.getLocalizedMessage(), e); &#125; &#125;&#125; 测试事件1234567891011121314151617@Controller@RequestMapping(value = "/api/event")public class ApiPluginEventController &#123; @Autowired private PlanAllocatePubPublisher planAllocatePubPublisher; @ResponseBody @GetMapping(value = "/publish") public Map pubEvent() throws Exception &#123; for(int i=1; i&lt;5; i++)&#123; planAllocatePubPublisher.publishEvent(i); &#125; return ImmutableMap.builder().put("message", "ok").build(); &#125;&#125; 异步事件执行结果123456789101112INFO:2017-09-11 20:14:08.799[publishEvent] [事件发布] 数据创建, 数据编号:1 INFO:2017-09-11 20:14:08.803[publishEvent] [事件发布] 数据创建, 数据编号:2 INFO:2017-09-11 20:14:08.803[onApplicationEvent] [异步][事件监听][开始]数据创建, 数据编号:1 INFO:2017-09-11 20:14:08.804[publishEvent] [事件发布] 数据创建, 数据编号:3 INFO:2017-09-11 20:14:08.804[onApplicationEvent] [异步][事件监听][开始]数据创建, 数据编号:2 INFO:2017-09-11 20:14:08.805[onApplicationEvent] [异步][事件监听][开始]数据创建, 数据编号:3 INFO:2017-09-11 20:14:08.804[publishEvent] [事件发布] 数据创建, 数据编号4 INFO:2017-09-11 20:14:08.805[onApplicationEvent] [异步][事件监听][开始]数据创建, 数据编号:4 INFO:2017-09-11 20:14:11.807[onApplicationEvent] [异步][事件监听][结束]数据创建, 数据编号:1 INFO:2017-09-11 20:14:11.807[onApplicationEvent] [异步][事件监听][结束]数据创建, 数据编号:2 INFO:2017-09-11 20:14:11.807[onApplicationEvent] [异步][事件监听][结束]数据创建, 数据编号:3 INFO:2017-09-11 20:14:11.809[onApplicationEvent] [异步][事件监听][结束]数据创建, 数据编号:4 同步事件执行结果123456789101112INFO:2017-09-11 20:17:07.470[publishEvent] [事件发布] 数据创建, 数据编号:1 INFO:2017-09-11 20:17:07.473[onApplicationEvent] [同步][事件监听][开始]数据创建, 数据编号:1 INFO:2017-09-11 20:17:09.476[onApplicationEvent] [同步][事件监听][结束]数据创建, 数据编号:1 INFO:2017-09-11 20:17:09.477[publishEvent] [事件发布] 数据创建, 数据编号:2 INFO:2017-09-11 20:17:09.478[onApplicationEvent] [同步][事件监听][开始]数据创建, 数据编号:2 INFO:2017-09-11 20:17:11.481[onApplicationEvent] [同步][事件监听][结束]数据创建, 数据编号:2 INFO:2017-09-11 20:17:11.481[publishEvent] [事件发布] 数据创建, 数据编号:3 INFO:2017-09-11 20:17:11.482[onApplicationEvent] [同步][事件监听][开始]数据创建, 数据编号:3 INFO:2017-09-11 20:17:13.486[onApplicationEvent] [同步][事件监听][结束]数据创建, 数据编号:3 INFO:2017-09-11 20:17:13.487[publishEvent] [事件发布] 数据创建, 数据编号:4 INFO:2017-09-11 20:17:13.487[onApplicationEvent] [同步][事件监听][开始]数据创建, 数据编号:4 INFO:2017-09-11 20:17:15.492[onApplicationEvent] [同步][事件监听][结束]数据创建, 数据编号:4 有序事件执行结果123456789101112INFO:2017-09-11 20:33:12.139[publishEvent] [事件发布] 数据创建, 编号:1 INFO:2017-09-11 20:33:12.140[onApplicationEvent] [有序事件][事件监听][Order:1]数据创建, 编号:1 INFO:2017-09-11 20:33:15.145[onApplicationEvent] [有序事件][事件监听][Order:2]数据创建, 编号:1 INFO:2017-09-11 20:33:18.146[publishEvent] [事件发布] 数据创建, 编号:2 INFO:2017-09-11 20:33:18.147[onApplicationEvent] [有序事件][事件监听][Order:1]数据创建, 编号:2 INFO:2017-09-11 20:33:21.151[onApplicationEvent] [有序事件][事件监听][Order:2]数据创建, 编号:2 INFO:2017-09-11 20:33:24.156[publishEvent] [事件发布] 数据创建, 编号:3 INFO:2017-09-11 20:33:24.157[onApplicationEvent] [有序事件][事件监听][Order:1]数据创建, 编号:3 INFO:2017-09-11 20:33:27.162[onApplicationEvent] [有序事件][事件监听][Order:2]数据创建, 编号:3 INFO:2017-09-11 20:33:30.167[publishEvent] [事件发布] 数据创建, 编号:4 INFO:2017-09-11 20:33:30.168[onApplicationEvent] [有序事件][事件监听][Order:1]数据创建, 编号:4 INFO:2017-09-11 20:33:33.171[onApplicationEvent] [有序事件][事件监听][Order:2]数据创建, 编号:4 注解事件监听Spring4.2开始提供了@EventListener注解，使得监听器不再需要实现ApplicationListener接口，只需要在监听方法上加上该注解即可，方法不一定叫onApplicationEvent，但有且只能有一个参数，指定监听的事件类型.上面示例中的有序事件监听逻辑可以使用注解方式实现: 1234567891011121314151617181920212223242526272829303132333435@Componentpublic class PlanAllocatePubHandler &#123; private static final Logger LOG = LoggerFactory.getLogger(PlanAllocatePubHandler.class); /** * 基于注解的事件监听 * @param planAllocatePubEvent */ @EventListener @Order(1) public void annoEnvListener1(PlanAllocatePubEvent planAllocatePubEvent) &#123; LOG.info("[注解事件监听][Order:1]数据创建, 编号:" + planAllocatePubEvent.getSource()); doSomething(); &#125; /** * 基于注解的事件监听 * @param planAllocatePubEvent */ @EventListener @Order(2) public void annoEnvListener2(PlanAllocatePubEvent planAllocatePubEvent) &#123; LOG.info("[注解事件监听][Order:2]数据创建, 编号:" + planAllocatePubEvent.getSource()); doSomething(); &#125; public void doSomething()&#123; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; LOG.error(e.getLocalizedMessage(), e); &#125; &#125;&#125; 注解事件监听执行结果123456789101112INFO:2017-09-11 20:57:00.167[publishEvent] [事件发布] 数据创建, 编号:1 INFO:2017-09-11 20:57:00.168[annoEnvListener1] [注解事件监听][Order:1]数据创建, 编号:1 INFO:2017-09-11 20:57:03.169[annoEnvListener2] [注解事件监听][Order:2]数据创建, 编号:1 INFO:2017-09-11 20:57:06.175[publishEvent] [事件发布] 数据创建, 编号:2 INFO:2017-09-11 20:57:06.176[annoEnvListener1] [注解事件监听][Order:1]数据创建, 编号:2 INFO:2017-09-11 20:57:09.180[annoEnvListener2] [注解事件监听][Order:2]数据创建, 编号:2 INFO:2017-09-11 20:57:12.184[publishEvent] [事件发布] 数据创建, 编号:3 INFO:2017-09-11 20:57:12.185[annoEnvListener1] [注解事件监听][Order:1]数据创建, 编号:3 INFO:2017-09-11 20:57:15.191[annoEnvListener2] [注解事件监听][Order:2]数据创建, 编号:3 INFO:2017-09-11 20:57:18.195[publishEvent] [事件发布] 数据创建, 编号:4 INFO:2017-09-11 20:57:18.196[annoEnvListener1] [注解事件监听][Order:1]数据创建, 编号:4 INFO:2017-09-11 20:57:21.198[annoEnvListener2] [注解事件监听][Order:2]数据创建, 编号:4]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>Event</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析工具 Zeppelin]]></title>
    <url>%2F2015%2F09%2F11%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%20Zeppelin%2F</url>
    <content type="text"><![CDATA[Zeppelin概述 Zeppelin下载 Zeppelin部署 Zeppelin解释器 安装特定解释器 安装全部解释器 Zeppelin启动 Zeppelin应用 解释器Markdown 解释器Elastic Zeppelin概述Apache Zeppelin 是一个让交互式数据分析变得可行的基于网页的notebook。Zeppelin提供了数据可视化的框架. 官网地址: http://zeppelin.apache.org/ Zeppelin下载1234567891011121314151617181920212223[root@icloud-store local]# wget http://mirror.bit.edu.cn/apache/zeppelin/zeppelin-0.7.3/zeppelin-0.7.3-bin-all.tgz[root@icloud-store local]# tar -zxvf zeppelin-0.7.3-bin-all.tgz [root@icloud-store zeppelin-0.7.3-bin-all]# ll -h总用量 28Mdrwxr-xr-x 2 501 wheel 4.0K 2月 26 03:29 bindrwxr-xr-x 2 501 wheel 4.0K 2月 26 03:29 confdrwxr-xr-x 23 501 wheel 4.0K 2月 26 03:28 interpreterdrwxr-xr-x 4 501 wheel 8.0K 2月 26 03:28 lib-rw-r--r-- 1 501 wheel 59K 9月 18 10:14 LICENSEdrwxr-xr-x 2 501 wheel 4.0K 2月 26 03:28 licensesdrwxr-xr-x 8 501 wheel 102 2月 26 03:28 notebook-rw-r--r-- 1 501 wheel 5.5K 9月 18 10:14 NOTICE-rw-r--r-- 1 501 wheel 1.3K 9月 18 10:07 README.md-rw-r--r-- 1 501 wheel 28M 9月 18 10:13 zeppelin-web-0.7.3.war[root@icloud-store zeppelin-0.7.3-bin-all]# ls bin/common.cmd functions.cmd install-interpreter.sh interpreter.sh zeppelin-daemon.shcommon.sh functions.sh interpreter.cmd zeppelin.cmd zeppelin.sh[root@icloud-store zeppelin-0.7.3-bin-all]# ls conf/configuration.xsl log4j.properties zeppelin-env.cmd.template zeppelin-site.xml.templateinterpreter-list shiro.ini.template zeppelin-env.sh.template[root@icloud-store zeppelin-0.7.3-bin-all]# ls interpreter/alluxio bqsql elasticsearch flink ignite kylin lib md psql scio sparkangular cassandra file hbase jdbc lens livy pig python sh Zeppelin部署Zeppelin部署前提是安装了JDK(建议JDK8). 安装过程比较简单,部署其应用war包即可. Zeppelin解释器查看其支持的解释器,内置解释器列表. 1234567891011121314151617181920212223242526272829303132[root@icloud-store zeppelin-0.7.3-bin-all]# bin/install-interpreter.sh --listJava HotSpot(TM) 64-Bit Server VM warning: Ignoring option MaxPermSize; support was removed in 8.0SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/zeppelin-0.7.3-bin-all/lib/interpreter/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/zeppelin-0.7.3-bin-all/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]alluxio Alluxio interpreterangular HTML and AngularJS view renderingbeam Beam interpreterbigquery BigQuery interpretercassandra Cassandra interpreter built with Scala 2.11elasticsearch Elasticsearch interpreterfile HDFS file interpreterflink Flink interpreter built with Scala 2.11hbase Hbase interpreterignite Ignite interpreter built with Scala 2.11jdbc Jdbc interpreterkylin Kylin interpreterlens Lens interpreterlivy Livy interpretermd Markdown supportpig Pig interpreterpostgresql Postgresql interpreterpython Python interpreterscio Scio interpretershell Shell command[root@icloud-store zeppelin-0.7.3-bin-all]# cd conf/[root@icloud-store conf]# [root@icloud-store conf]# cat interpreter-list [root@icloud-store conf]# cat zeppelin-site.xml -n | grep -C 3 interpreter 安装特定解释器1[root@icloud-store zeppelin-0.7.3-bin-all]# bin/install-interpreter.sh --name md,shell,jdbc,python 安装全部解释器12[root@localhost zeppelin-0.7.3-bin-all]# bin/install-interpreter.sh --all Zeppelin启动12345[root@icloud-store zeppelin-0.7.3-bin-all]# cp conf/zeppelin-env.sh.template conf/zeppelin-env.sh[root@icloud-store zeppelin-0.7.3-bin-all]# cp conf/zeppelin-site.xml.template conf/zeppelin-site.xml[root@icloud-store zeppelin-0.7.3-bin-all]# bin/zeppelin-daemon.sh startPid dir doesn't exist, create /usr/local/zeppelin-0.7.3-bin-all/runZeppelin start [ OK ] 效果图如下 Zeppelin应用Zeppelin基于其特定的解释器,可以做一些简单的数据可视化图标展示, 下面概述如何基于特定解释器创建可视化数据图表. 解释器Markdown步骤: 【Notebook】-&gt;【Create new note】, 解释器选择md即可会出现一个展示面板,编辑运行即可. 效果如下: 解释器Elastic解释器选择: elasticsearch. 当然需要在Interpreter中事先配置好解释器 当然也支持不同的图标效果展示 整体来说比较简单, 不做赘述.]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>Zeppelin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux流量复制工具]]></title>
    <url>%2F2015%2F09%2F11%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2FLinux%E6%B5%81%E9%87%8F%E5%A4%8D%E5%88%B6%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[流量复制线下的测试难以模拟真实流量, 尤其难以模拟正常流量混杂着各色异常流量;所以复制线上流量进行测试，能够覆盖很多无法预见的异常流量. 流量复制工具有很多, 例如Gor、tcpreplay、tcpcopy等; 这些工具贴合真实场景,能模拟真实流量, 并支持流量的放大或缩小，更容易测试出程序的瓶颈和潜在问题. goreplay: https://github.com/buger/goreplay tcpreplay: https://github.com/appneta/tcpreplay tcpcopy: https://github.com/session-replay-tools/tcpcopy nginx模块: ngx_http_mirror_module(在Nginx 1.13.4中开始引入) Nginx插件ngx_http_mirror_module 模块可以复制原始请求(镜像)通过内部跳转到另一个location. 配置如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445server &#123; listen 8080; access_log /home/work/log/nginx/org.log; root html/org;&#125;server &#123; listen 8081; access_log /home/work/log/nginx/mir.log ; root html/mir;&#125;upstream backend &#123; server 127.0.0.1:8080;&#125;upstream test_backend &#123; server 127.0.0.1:8081;&#125;server &#123; listen 80; server_name localhost; # original 配置 location / &#123; # mirror指定镜像uri为 /mirror mirror /mirror; # off|on 指定是否镜像请求body部分(开启为on，则请求自动缓存;) mirror_request_body off; # 指定上游server的地址 proxy_pass http://backend; &#125; # mirror 配置 location /mirror &#123; # 指定此location只能被“内部的”请求调用 internal; # 指定上游server的地址 proxy_pass http://test_backend$request_uri; # 设置镜像流量的头部 proxy_set_header X-Original-URI $request_uri; &#125;&#125; 流量放大, 配置两个mirror即可. 12345location / &#123; mirror /mirror; mirror /mirror; proxy_pass http://backend;&#125; 使用是很方便，但是线上nginx一般都承载了不止一个业务，修改nginx配置后需要nginx -s reload来使之生效，这种操作在线上还是尽量需要避免的. GoreplayGoreplay是用Golang写的一个 HTTP 实时流量复制工具。功能更强大，支持流量的放大、缩小，频率限制，还支持把请求记录到文件，方便回放和分析，也支持和 ElasticSearch 集成，将流量存入 ES 进行实时分析。 https://github.com/buger/goreplay 安装部署12&gt; wget https://github.com/buger/goreplay/releases/download/v0.16.1/gor_0.16.1_x64.tar.gz&gt; tar xzvf gor_0.16.1_x64.tar.gz 流量复制可以将流量复制到文件，然后再对他们进行回放。回放的时候，流量会维持原始的时间间隔。如果你使用了百分比来进行速率限制，那么回放的速率会相应的增加或减少。有了这种速率限制，gor就可以用来进行压力测试. 12345# write to filegor --input-raw :80 --output-file requests_origin.gor# read from filegor --input-file requests_origin.gor --output-http "http://localhost:8081" 可以使用时间戳命名录制文件，默认情况下,文件是按“块”存储的, 即文件大小到达上限后, 添加后缀并新建另一个文件, 示例如下: 1234567gor --input-raw :80 --output-file %Y%m%d.gor# append false20140608_0.gor20140608_1.gor20140609_0.gor20140609_1.gor 默认是按”块”存储文件的方式,但是可以参数配置–output-file-append,效果如下 12345gor --input-raw :80 --output-file %Y%m%d.gor --output-file-append# append true20140608.gor20140609.gor 时间格式化文件名的配置说明： %Y: year including the century (at least 4 digits) %m: month of the year (01..12) %d: Day of the month (01..31) %H: Hour of the day, 24-hour clock (00..23) %M: Minute of the hour (00..59) %S: Second of the minute (00..60) 默认格式是 %Y%m%d%H 流量回放目前这种方式只支持”input-file”, 而且只能用百分比去控制回放速率; 这个回放的速率比例是相对于input的, 即按照录下来的流量的时间戳去进行回放. 以2倍速率回放 1gor --input-file "requests_origin.gor|200%" --output-http "http://localhost:8081" 如果”input-flie”是多个文件,可以用正则去匹配 1gor --input-file "requests_origin*.gor|200%" --output-http "http://localhost:8081" 配合如下配置参数，可以更好进行压力测试: --input-file-loop: 重复循环执行input-file --exit-after 30s: 在30s后停止, 可以控制压力测试的时间, 分钟的单位是m. 常用命令简单的HTTP流量复制 1&gt; gor --input-raw :80 --output-http "http://localhost:8081" HTTP流量复制频率控制(获取每秒超过10个请求) 1&gt; gor --input-tcp :28020 --output-http "http://localhost:8081|10" HTTP流量复制缩小 1&gt; gor --input-raw :80 --output-tcp "http://localhost:8081|10%" HTTP流量记录到本地文件 1&gt; gor --input-raw :80 --output-file requests_origin.gor HTTP流量回放和压测 1&gt; gor --input-file "requests_origin.gor|200%" --output-http "http://localhost:8081" HTTP流量过滤复制 1&gt; gor --input-raw :8080 --output-http http://localhost:8081 --output-http-url-regexp ^www. 自定义一些流量复制的参数 1&gt; gor --input-raw :80 --output-http http://localhost:8081 --http-allow-method POST --http-set-header 'User-Agent: Gor' -output-http-workers=1 -http-allow-url test.php 将流量复制两份到不同的测试服务 1&gt; gor --input-tcp :8080 --output-http "http://localhost:8081" --output-http "http://localhost:8082" 将流量像负载均衡一样分配到不同的服务器 1&gt; gor --input-tcp :8080 --output-http "http://localhost:8081" --output-http "http://localhost:8082" --split-output true 配置参数1234567891011121314151617181920212223242526272829&gt; gor --help-http-allow-header valuegor --input-raw :8080 --output-http localhost:8081 --http-allow-header api-version:v1.1用一个正则表达式来匹配http头部，如果请求的头部没有匹配上，则被拒绝-http-allow-method valuegor --input-raw :8080 --output-http localhost:8081 --http-allow-method GET类似于一个白名单机制来允许通过的http请求方法，除此之外的方法都被拒绝.-http-allow-url valuegor --input-raw :8080 --output-http localhost:8081 --http-allow-url ^www一个正则表达式用来匹配url， 用来过滤完全匹配的的url，在此之外的都被过滤掉-http-disallow-header valuegor --input-raw :8080 --output-http localhost:8081 --http-disallow-header "User-Agent: Replayed by Gor"用一个正则表达式来匹配http头部，匹配到的请求会被拒绝掉-http-disallow-url valuegor --input-raw :8080 --output-http localhost:8081 --http-disallow-url ^www用一个正则表达式来匹配url，如果请求匹配上了，则会被拒绝-http-set-header valuegor --input-raw :8080 --output-http localhost:8081 --http-set-header 'User-Agent: Gor'设置头信息，如果已经存在会覆盖-http-set-param valuegor --input-raw :8080 --output-http localhost:8081 --http-set-param api_key=v1.1设置请求参数，如果已经存在会覆盖 更多参数请查阅官方文档 https://github.com/buger/goreplay/wiki TcpCopyTcpcopy是一种请求复制工具。可以将线上流量拷贝到测试机器，实时的模拟线上环境。在不影响线上用户的情况下，使用线上流量进行测试，以尽早发现bug。也可以通过放大流量，进行压力测试，评估系统承载能力. http://blog.gaoyuan.xyz/2014/01/08/use-tcpcopy-test-online/ https://winway.github.io/2017/10/17/tcpcopy-introduce/ 参考文档 http://nginx.org/en/docs/http/ngx_http_mirror_module.html http://www.jouypub.com/2018/cebe93bd026cbde6c4a6ec2020d44b1b/]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Goreplay</tag>
        <tag>Tcpcopy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[缓存组件-Ehcache]]></title>
    <url>%2F2015%2F09%2F01%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2F%E7%BC%93%E5%AD%98%E7%BB%84%E4%BB%B6-Ehcache%2F</url>
    <content type="text"><![CDATA[Ehcache概述Ehcache是一个用Java实现的使用简单、高速、线程安全的缓存管理类库, 其提供了用内存、磁盘文件寸纯、以及分布式存储等多种灵活的管理方案. Ehcache从Hibernate发展而来, 快速、简单、低消耗、依赖性小、扩展性强. Ehcache特点 快速、简单； 多种缓存策略: LRU、LFU和FIFO； 缓存数据有两级：内存和磁盘，因此无需担心容量问题； 缓存数据会在虚拟机重启的过程中写入磁盘； 可以通过 RMI、可插入 API 等方式进行分布式缓存； 具有缓存和缓存管理器的侦听接口: 缓存管理器监听器、缓存事件监听器； 支持多缓存管理器实例，以及一个实例的多个缓存区域； 提供 Hibernate 的缓存实现: Hibernate默认二级缓存是不启动的, 启动二级缓存需要采用 Ehcache来实现； Ehcache缓存数据过期策略Ehcache提供三种缓存过期策略. FIFO: (First In First Out)先进先出, 驱除原则是优先驱除最先储存的元素. LRU: (Least Recently Used) 最近最少使用, 驱除原则是优先驱除最后使用时间最早的元素. 缓存的元素有一个时间戳, 距离当前时间最远的元素优先被清出缓存. LFU: (Least Frequently Used)最不常使用, 每次我们从Cache中获取一个元素时都会更新该元素的hitCount属性值加1。当采用最不常使用原则进行驱除时hitCount属性值最小的元素将优先驱除. 对比LRU、LFU、FIFO LRU: 比较最后的访问时间 LFU: 比较get次数 FIFO: 根据创建或修改时间 Ehcache基本使用Ehcache配置CacheManager是Ehcache的核心，它的主要职责是对Cache的创建、移除和访问。只有CacheManager里面的Cache才能实现缓存数据的功能。一切使用Ehcache的应用都是从构建CacheManager开始的, 一个应用可以有多个CacheManager，而一个CacheManager下又可以有多个Cache。Cache内部保存的是一个个的Element，而一个Element中保存的是一个key和value的配对，相当于Map里面的一个Entry. ehcache.xml文件是用来定义Ehcache的配置信息的，更准确的来说它是定义CacheManager的配置信息的, 配置示例如下. 123456789101112131415161718192021222324252627282930313233&lt;ehcache xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://ehcache.org/ehcache.xsd" updateCheck="false"&gt; &lt;!-- 硬盘缓存的临时路径 --&gt; &lt;diskStore path="java.io.tmpdir"/&gt; &lt;!-- 默认的缓存区域的缓存策略 --&gt; &lt;defaultCache maxElementsInMemory="20000" eternal="false" timeToIdleSeconds="120" timeToLiveSeconds="120" overflowToDisk="true" maxElementsOnDisk="10000000" diskPersistent="false" diskExpiryThreadIntervalSeconds="120" memoryStoreEvictionPolicy="LRU" /&gt; &lt;!-- 自定义缓存区域 --&gt; &lt;cache name="eternalCache" maxElementsInMemory="20000" eternal="true" overflowToDisk="true" diskPersistent="false" timeToLiveSeconds="0" diskExpiryThreadIntervalSeconds="120" /&gt;&lt;/ehcache&gt; 参数说明 name: cache的唯一标识 maxElementsInMemory：内存中最大容纳的缓存对象数量 maxElementsOnDisk：硬盘上能存放的最大缓存对象数量, 0标识无穷大. eternal: 对象是否永生，默认是false, 一丹设置了, timeout将不起作用 overflowToDisk: 配置此属性,当内存中缓存对象数量达到maxElementsInMemory, Ehcahce会将Element写到磁盘中. timeToIdleSeconds: 设置缓存对象在失效前的允许闲置时间. 仅当缓存对象不是永久有效时使用, 默认值是0, 也就是闲置时间无穷大. timeToLiveSeconds: 设置缓存对象在失效前允许存活时间. 最大时间介于创建时间和失效时间之间; 仅当缓存对象不是永久有效时使用, 默认值是0, 也就是存活时间无穷大. diskPersistent: 是否将缓存数据持久化在磁盘上, 默认为false, true表示JVM重启 原来的缓存数据仍然存在. diskExpiryThreadIntervalSeconds: 磁盘失效线程运行时间间隔, 默认是120秒. memoryStoreEvictionPolicy：当达到maxElementsInMemory限制时, Ehcache将会根据指定的策略进行清理内存, 支持: LRU(默认侧),LFU,FIFO. 更多参数: http://elim.iteye.com/blog/2113728 新增元素1234567@Test public void create() &#123; Cache cache = cacheManager.getCache("cache"); Element ele = new Element("key", "value"); //把ele放入缓存cache中 cache.put(ele); &#125; 获取元素123456789public void read() &#123; Cache cache = cacheManager.getCache("cache"); //通过key来获取缓存中对应的元素 Element ele = cache.get("key"); System.out.println(ele); if (ele != null) &#123;//当缓存的元素存在时获取缓存的值 System.out.println(ele.getObjectValue()); &#125; &#125; 更新元素12345678public void update() &#123; Cache cache = cacheManager.getCache("cache"); cache.put(new Element("key", "value1")); System.out.println(cache.get("key")); //当添加元素的时候，如果缓存中已经存在相同key的元素则会将后者覆盖前者 cache.put(new Element("key", "value2")); System.out.println(cache.get("key")); &#125; 删除元素123456public void delete() &#123; Cache cache = cacheManager.getCache("cache"); //根据key来移除一个元素 cache.remove("key"); System.out.println(cache.get("key")); &#125; http://elim.iteye.com/blog/2112170 Ehcache集群方式Ehcache从1.7版本开始, 支持5种集群方案: Terracotta、RMI、JMS、JGroup、Ehcache Server; 其中RMI、JMS和Ehcache Server是最经常使用的. RMI组播RMI 是一种点对点的基于Java 对象的通讯方式, Ehcache从1.2版本开始就支持RMI方式的缓存机器. 在集群环境中Ehcache所有缓存对象的Key和value都必须是可序列化的， 也就是必须实现java.io.serializable. 这点在其它集群方式下也是需要遵守的. JMS消息方式JSM 是两个应用程序之间进行异步通信的API, 它为标准消息协议和消息服务提供了一组通用接口, 包括创建、发送、读取消息等, 用于支持Java应用程序开发, JMS也支持基于事件的通信机制, 通过发布事件机制向所有服务器保持连接的客户端发送消息, 在发送消息时, 接受者不需要在线, 等到客户端上线的时候, 能保证接收到服务器发送的消息. JMS 的核心就是一个消息队列, 每个应用节点都订阅预先定义好的主题, 同时,节点有元素更新时, 也会发布更新元素到主题去, 各个应用服务器节点通过侦听MQ获取到最新的数据, 然后分别更新自己的Ehcache缓存. Ehcache默认支持ActiveMQ, 也可以通过自定义组件的方式实现类似Kafka和RabbitMQ等. Cache Server 模式缓存服务器集群模式, 缓存数据集中放在Ehcache Server中, Ehcache Server之间做数据复制. Cache Server一般以War包方式独立部署, Cache Server有两种类型的API: 面向资源的Restful和Soap. 这两种API都能够跨语言支持. Spring中使用cache和Spring对事务管理的支持一样，Spring对Cache的支持也有基于注解和基于XML配置两种方式。下面我们先来看看基于注解的方式 Spring提供了4中方法级的缓存注解: @Cacheable@Cacheable 可以标记在一个方法上，也可以标记在一个类上。当标记在一个方法上时表示该方法是支持缓存的，当标记在一个类上时则表示该类所有的方法都是支持缓存的. 该注解支持用condition属性来设置条件, 如果不满足条件, 就不适用缓存能力, 直接执行方法. 使用示例示例表示只有当user的id为偶数时才会进行缓存. 12345@Cacheable(value=&#123;"users"&#125;, key="#user.id", condition="#user.id%2==0")public User find(User user) &#123; System.out.println("find user by user " + user); return user;&#125; @CachePut此注解的支持的属性和方法与@Cacheable一致, 对于使用@Cacheable标注的方法，Spring在每次执行前都会检查Cache中是否存在相同key的缓存元素，如果存在就不再执行该方法，而是直接从缓存中获取结果进行返回，否则才会执行并将返回结果存入指定的缓存中。@CachePut也可以声明一个方法支持缓存功能。与@Cacheable不同的是使用@CachePut标注的方法在执行前不会去检查缓存中是否存在之前执行过的结果，而是每次都会执行该方法，并将执行结果以键值对的形式存入指定的缓存中。 使用示例1234@CachePut("users")public User find(Integer id) &#123; return getFromDB(id);&#125; 对比@CachePut 和 @Cacheable @CachePut: 这个注释可以确保方法被执行，同时方法的返回值也被记录到缓存中。 @Cacheable: 当重复使用相同参数调用方法的时候，方法本身不会被调用执行，即方法本身被略过了，取而代之的是方法的结果直接从缓存中找到并返回了 @CacheEvict@CacheEvict是用来标注在需要清除缓存元素的方法或类上的. 应用示例1234@CacheEvict(value="users", allEntries=true)public void delete(Integer id) &#123; System.out.println("delete user by id: " + id);&#125; @CacheConfig这是一个类级别的注解, 主要是共享缓存的名称, 比如@Cacheable里面有一个value=”xxx”的属性, 如果需要配置的方法很多, 以后维护起来非常不方便, 所以@CacheConfig就是用来统一声明. Ehcache架构图 Ehcache应用场景 比较少的更新数据表的场景 对并发要求不是很严格的场景 对一致性要求不高的场景 Ehcache瓶颈点缓存漂移每个应用节点只管理自己的缓存, 在更新某个节点的时候, 不会影响到其它的节点, 这样数据之间可能就不同步了. 数据库瓶颈对于单实例应用来说, 缓存可以保护数据库的读风暴, 但是在集群环境下, 每个应用节点都要定期保证数据最新, 节点越多, 节点越大, 维护这样的情况对数据库开销也越大. 参考文档 《深入分布式缓存-从原理到实践》 https://www.cnblogs.com/s-b-b/p/6047954.html http://www.iteye.com/blogs/subjects/ehcache http://elim.iteye.com/blog/2123030]]></content>
      <categories>
        <category>组件框架</category>
        <category>缓存组件</category>
      </categories>
      <tags>
        <tag>Cache</tag>
        <tag>Ehcache</tag>
        <tag>Spring Cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[缓存组件-Memcached]]></title>
    <url>%2F2015%2F09%2F01%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2F%E7%BC%93%E5%AD%98%E7%BB%84%E4%BB%B6-Memcached%2F</url>
    <content type="text"><![CDATA[Memcached介绍Memcached是一个自由开源的,高性能,分布式内存对象缓存系统. Memcached特性基于C/S架构,协议简单 服务端启用memcached进程, 客户端可以通过telnet操作, 也可以通过各种编程语言实现的客户端存储数据及查询状态. 客户端和服务端采用的协议简单, 支持文本协议或二进制协议 基于libevent事件处理 libevent是一套跨平台事件处理接口的封装,兼容Windows/Linux/BSD/Solaris等操作系统, 封装了的接口包括poll、select(Windows)、epoll(Linux)、 kqueue(BSD), /dev/poll(Solaris) Memcached使用libevent来进行网络并发连接的处理, 能够保证在很大并发情况下, 仍旧能够保持快速的响应能力. libevent: http://www.monkey.org/~provos/libevent/ 基于内存存储方式 为了提高性能,memcached中保存的数据都存储在memcached内置的内存存储空间中。由于数据仅存在于内存中,因此重启memcached、重启操作系统会导致全部数据消失。另外,内容容量达到指定值之后,就基于LRU(Least Recently Used)算法自动删除不使用的缓存。memcached本身是为缓存而设计的服务器,因此并没有过多考虑数据的永久性问题。 基于客户端的分布式 Memcached尽管是“分布式”缓存服务器, 但其服务端并没有分布式功能, 各个Memcached实例不会互相通信以共享信息, 它的分布式主要是通过客户端实现的. Memcached安装部署下载安装1234567$ sudo yum install libevent libevent-devel$ wget http://www.danga.com/memcached/dist/memcached-1.2.5.tar.gz$ tar zxf memcached-1.2.5.tar.gz$ cd memcached-1.2.5$ ./configure$ make$ sudo make install 启动进程1234567891011121314# 这里显示了调试信息, 监听TCP端口11211 最大内存使用量为64M.$ /usr/local/bin/memcached -p 11211 -m 64m -vvslab class 1: chunk size 88 perslab 11915slab class 2: chunk size 112 perslab 9362slab class 3: chunk size 144 perslab 7281# 中间省略slab class 38: chunk size 391224 perslab 2slab class 39: chunk size 489032 perslab 2&lt;23 server listening&lt;24 send buffer was 110592, now 268435456&lt;24 server listening (udp)&lt;24 server listening (udp)&lt;24 server listening (udp)&lt;24 server listening (udp) 后台启动1$ /usr/local/bin/memcached -p 11211 -m 64m -d 参数说明 -p: 使用的TCP端口。默认为11211 -m: 最大内存大小。默认为64M -vv: 用very vrebose模式启动,调试信息和错误输出到控制台 -d: 作为daemon在后台启动 更多命令可以通过-h查看 1$ /usr/local/bin/memcached -h Memcached内存分配Memcached默认情况下采用了名为Slab Allocator的机制分配、管理内存。在该机制出现以前,内存的分配是通过对所有记录简单地进行malloc和free来进行的。但是,这种方式会导致内存碎片,加重操作系统内存管理器的负担,最坏的情况下,会导致操作系统比memcached进程本身还慢。Slab Allocator就是为解决该问题而诞生的。 Slab Allocator基本原理Slab Allocator的基本原理是按照预先规定的大小,将分配的内存以page为单位,默认情况下一个page是1M,可以通过-I参数在启动时指定,分割成各种尺寸的块(chunk), 并把尺寸相同的块分成组(chunk的集合),如果需要申请内存时,memcached会划分出一个新的page并分配给需要的slab区域。page一旦被分配在重启前不会被回收或者重新分配,以解决内存碎片问题。 说明: Page: 分配给Slab的内存空间,默认是1MB。分配给Slab之后根据slab的大小切分成chunk。 Chunk: 用于缓存记录的内存空间(chunk中不仅仅存放缓存对象的value,而且保存了缓存对象的key,expire time, flag等详细信息)。 Slab Class: 特定大小的chunk的组 Memcached并不是将所有大小的数据都放在一起的,而是预先将数据空间划分为一系列slabs,每个slab只负责一定范围内的数据存储。memcached根据收到的数据的大小,选择最适合数据大小的slab。memcached中保存着slab内空闲chunk的列表,根据该列表选择chunk,然后将数据缓存于其中。 Slab Allocator的缺点Slab Allocator解决了当初的内存碎片问题,但新的机制也给memcached带来了新的问题,由于分配的是特定长度的内存,因此无法有效利用分配的内存。 例如,将100字节的数据缓存到128字节的chunk中,剩余的28字节就浪费了。 Growth Factor调优控制memcached在启动时指定Growth Factor因子(通过-f选项),就可以在某种程度上控制slab之间的差异。默认值为1.25。 但是,在该选项出现之前,这个因子曾经固定为2,称为“powers of 2”策略. 让我们用以前的设置,以verbose模式启动memcached试试看： 1234567891011121314$ memcached -f 2 -vvslab class 1: chunk size 128 perslab 8192slab class 2: chunk size 256 perslab 4096slab class 3: chunk size 512 perslab 2048slab class 4: chunk size 1024 perslab 1024slab class 5: chunk size 2048 perslab 512slab class 6: chunk size 4096 perslab 256slab class 7: chunk size 8192 perslab 128slab class 8: chunk size 16384 perslab 64slab class 9: chunk size 32768 perslab 32slab class 10: chunk size 65536 perslab 16slab class 11: chunk size 131072 perslab 8slab class 12: chunk size 262144 perslab 4slab class 13: chunk size 524288 perslab 2 可见从128字节的组开始,组的大小依次增大为原来的2倍。这样设置的问题是,slab之间的差别比较大,有些情况下就相当浪费内存. 来看看现在的默认设置(f=1.25)时的输出(篇幅所限,这里只写到第10组) 12345678910slab class 1: chunk size 88 perslab 11915slab class 2: chunk size 112 perslab 9362slab class 3: chunk size 144 perslab 7281slab class 4: chunk size 184 perslab 5698slab class 5: chunk size 232 perslab 4519slab class 6: chunk size 296 perslab 3542slab class 7: chunk size 376 perslab 2788slab class 8: chunk size 472 perslab 2221slab class 9: chunk size 592 perslab 1771slab class 10: chunk size 744 perslab 1409 Memcached过期机制Memcached有两种过期机制: Lazy Expiration 和 LRU 惰性过期(Lazy Expiration): memcached内部不会监视记录是否过期,而是在get时查看记录的时间戳,检查记录是否过 最近最少使用(LRU): 当memcached的内存空间不足时(无法从slab class 获取到新的空间时),就从最近未被使用的记录中搜索,并将其空间分配给新的记录。 不过,有些情况下LRU机制反倒会造成麻烦。memcached启动时通过“-M”参数可以禁止LRU. 12# 小写的“-m”选项是用来指定最大内存大小的。不指定具体数值则使用默认值64MB$ memcached -M -m 1024 客户端一致性HashMemcached的分布式是通过客户端Consistent Hashing(一致性Hash)算法实现的, 如下所示: 首先求出memcached服务器（节点）的哈希值, 并将其配置到0～2SUP(32)的圆（continuum）上。 然后用同样的方法求出存储数据的键的哈希值,并映射到圆上。 然后从数据映射到的位置开始顺时针查找,将数据保存到找到的第一个服务器上。 如果超过2SUP(32)仍然找不到服务器,就会保存到第一台memcached服务器上。 从上图的状态中添加一台memcached服务器。余数分布式算法由于保存键的服务器会发生巨大变化 而影响缓存的命中率,但Consistent Hashing中,只有在continuum上增加服务器的地点逆时针方向的 第一台服务器上的键会受到影响. 因此,Consistent Hashing最大限度地抑制了键的重新分布。 而且,有的Consistent Hashing的实现方法还采用了虚拟节点的思想。 使用一般的hash函数的话,服务器的映射地点的分布非常不均匀。 因此,使用虚拟节点的思想,为每个物理节点（服务器） 在continuum上分配100～200个点。这样就能抑制分布不均匀, 最大限度地减小服务器增减时的缓存重新分布。 通过下文中介绍的使用Consistent Hashing算法的memcached客户端函数库进行测试的结果是, 由服务器台数（n）和增加的服务器台数（m）计算增加服务器后的命中率计算公式如下： 1(1 - n/(n+m)) * 100 Memcached客户端 参考文档 https://www.w3cschool.cn/memcached/ http://www.runoob.com/memcached/memcached-tutorial.html http://gihyo.jp/tagList/memcached https://charlee.li/memcached-004.html]]></content>
      <categories>
        <category>组件框架</category>
        <category>缓存组件</category>
      </categories>
      <tags>
        <tag>Cache</tag>
        <tag>Memcached</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RPC服务-Dubbo]]></title>
    <url>%2F2015%2F08%2F01%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FRPC%E7%BB%84%E4%BB%B6-Dubbo%2F</url>
    <content type="text"><![CDATA[架构演进单一应用架构当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。此时，用于简化增删改查工作量的数据访问框架(ORM)是关键。 垂直应用架构当访问量逐渐增大，单一应用增加机器带来的加速度越来越小，将应用拆成互不相干的几个应用，以提升效率。此时，用于加速前端页面开发的Web框架(MVC)是关键。 分布式服务架构当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。此时，用于提高业务复用及整合的分布式服务框架(RPC)是关键。 流动计算架构当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。此时，用于提高机器利用率的资源调度和治理中心(SOA)是关键 Dubbo概述Dubbo是阿里巴巴公司开源的一个高性能优秀的分布式服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和Spring框架无缝集成. Dubbo架构Dubbo架构如下图, Dubbo 架构具有以下几个特点，分别是连通性、健壮性、伸缩性、以及向未来架构的升级性。 节点角色说明 Provider : 暴露服务的服务提供方 Consumer : 调用远程服务的服务消费方 Registry : 服务注册与发现的注册中心 Monitor: 统计服务的调用次数和调用时间的监控中心 Container: 服务运行容器 调用关系说明 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 连通性 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小 监控中心负责统计各服务调用次数，调用时间等，统计先在内存汇总后每分钟一次发送到监控中心服务器，并以报表展示 服务提供者向注册中心注册其提供的服务，并汇报调用时间到监控中心，此时间不包含网络开销 服务消费者向注册中心获取服务提供者地址列表，并根据负载算法直接调用提供者，同时汇报调用时间到监控中心，此时间包含网络开销 注册中心，服务提供者，服务消费者三者之间均为长连接，监控中心除外 注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者 注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表 注册中心和监控中心都是可选的，服务消费者可以直连服务提供者 健壮性 监控中心宕掉不影响使用，只是丢失部分采样数据 数据库宕掉后，注册中心仍能通过缓存提供服务列表查询，但不能注册新服务 注册中心对等集群，任意一台宕掉后，将自动切换到另一台 注册中心全部宕掉后，服务提供者和服务消费者仍能通过本地缓存通讯 服务提供者无状态，任意一台宕掉后，不影响使用 服务提供者全部宕掉后，服务消费者应用将无法使用，并无限次重连等待服务提供者恢复 伸缩性 注册中心为对等集群，可动态增加机器部署实例，所有客户端将自动发现新的注册中心 服务提供者无状态，可动态增加机器部署实例，注册中心将推送新的服务提供者信息给消费者 升级性当服务集群规模进一步扩大，带动IT治理结构进一步升级，需要实现动态部署，进行流动计算，现有分布式服务架构不会带来阻力。 Dubbo应用Dubbo采用全Spring配置方式，透明化接入应用，对应用没有任何API侵入, 在本地服务的基础上只需做简单配置，即可完成服务化: Dubbo应用示例传统Spring服务扩展为Dubbo服务的步骤比较简单. 将传统服务中接口的实例化定义拆分为两部分: 服务提供方、服务消费方 将服务定义部分放在服务提供方 remote-provider.xml， 将服务引用部分放在服务消费方 remote-consumer.xml。 并在提供方增加暴露服务配置 dubbo:service，在消费方增加引用服务配置 dubbo:reference 假设我们有一个服务接口(DemoService)和接口实现(DemoServiceImpl), 对其做服务化. 服务提供方 123456789101112131415161718192021222324&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://dubbo.apache.org/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd"&gt; &lt;!-- 提供方应用信息，用于计算依赖关系 --&gt; &lt;dubbo:application name="hello-world-app" /&gt; &lt;!-- 使用multicast广播注册中心暴露服务地址 --&gt; &lt;dubbo:registry address="multicast://224.5.6.7:1234" /&gt; &lt;!-- 用dubbo协议在20880端口暴露服务 --&gt; &lt;dubbo:protocol name="dubbo" port="20880" /&gt; &lt;!-- 声明需要暴露的服务接口 --&gt; &lt;dubbo:service interface="com.alibaba.dubbo.demo.DemoService" ref="demoService" /&gt; &lt;!-- 和本地bean一样实现服务 --&gt; &lt;bean id="demoService" class="com.alibaba.dubbo.demo.provider.DemoServiceImpl" /&gt;&lt;/beans&gt; 服务消费方 123456789101112131415161718&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://dubbo.apache.org/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd"&gt; &lt;!-- 消费方应用名，用于计算依赖关系，不是匹配条件，不要与提供方一样 --&gt; &lt;dubbo:application name="consumer-of-helloworld-app" /&gt; &lt;!-- 使用multicast广播注册中心暴露发现服务地址 --&gt; &lt;dubbo:registry address="multicast://224.5.6.7:1234" /&gt; &lt;!-- 生成远程服务代理，可以和本地bean一样使用demoService --&gt; &lt;dubbo:reference id="demoService" interface="com.alibaba.dubbo.demo.DemoService" /&gt;&lt;/beans&gt; 在服务消费方我们就可以正常使用DemoService的实例化Bean demoService了. 在开发及测试环境下，经常需要绕过注册中心，只测试指定服务提供者，这时候可能需要点对点直连. 1&lt;dubbo:reference id="demoService" interface="com.alibaba.dubbo.demo.DemoService" url="dubbo://localhost:20890"/&gt; Dubbo配置方式Dubbo服务化配置提供了4种方式: 硬编码方式、基于XML配置、基于属性文件配置、基于注解配置. 配置示例 编码方式: http://dubbo.apache.org/zh-cn/docs/user/configuration/api.html XML方式: http://dubbo.apache.org/zh-cn/docs/user/configuration/xml.html 属性配置: http://dubbo.apache.org/zh-cn/docs/user/configuration/properties.html 注解配置: http://dubbo.apache.org/zh-cn/docs/user/configuration/annotation.html Dubbo模块特性启动检查Dubbo 缺省会在启动时检查依赖的服务是否可用，不可用时会抛出异常，阻止 Spring 初始化完成，以便上线时，能及早发现问题，默认 check=”true”。 可以通过 check=”false” 关闭检查，比如，测试时，有些服务不关心，或者出现了循环依赖，必须有一方先启动. 说明: 如果你的 Spring 容器是懒加载的，或者通过 API 编程延迟引用服务，请关闭 check，否则服务临时不可用时，会抛出异常，拿到 null 引用，如果 check=”false”，总是会返回引用，当服务恢复时，能自动连上. 关闭特定服务 12&lt;!-- 没有提供者时报错 --&gt;&lt;dubbo:reference interface="com.alibaba.dubbo.demo.DemoService" check="false" /&gt; 关闭所有服务 12&lt;!-- 没有提供者时报错 --&gt;&lt;dubbo:consumer check="false" /&gt; 关不注册中心 12&lt;!-- 注册订阅失败时报错 --&gt;&lt;dubbo:consumer check="false" /&gt; 集群容错在集群调用失败时，Dubbo 提供了多种容错方案，缺省为Failover Cluster重试策略. Failover Cluster: 失败自动切换(默认策略)，当出现失败，重试其它服务器, 通常用于读操作，但重试会带来更长延迟。可通过 retries=”2” 来设置重试次数(不含第一次) Failfast Cluster: 快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。 Failsafe Cluster: 失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。 Failback Cluster: 失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。 Forking Cluster: 并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=”2” 来设置最大并行数. Broadcast Cluster: 广播调用所有提供者，逐个调用，任意一台报错则报错。通常用于通知所有提供者更新缓存或日志等本地资源信息。 下面示例展示默认策略Failover Cluster的配置应用: 服务端示例 1234567&lt;!-- 针对接口 --&gt;&lt;dubbo:service interface="..." retries="2" /&gt;&lt;!-- 针对特定方法 --&gt;&lt;dubbo:service interface="..."&gt; &lt;dubbo:method name="..." retries="2" /&gt;&lt;/dubbo:service&gt; 消费端示例 1234567&lt;!-- 针对接口 --&gt;&lt;dubbo:reference retries="2" /&gt;&lt;!-- 针对特定方法 --&gt;&lt;dubbo:reference interface="..."&gt; &lt;dubbo:method name="..." retries="2" /&gt;&lt;/dubbo:reference&gt; 负载均衡在集群负载均衡时，Dubbo 提供了多种均衡策略，缺省为 random 随机调用, 用户也可以自行扩展负载均衡策略. 随机 Random LoadBalance 随机，按权重设置随机概率。 在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重 轮询 RoundRobin LoadBalance 轮询，按公约后的权重设置轮询比率。 存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上 最少活跃调用数 LeastActive LoadBalance 最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。 使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 一致性哈希 ConsistentHash LoadBalance 一致性 Hash，相同参数的请求总是发到同一提供者。 当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。 算法参见：http://en.wikipedia.org/wiki/Consistent_hashing 缺省只对第一个参数 Hash，如果要修改，请配置 &lt;dubbo:parameter key=”hash.arguments” value=”0,1” /&gt; 缺省用 160 份虚拟节点，如果要修改，请配置 &lt;dubbo:parameter key=”hash.nodes” value=”320” /&gt; 配置示例 服务端配置 1234567&lt;!-- 针对接口 --&gt;&lt;dubbo:service interface="..." loadbalance="roundrobin" /&gt;&lt;!-- 针对特定方法 --&gt;&lt;dubbo:service interface="..."&gt; &lt;dubbo:method name="..." loadbalance="roundrobin"/&gt;&lt;/dubbo:service&gt; 消费端配置 1234567&lt;!-- 针对接口 --&gt;&lt;dubbo:reference interface="..." loadbalance="roundrobin" /&gt;&lt;!-- 针对特定方法 --&gt;&lt;dubbo:reference interface="..."&gt; &lt;dubbo:method name="..." loadbalance="roundrobin"/&gt;&lt;/dubbo:reference&gt; 线程模型如果事件处理的逻辑能迅速完成，并且不会发起新的 IO 请求，比如只是在内存中记个标识，则直接在 IO 线程上处理更快，因为减少了线程池调度。 但如果事件处理逻辑较慢，或者需要发起新的 IO 请求，比如需要查询数据库，则必须派发到线程池，否则 IO 线程阻塞，将导致不能接收其它请求。 如果用 IO 线程处理事件，又在事件处理过程中发起新的 IO 请求，比如在连接事件中发起登录请求，会报“可能引发死锁”异常，但不会真死锁。 因此，需要通过不同的派发策略和不同的线程池配置的组合来应对不同的场景: 1&lt;dubbo:protocol name="dubbo" dispatcher="all" threadpool="fixed" threads="100" /&gt; Dispatcher all: 所有消息都派发到线程池，包括请求，响应，连接事件，断开事件，心跳等。 direct: 所有消息都不派发到线程池，全部在 IO 线程上直接执行。 message: 只有请求响应消息派发到线程池，其它连接断开事件，心跳等消息，直接在 IO 线程上执行。 execution: 只请求消息派发到线程池，不含响应，响应和其它连接断开事件，心跳等消息，直接在 IO 线程上执行。 connection: 在 IO 线程上，将连接断开事件放入队列，有序逐个执行，其它消息派发到线程池。 ThreadPool fixed: 固定大小线程池，启动时建立线程，不关闭，一直持有。(缺省) cached: 缓存线程池，空闲一分钟自动删除，需要时重建。 limited: 可伸缩线程池，但池中的线程数只会增长不会收缩。只增长不收缩的目的是为了避免收缩时突然来了大流量引起的性能问题。 eager: 优先创建Worker线程池。在任务数量大于corePoolSize但是小于maximumPoolSize时，优先创建Worker来处理任务。当任务数量大于maximumPoolSize时，将任务放入阻塞队列中。阻塞队列充满时抛出RejectedExecutionException。(相比于cached:cached在任务数量超过maximumPoolSize时直接抛出异常而不是将任务放入阻塞队列) 静态服务有时候希望人工管理服务提供者的上线和下线，此时需将注册中心标识为非动态管理模式。 1&lt;dubbo:registry address="10.20.141.150:9090" dynamic="false" /&gt; 或者 1&lt;dubbo:registry address="10.20.141.150:9090?dynamic=false" /&gt; 服务提供者初次注册时为禁用状态，需人工启用。断线时，将不会被自动删除，需人工禁用. 多协议Dubbo 允许配置多协议，在不同服务上支持不同协议或者同一服务上同时支持多种协议。 不同服务不同协议不同服务在性能上适用不同协议进行传输，比如大数据用短连接协议，小数据大并发用长连接协议 1234567891011121314151617&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://dubbo.apache.org/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd"&gt; &lt;dubbo:application name="world" /&gt; &lt;dubbo:registry id="registry" address="10.20.141.150:9090" username="admin" password="hello1234" /&gt; &lt;!-- 多协议配置 --&gt; &lt;dubbo:protocol name="dubbo" port="20880" /&gt; &lt;dubbo:protocol name="rmi" port="1099" /&gt; &lt;!-- 使用dubbo协议暴露服务 --&gt; &lt;dubbo:service interface="com.alibaba.hello.api.HelloService" version="1.0.0" ref="helloService" protocol="dubbo" /&gt; &lt;!-- 使用rmi协议暴露服务 --&gt; &lt;dubbo:service interface="com.alibaba.hello.api.DemoService" version="1.0.0" ref="demoService" protocol="rmi" /&gt; &lt;/beans&gt; 多协议暴露服务需要与 http 客户端互操作 12345678910111213141516&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://dubbo.apache.org/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd http://dubbo.apache.org/schema/dubbo http://dubbo.apache.org/schema/dubbo/dubbo.xsd"&gt; &lt;dubbo:application name="world" /&gt; &lt;dubbo:registry id="registry" address="10.20.141.150:9090" username="admin" password="hello1234" /&gt; &lt;!-- 多协议配置 --&gt; &lt;dubbo:protocol name="dubbo" port="20880" /&gt; &lt;dubbo:protocol name="hessian" port="8080" /&gt; &lt;!-- 使用多个协议暴露服务 --&gt; &lt;dubbo:service id="helloService" interface="com.alibaba.hello.api.HelloService" version="1.0.0" protocol="dubbo,hessian" /&gt;&lt;/beans&gt; 多注册中心Dubbo 支持同一服务向多注册中心同时注册，或者不同服务分别注册到不同的注册中心上去，甚至可以同时引用注册在不同注册中心上的同名服务。另外，注册中心是支持自定义扩展的。 http://dubbo.apache.org/zh-cn/docs/user/demos/multi-registry.html 服务分组当一个接口有多种实现时，可以用 group 区分. 服务方 12&lt;dubbo:service group="feedback" interface="com.xxx.IndexService" /&gt;&lt;dubbo:service group="member" interface="com.xxx.IndexService" /&gt; 消费方 12&lt;dubbo:reference id="feedbackIndexService" group="feedback" interface="com.xxx.IndexService" /&gt;&lt;dubbo:reference id="memberIndexService" group="member" interface="com.xxx.IndexService" /&gt; 任意组 1&lt;dubbo:reference id="barService" interface="com.foo.BarService" group="*" /&gt; 多版本当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用。 可以按照以下的步骤进行版本迁移： 在低压力时间段，先升级一半提供者为新版本 再将所有消费者升级为新版本 然后将剩下的一半提供者升级为新版本 服务方 12345&lt;!-- 老版本服务提供者配置 --&gt;&lt;dubbo:service interface="com.foo.BarService" version="1.0.0" /&gt;&lt;!-- 新版本服务提供者配置--&gt;&lt;dubbo:service interface="com.foo.BarService" version="2.0.0" /&gt; 消费方 12345&lt;!-- 老版本服务提供者配置 --&gt;&lt;dubbo:reference id="barService" interface="com.foo.BarService" version="1.0.0" /&gt;&lt;!-- 新版本服务提供者配置--&gt;&lt;dubbo:reference id="barService" interface="com.foo.BarService" version="2.0.0" /&gt; 如果不需要区分版本,可如下配置 1&lt;dubbo:reference id="barService" interface="com.foo.BarService" version="*" /&gt; 参数验证参数验证功能是基于JSR303实现的, 用户只需标识 JSR303 标准的验证 annotation，并通过声明 filter来实现验证. Maven依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;javax.validation&lt;/groupId&gt; &lt;artifactId&gt;validation-api&lt;/artifactId&gt; &lt;version&gt;1.0.0.GA&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt; &lt;version&gt;4.2.0.Final&lt;/version&gt;&lt;/dependency&gt; 参数标注 1234567891011121314151617181920212223@Data public class ValidationParameter implements Serializable &#123; private static final long serialVersionUID = 7158911668568000392L; @NotNull // 不允许为空 @Size(min = 1, max = 20) // 长度或大小范围 private String name; @NotNull(groups = ValidationService.Save.class) // 保存时不允许为空，更新时允许为空 ，表示不更新该字段 @Pattern(regexp = "^\\s*\\w+(?:\\.&#123;0,1&#125;[\\w-]+)*@[a-zA-Z0-9]+(?:[-.][a-zA-Z0-9]+)*\\.[a-zA-Z]+\\s*$") private String email; @Min(18) // 最小值 @Max(100) // 最大值 private int age; @Past // 必须为一个过去的时间 private Date loginDate; @Future // 必须为一个未来的时间 private Date expiryDate; &#125; 参数验证 12345678910import javax.validation.constraints.Min;import javax.validation.constraints.NotNull; public interface ValidationService &#123; // 验证参数不为空 void save(@NotNull ValidationParameter parameter); // 直接对基本类型参数验证 void delete(@Min(1) int id); &#125; 配置示例 客户端验证 1&lt;dubbo:reference id="validationService" interface="com.alibaba.dubbo.examples.validation.api.ValidationService" validation="true" /&gt; 服务端验证 1&lt;dubbo:service interface="com.alibaba.dubbo.examples.validation.api.ValidationService" ref="validationService" validation="true" /&gt; 结果缓存结果缓存，用于加速热门数据的访问速度，Dubbo 提供声明式缓存，以减少用户加缓存的工作量. 缓存类型 lru: 基于最近最少使用原则删除多余缓存，保持最热的数据被缓存。 threadlocal: 当前线程缓存，比如一个页面渲染，用到很多 portal，每个 portal 都要去查用户信息，通过线程缓存，可以减少这种多余访问。 jcache: 与 JSR107 集成，可以桥接各种缓存实现 配置示例 1234567&lt;!-- 针对接口 --&gt;&lt;dubbo:reference interface="com.foo.BarService" cache="lru" /&gt;&lt;!-- 针对方法 --&gt;&lt;dubbo:reference interface="com.foo.BarService"&gt; &lt;dubbo:method name="findBar" cache="lru" /&gt;&lt;/dubbo:reference&gt; 隐式参数可以通过 RpcContext 上的 setAttachment 和 getAttachment 在服务消费方和提供方之间进行参数的隐式传递。 说明: RpcContext 是一个 ThreadLocal 的临时状态记录器，当接收到 RPC 请求，或发起 RPC 请求时，RpcContext 的状态都会变化。比如：A 调 B，B 再调 C，则 B 机器上，在 B 调 C 之前，RpcContext 记录的是 A 调 B 的信息，在 B 调 C 之后，RpcContext 记录的是 B 调 C 的信息 消费端设置隐式参数 setAttachment 设置的 KV 对，在完成下面一次远程调用会被清空，即多次远程调用要多次设置。 123RpcContext.getContext().setAttachment("index", "1"); // 隐式传参，后面的远程调用都会隐式将这些参数发送到服务器端，类似cookie，用于框架集成，不建议常规业务使用xxxService.xxx(); // 远程调用// ... 服务端获取隐式参数 1234567public class XxxServiceImpl implements XxxService &#123; public void xxx() &#123; // 获取客户端隐式传入的参数，用于框架集成，不建议常规业务使用 String index = RpcContext.getContext().getAttachment("index"); &#125;&#125; 异步调用基于 NIO 的非阻塞实现并行调用，客户端不需要启动多线程即可完成并行调用多个远程服务，相对多线程开销较小. 消费端配置 123456&lt;dubbo:reference id="fooService" interface="com.alibaba.foo.FooService"&gt; &lt;dubbo:method name="findFoo" async="true" /&gt;&lt;/dubbo:reference&gt;&lt;dubbo:reference id="barService" interface="com.alibaba.bar.BarService"&gt; &lt;dubbo:method name="findBar" async="true" /&gt;&lt;/dubbo:reference&gt; 代码调用 123456789101112131415161718// 此调用会立即返回nullfooService.findFoo(fooId);// 拿到调用的Future引用，当结果返回后，会被通知和设置到此FutureFuture&lt;Foo&gt; fooFuture = RpcContext.getContext().getFuture(); // 此调用会立即返回nullbarService.findBar(barId);// 拿到调用的Future引用，当结果返回后，会被通知和设置到此FutureFuture&lt;Bar&gt; barFuture = RpcContext.getContext().getFuture(); // 此时findFoo和findBar的请求同时在执行，客户端不需要启动多线程来支持并行，而是借助NIO的非阻塞完成 // 如果foo已返回，直接拿到返回值，否则线程wait住，等待foo返回后，线程会被notify唤醒Foo foo = fooFuture.get(); // 同理等待bar返回Bar bar = barFuture.get(); // 如果foo需要5秒返回，bar需要6秒返回，实际只需等6秒，即可获取到foo和bar，进行接下来的处理。 也可以设置是否等待消息发出: sent=”true” 等待消息发出，消息发送失败将抛出异常。 sent=”false” 不等待消息发出，将消息放入IO队列，即刻返回. 1&lt;dubbo:method name="findFoo" async="true" sent="true" /&gt; 如果只是想异步，完全忽略返回值，可以配置 return=”false”，以减少 Future 对象的创建和管理成本 1&lt;dubbo:method name="findFoo" async="true" return="false" /&gt; 本地调用本地调用使用了 injvm 协议，是一个伪协议，它不开启端口，不发起远程调用，只在 JVM 内直接关联，但执行 Dubbo 的 Filter 链. 定义 injvm 协议 1&lt;dubbo:protocol name="injvm" /&gt; 设置默认协议 1&lt;dubbo:provider protocol="injvm" /&gt; 设置服务协议 1&lt;dubbo:service protocol="injvm" /&gt; 优先使用injvm 12&lt;dubbo:consumer injvm="true" .../&gt;&lt;dubbo:provider injvm="true" .../&gt; 或 12&lt;dubbo:reference injvm="true" .../&gt;&lt;dubbo:service injvm="true" .../&gt; 注意：服务暴露与服务引用都需要声明 injvm=”true” 从 2.2.0 开始，每个服务默认都会在本地暴露。在引用服务的时候，默认优先引用本地服务。如果希望引用远程服务可以使用一下配置强制引用远程服务。 1&lt;dubbo:reference ... scope="remote" /&gt; 参数回调 http://dubbo.apache.org/zh-cn/docs/user/demos/callback-parameter.html 事件通知 http://dubbo.apache.org/zh-cn/docs/user/demos/events-notify.html 延迟连接延迟连接用于减少长连接数。当有调用发起时，再创建长连接。 12&lt;!-- 该配置只对使用长连接的 dubbo 协议生效 --&gt;&lt;dubbo:protocol name="dubbo" lazy="true" /&gt; 延迟暴露如果你的服务需要预热时间，比如初始化缓存，等待相关资源就位等，可以使用 delay 进行延迟暴露。 12345&lt;!-- 延迟 5 秒暴露服务 --&gt;&lt;dubbo:service delay="5000" /&gt;&lt;!-- 延迟到 Spring 初始化完成后，再暴露服务 --&gt;&lt;dubbo:service delay="-1" /&gt; 并发控制 http://dubbo.apache.org/zh-cn/docs/user/demos/concurrency-control.html 连接控制 http://dubbo.apache.org/zh-cn/docs/user/demos/config-connections.html 粘滞连接粘滞连接用于有状态服务，尽可能让客户端总是向同一提供者发起调用，除非该提供者挂了，再连另一台。 粘滞连接将自动开启延迟连接，以减少长连接数. 1&lt;dubbo:protocol name="dubbo" sticky="true" /&gt; 令牌验证 http://dubbo.apache.org/zh-cn/docs/user/demos/token-authorization.html 线程栈自动Dump当业务线程池满时，我们需要知道线程都在等待哪些资源、条件，以找到系统的瓶颈点或异常点。dubbo通过Jstack自动导出线程堆栈来保留现场，方便排查问题. 默认策略: 导出路径，user.home标识的用户主目录 导出间隔，最短间隔允许每隔10分钟导出一次 配置示例 123&lt;dubbo:application ...&gt; &lt;dubbo:parameter key="dump.directory" value="/tmp" /&gt;&lt;/dubbo:application&gt; 优雅停机Dubbo 是通过 JDK 的 ShutdownHook 来完成优雅停机的，所以如果用户使用 kill -9 PID 等强制关闭指令，是不会执行优雅停机的，只有通过 kill PID 时，才会执行. 12345678910111213141516171819202122232425/** * Java.Runtime.addShutdownHook(Thread hook)方法，可以注册一个JVM关闭的钩子，这个钩子可以在一下几种场景中被调用 * &lt;p&gt; * 1. 程序正常退出 * 2. 使用System.exit() * 3. 终端使用Ctrl+C触发的中断 * 4. 系统关闭 * 5. OutOfMemory宕机 * 6. 使用Kill pid命令干掉进程(注: 在使用kill -9 pid时, 是不会被调用的) * &lt;p&gt; * 建议: 同一个JVM最好只使用一个关闭钩子，而不是每个服务都使用一个不同的关闭钩子，使用多个关闭钩子可能会出现当前这个钩子所要依赖的服务可能已经被另外一个关闭钩子关闭了。为了避免这种情况，建议关闭操作在单个线程中串行执行，从而避免了再关闭操作之间出现竞态条件或者死锁等问题 */public static void JvmHookWith() &#123; System.out.println("The JVM is started"); Runtime.getRuntime().addShutdownHook(new Thread() &#123; public void run() &#123; try &#123; // do something System.out.println("Execute JVM Hook"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;);&#125; http://dubbo.apache.org/zh-cn/docs/user/demos/graceful-shutdown.html 参考文档http://dubbo.apache.org/zh-cn/docs/user/quick-start.html]]></content>
      <categories>
        <category>组件框架</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>Dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RPC服务-Thrift]]></title>
    <url>%2F2015%2F08%2F01%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FRPC%E7%BB%84%E4%BB%B6-Thrift%2F</url>
    <content type="text"><![CDATA[Thrift概述Thrift作为Facebook开源的RPC框架, 通过IDL(Interface Definition Language)中间语言, 并借助代码生成引擎生成各种主流语言的rpc框架服务端/客户端代码 语义语法注释 Thrift支持C多行风格和Java/C++单行风格. 命名空间 Thrift中的命名空间类似于C++中的namespace和java中的package，它们提供了一种组织(隔离)代码的简便方式; 名字空间也可以用于解决类型定义中的名字冲突. 基本类型 bool: 布尔值 对应Java中的boolean byte: 8位有符号整型 对应Java中的byte i16: 16位有符号整型 对应Java中的short i32: 32位有符号整型 对应Java中的int i64: 64位有符号整型 对应Java中的long double: 64位浮点型 对应Java中的double string: 字符串 对应Java中的String binary: Blob 类型 对应Java中的byte[] 集合类型 list&lt;t&gt;: 元素类型为t的有序表,容许元素重复. set&lt;t&gt;: 元素类型为t的无序表,不容许元素重复. map&lt;t, t&gt;: 值类型为t的键值对,键不容许重复. 枚举类型 Thrift不支持枚举类嵌套，枚举常量必须是32位的正整数, 末尾属性没有分号 常量 在变量前面加上const, 改变量被声明为一个常量. 结构体 类似与Java中的对象, struct不能继承，但是可以嵌套(不能嵌套自己), 其成员都是有明确类型. 不支持泛型. 成员分割符可以是,或是; 定义结构体成员时, 成员必须是被正整数编号过的，其中的编号使不能重复的，这个是为了在传输过程中编码使用. 字段会有optional和required之分, 如果不指定则为无类型–可以不填充该值，但是在序列化传输的时候也会序列化进去，optional是不填充则部序列化，required是必须填充也必须序列化. 同一文件可以定义多个struct，也可以定义在不同的文件，进行include引入. 异常 异常在语法和功能上类似于结构体，差别是异常使用关键字exception，而且异常是继承每种语言的基础异常类 文件包含 为了便于管理、重用和提高模块性/组织性，我们常常分割Thrift定义在不同的文件中; 通过include关键字进行分文件的引入包含 应用示例基于IDL示例示例结构12345678[root@localhost etc]$ tree samplesample├── generate.sh├── idl│ ├── Request.thrift│ ├── Response.thrift│ └── ServiceIface.thrift└── src 定义结构体定义入参 12345678910111213[root@localhost sample]$ cat idl/Request.thrift// 定义包名namespace java com.sample.thrift.modelinclude "Response.thrift"/** * 查询入参 */struct QueryEntry &#123; required string keyword; // 检索词组(required 必填) optional Response.AuditEnum audit; // 审核状态(optional 选填)&#125; 定义出参 123456789101112131415161718192021222324252627282930313233[root@localhost sample]$ cat idl/Response.thrift// 定义包名namespace java com.sample.thrift.model/** * 审批状态枚举定义 */enum AuditEnum &#123; SUBMIT = 1, // 提交待审 APPROVED = 2, // 审核通过 REJECT = 3, // 提交驳回 CANCEL = 4 // 提交取消&#125;/** * 数据结果集 */struct SampleView &#123; optional i64 id; // 主键 optional string title; // 标题 optional list&lt;string&gt; tags; // 标签 optional bool online; // 是否上线 optional AuditEnum audit; // 审核状态&#125;/** * 数据状态模型 */struct ApiResult &#123; optional i32 status; // 状态 optional string message; // 描述 optional map&lt;i64,SampleView&gt; data; // 数据&#125; 定义接口1234567891011121314151617181920[root@localhost sample]$ cat idl/ServiceIface.thrift// 定义包名namespace java com.sample.thrift.ifaceinclude "Request.thrift"include "Response.thrift"/** * 定义服务接口 */service SampleService &#123; /** * 定义RPC服务接口 * param query 查询条件 * param page 页码 * param size 页量 */ Response.ApiResult query(1:Request.QueryEntry query, 2: i32 page, 3: i32 size);&#125; 代码生成123456789101112131415161718192021222324[root@localhost sample]$ thrift -versionThrift version 0.11.0[root@localhost sample]$ cat generate.sh#!/bin/bashfor file in `ls idl`do thrift -r -gen java -out ./src idl/$filedone[root@localhost sample]$ sh generate.sh[root@localhost sample]$ tree srcsrc└── com └── sample └── thrift ├── iface │ └── SampleService.java └── model ├── ApiResult.java ├── AuditEnum.java ├── QueryEntry.java └── SampleView.java5 directories, 5 files 基于注解示例上面的IDL生成的方式我们可以使用注解方式进行定义 定义结构体QueryEntry.java 12345678910111213141516171819202122@ThriftStruct@Setter@NoArgsConstructorpublic class QueryEntry &#123; /** * 关键词 */ private String keyword; // 如果有非无参构造函数，需要在构造函数上加上@ThriftConstructor注解， // 一个类只能有一个带有@ThriftConstructor注解的构造函数 @ThriftConstructor public QueryEntry(String keyword) &#123; this.keyword = keyword; &#125; @ThriftField(1) public String getKeyword() &#123; return keyword; &#125;&#125; AuditEnum.java 12345678910111213// 定义枚举@ThriftEnum@Getter@AllArgsConstructorpublic enum AuditEnum &#123; SUBMIT(1), // 提交待审 APPROVED(2), // 审核通过 REJECT(3), // 提交驳回 CANCEL(4); // 提交取消 private int audit;&#125; SampleView.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@ThriftStruct@Getter@NoArgsConstructorpublic class SampleView &#123; /** * 主键 */ private Long id; /** * 标题 */ private String title; /** * 标签 */ private List&lt;String&gt; tags; /** * 是否上线 */ private boolean online; /** * 审核状态 */ private AuditEnum audit; @ThriftField(1) public Long getId() &#123; return id; &#125; @ThriftField(2) public String getTitle() &#123; return title; &#125; @ThriftField(3) public List&lt;String&gt; getTags() &#123; return tags; &#125; @ThriftField(4) public boolean isOnline() &#123; return online; &#125; @ThriftField(5) public AuditEnum getAudit() &#123; return audit; &#125;&#125; ApiResult.java 123456789101112131415161718192021222324252627@ThriftStruct@Setter@NoArgsConstructorpublic class ApiResult &#123; // 状态 private int status; // 描述 private String message; // 数据 private Map&lt;Long, SampleView&gt; data; @ThriftField(value = 1, requiredness = ThriftField.Requiredness.REQUIRED) public int getStatus() &#123; return status; &#125; @ThriftField(2) public String getMessage() &#123; return message; &#125; @ThriftField(3) public Map&lt;Long, SampleView&gt; getData() &#123; return data; &#125;&#125; 定义接口服务1234567891011@ThriftServicepublic interface SampleService &#123; /** * 服务接口 * @return */ @ThriftMethod(exception = &#123;@ThriftException(type = TException.class, id = 1)&#125;) public ApiResult query(QueryEntry query, int page, int size);&#125; 说明: 示例中@Getter、@Setter、@NoArgsConstructor是lombok注解(简化模板代码),非thrift注解. Maven生成插件thrift对应maven生成插件为maven-thrift-plugin(注意：将thrift文件放置在src/main/thrift/目录下) 12345678910111213141516171819202122232425262728293031323334353637&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;compiler-plugin.version&#125;&lt;/version&gt; &lt;configuration&gt; &lt;encoding&gt;$&#123;project.build.sourceEncoding&#125;&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.thrift.tools&lt;/groupId&gt; &lt;artifactId&gt;maven-thrift-plugin&lt;/artifactId&gt; &lt;version&gt;0.1.11&lt;/version&gt; &lt;configuration&gt; &lt;!-- config thrift bin file path --&gt; &lt;thriftExecutable&gt;/usr/local/bin/thrift&lt;/thriftExecutable&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;thrift-sources&lt;/id&gt; &lt;phase&gt;generate-sources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;thrift-test-sources&lt;/id&gt; &lt;phase&gt;generate-test-sources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 运行mvn package后，会自动在target目录下生成java源码及编译后的class 数据传输协议Thrift在传输协议(传输格式)上总体上划分为文本(text)和二进制(binary)传输协议;为节约带宽，提供传输效率, 一般情况下使用二进制类型的传输协议为多数. TBinaryProtocol: 二进制编码格式进行数据传输。 TCompactProtocol: 压缩格式进行数据传输。 TTupleProtoco: 继承于TCompactProtocol，Struct的编解码时使用更省地方但IDL间版本不兼容的TupleScheme. TJSONProtocol: JSON格式编码协议进行数据传输。 TSimpleJSONProtocol: 这种协议只提供JSON只写的协议，适用于通过脚本语言解析 TDebugProtocol: 在开发的过程中帮助开发人员调试用的，使用易懂的可读的文本格式. 说明: 客户端和服务端的协议要一致. 数据传输方式protocol说明的是什么被传送(传送内容格式)，transports说明的是如何传送(传送方式) 一个server只允许定义一个接口服务。这样的话多个接口需要多个server。这样会带来资源的浪费。通常可以通过定义一个组合服务来解决。 TSocket: 采用blocking socket I/O TFileTransport: 以文件(日志)形式进行传输。 TFramedTransport: 以帧的形式发送，每帧前面是一个长度。要求服务器方式为non-blocking server TZlibTransport: 使用zlib进行压缩传输,与其他传输方式联合使用. TMemoryTransport: 使用内存I/O,java实现中在内部使用了ByteArrayOutputStream TNonblockingTransport: 使用JDKNIO的Transport实现,读写的byte[]会每次被wrap成一个ByteBuffer. 序列化和反序列化Thrift序列化时属性标识如下, 没有域的名称,因此与JSON/XML这种序列化工具相比，thrift序列化后生成的文件体积要小很多. 1数字ID + 属性类型TYPE Thrift的向后兼容性需要满足2个条件, 这样无论增加还是删除域，都可以实现向后兼容. 域的序号不能改变(数值) 域的类型不能改变(类型) 服务端类型Thrift提供网络模型有单线程、多线程、事件驱动; 也可划分为：阻塞服务模型、非阻塞服务模型。 阻塞服务模型 TSimpleServer: 简单的单线程服务器，主要用于测试. TThreadPoolServer: 使用标准阻塞式IO的多线程服务器 非阻塞服务模型 TNonblockingServer: 采用NIO的模式, 借助Channel/Selector机制, 采用IO事件模型来处理.唯一可惜的是这个单线程处理. 当遇到handler里有阻塞的操作时, 会导致整个服务被阻塞住.（需使用TFramedTransport数据传输方式） THsHaServer: 同步半异步的服务模型，一个单独的线程用来处理网络I/O，一个worker线程池用来进行消息的处理. TThreadedSelectorServer: 有一条线程专门负责accept，若干条Selector线程处理网络IO，一个Worker线程池处理消息(使用得最多)。 相关文档 http://calvin1978.blogcn.com/articles/apache-thrift.html https://blog.csdn.net/xuemengrui12/article/details/60876260 参考文档 http://www.micmiu.com/soa/rpc/thrift-sample/ https://www.cnblogs.com/cyfonly/p/6059374.html https://www.ibm.com/developerworks/cn/java/j-lo-apachethrift/ http://dongxicheng.org/search-engine/thrift-framework-intro/ https://www.cnblogs.com/mumuxinfei/p/3875165.html http://1csh1.github.io/2017/02/21/Thrift%20IDL%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95]]></content>
      <categories>
        <category>组件框架</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>Thrift</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[版本工具-Git]]></title>
    <url>%2F2015%2F07%2F17%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2F%E7%89%88%E6%9C%AC%E5%B7%A5%E5%85%B7-Git%2F</url>
    <content type="text"><![CDATA[克隆代码到本地123$ git clone https://git.oschina.net/wuyu/wuyu-plugin.git$ cd wuyu-plugin$ git checkout master 已有项目推送仓库12345$ git init$ git add ./*$ git commit -m "init"$ git remote add origin https://git.oschina.net/wuyu/wuyu-plugin.git$ git push -u origin master 说明: https://git.oschina.net/wuyu/wuyu-plugin.git为示例工程目录,个人的工程以自己的路径为准. 创建分之推送123$ git branch develop$ git push origin develop$ git checkout develop 工程提交修改123$ git add ./*$ git commit -m "init"$ git push -u origin master 修改提交Commit 如果这是你最近一次提交并且没有push到远程分支，可用以下命令直接修改: 1[root@localhost vplus (master)]$ git commit --amend -m "your new message" 其他情况可参考 https://stackoverflow.com/questions/179123/how-to-modify-existing-unpushed-commits 修改commit的用户名和邮箱GIT修改当前工程的提交人信息123$ git config user.name "hunts"$ git config user.email "develop@hunts.work" Git修改全局的工程提交人信息12$ git config --global user.name "hunts"$ git config --global user.email "develop@hunts.work" 或则 或者直接在全局配置文件中修改 12345678[hunts@localhost logs ]$ cat ~/.gitconfig [user]name = huntsemail = develop@hunts.work[core]autocrlf = input[http]postBuffer = 524288000 清空git默认的用户名和密码1[hunts@localhost ~]$ git config --local --unset credential.helper Git全局忽略文件有些场景下我们需要忽略Git项目中的某些文件，对于自己参与的项目，在项目下新建.gitignore文件即可。但如果本地项目较多，或者临时维护别人的项目，一般不会单独新建.gitignore文件，这时就可以选择全局性的忽略这些文件. 在当前用户目录下新建.gitignore_global文件 1[root@localhost ~]$ vim ~/.gitignore_global 在.gitignore_global文件中添加要忽略的文件,和.ignore里边格式一致(每行一个,支持正则) Git区分文件名大小写例如: 创建一个文件readme.md，写入内容，提交到线上仓库，然后修改本地文件名为Readme.md，提交，会发现没有变化，无任何提示信息; 其实Git默认对于文件名大小写是不敏感的. 可以通过如下方式设置 配置 Git 使其对文件名大小写敏感 1[root@localhost master]$ git config core.ignorecase false​ 修改本地文件名为大写 1[root@localhost master]$ mv readme.md Readme.md​ 提交修改后的文件, 如果未生效, 请先删除线上仓库中的文件,重新提交 12345[root@localhost master]$ git add Readme.md[root@localhost master]$ git commit -m 'Readme.md'[root@localhost master]$ git push origin master[root@localhost master]$ # 如果提交后没变化，执行该命令，之后再执行上述命令，删除本地Git管理的文件，当成新文件提交[root@localhost master]$ git rm -r --cached readme.md Git修改远程分支仓库A：https://github.com/old/json仓库B：https://gitee.com/new/json 从A切换到B,有3种方式 命令行修改远程地址12[root@localhost ~]$ cd json[root@localhost json]$ git remote set-url origin https://gitee.com/new/json.git 先删除再添加新地址123[root@localhost ~]$ cd json[root@localhost json]$ git remote rm origin[root@localhost json]$ git remote add origin https://gitee.com/new/json.git 修改配置文件12345678910111213141516[root@localhost ~]$ cd json/.git[root@localhost .git]$ vim config[core] repositoryformatversion = 0 filemode = true logallrefupdates = true precomposeunicode = true[remote "origin"] # 修改成新的仓库地址 url = https://gitee.com/new/json.git fetch = +refs/heads/*:refs/remotes/origin/*[branch "master"] remote = origin merge = refs/heads/master Git项目创建Tag查看项目Tag123[root@localhost vplus (master)]$ git tag v1.1.1v1.1.2 创建新的Tag1234567891011[root@localhost vplus (master)]$ git tag v1.1.3 -m '增加工具类,生成实体类打标lombok注解'[root@localhost vplus (master)]$ git push origin v1.1.3Counting objects: 1, done.Writing objects: 100% (1/1), 215 bytes | 0 bytes/s, done.Total 1 (delta 0), reused 0 (delta 0)To https://github.com/dennisit/vplus.git * [new tag] v1.1.3 -&gt; v1.1.3[root@localhost vplus (master)]$ git tagv1.1.1v1.1.2v1.1.3 删除指定tag1234[root@localhost vplus (master)]$ git push origin --delete tag V1.1.3 remote: warning: Deleting a non-existent ref.To https://github.com/dennisit/vplus.git - [deleted] V1.1.3 上面我们通过git push origin v1.1.3推送了我们新创建的tag v1.1.3, 进度git主页查看 如图,可以看到新推送的tag已经展示在列表 发布Release点击上图中的【Draft a new release】进行release tag 创建. 查看发布效果 说明 上面的操作仅仅只是将我们最新更新的tag发布成了release, 如果想让别人直接引用你最细版本的包, 需要将最新更新版本RELEASE到中央仓库中(后续有时间补充如何推送)。 Git命令一览 Github作为图床Github作为图床比较简单,只需要一个地址映射即可. 如下: 原始文件 https://github.com/hunts-work/CardOCR/blob/master/ScreenRecord_20181116181123.gif 图床地址 https://raw.githubusercontent.com/hunts-work/CardOCR/master/ScreenRecord_20181116181123.gif 发布中央仓库 我们可以把自己沉淀的通用工具代码RELEASE到Maven中央仓库, 然后后期直接Maven引入使用, 方便统一管理更新. 创建发布工单创建审核单, 网址: https://issues.sonatype.org/secure/Dashboard.jspa, 工单按照提示填写即可, 工单只有等审核通过后自己包对应的groupId才能进行包推送到中央仓库. 版本包防篡改我们的工程包,安全方面上讲是不允许被非法篡改的,所以我们需要在打包的时候进行签名处理. 安装Gnupg12345678910111213141516[root@localhost ~]# yum install gnupg[root@localhost ~]# gpg --versiongpg (GnuPG) 2.0.22libgcrypt 1.5.3Copyright (C) 2013 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law.Home: ~/.gnupg支持的算法：公钥：RSA, ?, ?, ELG, DSA对称加密：IDEA, 3DES, CAST5, BLOWFISH, AES, AES192, AES256, TWOFISH, CAMELLIA128, CAMELLIA192, CAMELLIA256散列：MD5, SHA1, RIPEMD160, SHA256, SHA384, SHA512, SHA224压缩：不压缩, ZIP, ZLIB, BZIP2 生成密钥1234567891011121314151617181920212223242526272829303132333435363738394041[root@localhost ~]# gpg --list-keys[root@localhost ~]# gpg --gen-keygpg (GnuPG) 2.0.22; Copyright (C) 2013 Free Software Foundation, Inc.This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law.请选择您要使用的密钥种类： (1) RSA and RSA (default) (2) DSA and Elgamal (3) DSA (仅用于签名) (4) RSA (仅用于签名)您的选择？RSA 密钥长度应在 1024 位与 4096 位之间。您想要用多大的密钥尺寸？(2048)您所要求的密钥尺寸是 2048 位请设定这把密钥的有效期限。 0 = 密钥永不过期 &lt;n&gt; = 密钥在 n 天后过期 &lt;n&gt;w = 密钥在 n 周后过期 &lt;n&gt;m = 密钥在 n 月后过期 &lt;n&gt;y = 密钥在 n 年后过期密钥的有效期限是？(0)密钥永远不会过期以上正确吗？(y/n)yYou need a user ID to identify your key; the software constructs the user IDfrom the Real Name, Comment and Email Address in this form: "Heinrich Heine (Der Dichter) &lt;heinrichh@duesseldorf.de&gt;"真实姓名：苏若年电子邮件地址：dennisit@163.com注释：suruonian您正在使用‘utf-8’字符集。您选定了这个用户标识： “苏若年 (suruonian) &lt;dennisit@163.com&gt;”更改姓名(N)、注释(C)、电子邮件地址(E)或确定(O)/退出(Q)？O您需要一个密码来保护您的私钥。我们需要生成大量的随机字节。这个时候您可以多做些琐事(像是敲打键盘、移动鼠标、读写硬盘之类的)，这会让随机数字发生器有更好的机会获得足够的熵数 说明: 上面的步骤如果不需要特别选定直接使用默认的即可,在设定密码过程中会弹出对话框让你设定私钥密码,该密码需要记住后面发布包时候需要用. 查看密钥123456[root@localhost local]# gpg --list-keys/root/.gnupg/pubring.gpg------------------------pub 2048R/30E287D1 2016-02-25uid 苏若年 (suruonian) &lt;dennisit@163.com&gt;sub 2048R/FA4E2A5C 2016-02-25 密钥推送服务器注意: 这里发送的是公钥,和上面密钥列表中对应pub模块对应. 1234567[root@localhost local]# gpg --keyserver hkp://keys.gnupg.net --send-keys 30E287D1gpg: 将密钥‘30E287D1’上传到 hkp 服务器 keys.gnupg.net[root@localhost local]# gpg --keyserver hkp://keys.gnupg.net --recv-keys 30E287D1gpg: 下载密钥‘30E287D1’，从 hkp 服务器 keys.gnupg.netgpg: 密钥 30E287D1：“苏若年 (suruonian) &lt;dennisit@163.com&gt;”未改变gpg: 合计被处理的数量：1gpg: 未改变：1 Maven配置该配置主要用于配置deploy包到oss.sonatype.org时的账号授权信息. 123456789101112131415[root@localhost apache-maven-3.5.2]# tree conf/conf/├── logging│ └── simplelogger.properties├── settings.xml└── toolchains.xml[root@localhost apache-maven-3.5.2]# vim conf/settings.xml[root@localhost apache-maven-3.5.2]# cat conf/settings.xml -n | grep -C 3 un-zone 124 125 &lt;server&gt; 126 &lt;id&gt;sonatype&lt;/id&gt; 127 &lt;username&gt;un-zone&lt;/username&gt; 128 &lt;password&gt;登录oss.sonatype.org的密码&lt;/password&gt; 129 &lt;/server&gt; 130 这里的un-zone为我的oss.sonatype.org账号ID 工程配置引入仓库123456789101112&lt;distributionManagement&gt; &lt;snapshotRepository&gt; &lt;id&gt;sonatype&lt;/id&gt; &lt;name&gt;Sonatype Nexus Snapshots&lt;/name&gt; &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;repository&gt; &lt;id&gt;sonatype&lt;/id&gt; &lt;name&gt;Nexus Release Repository&lt;/name&gt; &lt;url&gt;https://oss.sonatype.org/service/local/staging/deploy/maven2/&lt;/url&gt; &lt;/repository&gt;&lt;/distributionManagement&gt; 注意: 这里的推送地址id和maven的setting.xml中配置的server节点中的id保持一致. 发布插件工程pom.xml文件中增加如下配置: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109&lt;developers&gt; &lt;developer&gt; &lt;id&gt;Elon.Su&lt;/id&gt; &lt;name&gt;苏若年&lt;/name&gt; &lt;email&gt;dennisit@163.com&lt;/email&gt; &lt;timezone&gt;+8&lt;/timezone&gt; &lt;/developer&gt;&lt;/developers&gt;&lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jar-no-fork&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;version&gt;2.10.4&lt;/version&gt; &lt;configuration&gt; &lt;charset&gt;$&#123;project.build.sourceEncoding&#125;&lt;/charset&gt; &lt;docencoding&gt;$&#123;project.build.sourceEncoding&#125;&lt;/docencoding&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.3&lt;/version&gt; &lt;configuration&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;fork&gt;true&lt;/fork&gt; &lt;compilerVersion&gt;$&#123;java.version&#125;&lt;/compilerVersion&gt; &lt;source&gt;$&#123;java.version&#125;&lt;/source&gt; &lt;target&gt;$&#123;java.version&#125;&lt;/target&gt; &lt;encoding&gt;$&#123;project.build.sourceEncoding&#125;&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-gpg-plugin&lt;/artifactId&gt; &lt;version&gt;1.6&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;verify&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;sign&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.19.1&lt;/version&gt; &lt;configuration&gt; &lt;skipTests&gt;true&lt;/skipTests&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt;&lt;/build&gt;&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;release&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-gpg-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt;&lt;/profiles&gt; 执行发布并添加签名 1[root@localhost vplus]# mvn clean deploy -Prelease -Dmaven.test.skip=true -Dgpg.passphrase=密钥生成时设定的私钥密码 包公有化 上面我们将包大到maven仓库中, 但是此时并不能直接引入使用. 需要在https://oss.sonatype.org/中进行包正式发布. 登录https://oss.sonatype.org 选择菜单【Staging Repositories】 搜索自己相关包关键词 选中要正式发布的包依次执行【close】、【release】(【close】会进行包发布合规性检测. 只有各个指标都通过了才能真正的release成功). 参考文档 http://rogerdudler.github.io/git-guide/index.zh.html https://www.iteblog.com/archives/1807.html https://www.jianshu.com/p/5f6135e1925f 廖雪峰Git教程]]></content>
      <categories>
        <category>组件框架</category>
        <category>版本工具</category>
      </categories>
      <tags>
        <tag>GIT</tag>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用应用资源集]]></title>
    <url>%2F2015%2F07%2F15%2F%E5%B8%B8%E7%94%A8%E5%BA%94%E7%94%A8%E8%B5%84%E6%BA%90%E9%9B%86%2F</url>
    <content type="text"><![CDATA[以下资源位网络收集, 排名不分先后. 应用工具 在线JSON查看器 在线文件大小 技术博客 排名不分先后 http://lovestblog.cn/ https://www.souyunku.com/categories/ https://www.hollischuang.com/ http://www.ityouknow.com/ http://www.iocoder.cn/ http://cmsblogs.com http://www.52im.net/ https://www.liaoxuefeng.com/ http://www.ruanyifeng.com/blog/ https://blog.52itstyle.com http://www.jouypub.com/ https://www.cnblogs.com/maybe2030/ http://www.tianxiaobo.com https://www.cnblogs.com/puyangsky http://ifeve.com/doug-lea/ https://www.zhihu.com/people/rednaxelafx/answers(R大) http://calvin1978.blogcn.com/(江南白衣) https://494947.kuaizhan.com/(开涛) http://chuansong.me/account/gh_10a6b96351a9(沈剑) https://www.jianshu.com/p/5ec125fd55f9(占小狼) https://blog.fazero.me/ https://blog.brucefeng.info/ https://www.ctolib.com/java/ http://www.cnblogs.com/zhouhbing/category/620041.html https://www.bbsmax.com/ http://www.52im.net/ 设计资源 http://chuangzaoshi.com/ 技术资源 分布式锁: https://blog.brucefeng.info/post/distributed-locks 问题排查: https://github.com/nereuschen/blog/issues/29 面试笔试: https://github.com/randian666/algorithm-study]]></content>
      <categories>
        <category>资源共享</category>
      </categories>
      <tags>
        <tag>Blog</tag>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis常用操作示例]]></title>
    <url>%2F2015%2F06%2F19%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2FORM%E6%A1%86%E6%9E%B6-Mybatis%2F</url>
    <content type="text"><![CDATA[该篇主要是Mybatis在日常开发中的使用积累, 对于Intellij IDEA 推荐一款Mybatis插件 Free Mybatis Mybatis生成插件 mybatis-generator是一个可以生成mybatis通用代码的maven插件 插件引入123456789101112131415161718192021&lt;build&gt; &lt;plugins&gt; &lt;!-- mybatis 代码生成器插件 mybatis-generator:generate--&gt; &lt;plugin&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;configuration&gt; &lt;verbose&gt;true&lt;/verbose&gt; &lt;overwrite&gt;true&lt;/overwrite&gt; &lt;/configuration&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.37&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 生成命令1mvn mybatis-generator:generate 配置说明工程目录下:src/main/resources/generatorConfig.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC "-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN" "http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd"&gt;&lt;!-- 使用 mvn mybatis-generator:generate --&gt;&lt;generatorConfiguration&gt; &lt;!-- 配置文件 &lt;properties resource="classpath*:property/jdbc.properties" /&gt; --&gt; &lt;context id="dbTestTables" targetRuntime="MyBatis3" defaultModelType="flat"&gt; &lt;!-- plugin --&gt; &lt;plugin type="org.mybatis.generator.plugins.SerializablePlugin" /&gt; &lt;!-- 是否取消生成的代码注释 true : 取消 false: 保留 --&gt; &lt;commentGenerator&gt; &lt;property name="suppressAllComments" value="false" /&gt; &lt;/commentGenerator&gt; &lt;!-- 连接配置 --&gt; &lt;jdbcConnection driverClass="com.mysql.jdbc.Driver" connectionURL="jdbc:mysql://devel.mysql.com:3306/db_test?autoReconnect=true&amp;amp;useUnicode=true&amp;amp;characterEncoding=UTF-8" userId="root" password="rootpwd"&gt; &lt;/jdbcConnection&gt; &lt;!-- javaModelGenerator是模型的生成信息，这里将指定这些Java model类的生成路径 --&gt; &lt;javaModelGenerator targetPackage="com.tutorial.mybatis.gen.domain" targetProject="/Users/elonsu/IdeaProjects/examples/elonsu-batisgen-example/src/main/java"&gt; &lt;property name="enableSubPackages" value="true" /&gt; &lt;property name="trimStrings" value="true" /&gt; &lt;/javaModelGenerator&gt; &lt;!-- 生成Mapper 配置文件 --&gt; &lt;sqlMapGenerator targetPackage="mapper/xml" targetProject="/Users/elonsu/IdeaProjects/examples/elonsu-batisgen-example/src/main/resources"&gt; &lt;property name="enableSubPackages" value="true" /&gt; &lt;/sqlMapGenerator&gt; &lt;!-- javaClientGenerator是应用接口的生成信息 --&gt; &lt;javaClientGenerator targetPackage="com.tutorial.mybatis.gen.mapper" type="XMLMAPPER" targetProject="/Users/elonsu/IdeaProjects/examples/elonsu-batisgen-example/src/main/java"&gt; &lt;property name="enableSubPackages" value="true" /&gt; &lt;/javaClientGenerator&gt; &lt;!-- oracle 使用 schema 对应 用户名称空间 mysql 使用 catalog 对应 数据库, xByExample取掉生成的Example接口相关 --&gt; &lt;table tableName="tb_person" domainObjectName="Person" enableCountByExample="false" enableDeleteByExample="false" enableUpdateByExample="false" selectByExampleQueryId="false" enableSelectByExample="false"&gt; &lt;generatedKey column="id" sqlStatement="MySql" identity="true"/&gt; &lt;/table&gt; &lt;table tableName="tb_order" domainObjectName="Order" enableCountByExample="false" enableDeleteByExample="false" enableUpdateByExample="false" selectByExampleQueryId="false" enableSelectByExample="false"&gt; &lt;generatedKey column="id" sqlStatement="MySql" identity="true"/&gt; &lt;/table&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; Mybatis常用业务SQL示例库表123456789CREATE TABLE `tb_test` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '自增主键', `name` varchar(64) DEFAULT NULL COMMENT '名称', `age` bigint(20) DEFAULT '20', `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '记录建立时间', `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '记录修改时间', PRIMARY KEY (`id`), UNIQUE KEY `name` (`name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='测试表' 示例模型12345678910111213141516@Data@Builder@NoArgsConstructor@AllArgsConstructorpublic class TbTest implements Serializable &#123; private Long id; private String name; private Long age; private Date createTime; private Date updateTime;&#125; 示例操作插入并返回主键接口定义1public int insertFullId(TbTest tbTest); Xml编写123456&lt;insert id="insertFullId" useGeneratedKeys="true" keyProperty="id" parameterType="com.boot.stream.domain.mint.TbTest"&gt;insert into tb_test (id, name, age, create_time, update_time)values (#&#123;id&#125;, #&#123;name&#125;, #&#123;age&#125;,#&#123;createTime&#125;, #&#123;updateTime&#125;)&lt;/insert&gt; 用例测试123456789@Testpublic void insertFullId()&#123; Date now = new Date(); TbTest tbTest = TbTest.builder() .age(20L).name("Elonsu").createTime(now).updateTime(now).build(); tbTestMapper.insertFullId(tbTest); // &#123;"age":20,"createTime":1505443787809,"id":3,"name":"Elonsu","updateTime":1505443787809&#125; System.out.println(JSON.toJSONString(tbTest));&#125; SQL执行123DEBUG:2017-09-15 10:49:48.041[debug] ==&gt; Preparing: insert into tb_test (id, name, age, create_time, update_time) values (?, ?, ?,?, ?)DEBUG:2017-09-15 10:49:48.070[debug] ==&gt; Parameters: null, Elonsu(String), 20(Long), 2017-09-15 10:49:47.809(Timestamp), 2017-09-15 10:49:47.809(Timestamp)DEBUG:2017-09-15 10:49:48.086[debug] &lt;== Updates: 1 执行结果 插入或者更新(记录存在则更新)接口定义1public void batchUpset(@Param("list") List&lt;TbTest&gt; list); Xml编写12345678910111213&lt;insert id="batchUpset" parameterType="java.util.Collection"&gt; insert into tb_test (name, age, create_time, update_time) values &lt;foreach collection="list" item="li" separator=","&gt; &lt;trim prefix="(" suffix=")" suffixOverrides=","&gt; #&#123;li.name&#125;, #&#123;li.age&#125;, #&#123;li.createTime&#125;, #&#123;li.updateTime&#125; &lt;/trim&gt; &lt;/foreach&gt; on duplicate key update age = values(age), update_time = values(update_time)&lt;/insert&gt; 用例测试123456789@Testpublic void batchUpset()&#123; Date now = new Date(); TbTest tbTest1 = TbTest.builder() .age(22L).name("Elonsu").createTime(now).updateTime(now).build(); TbTest tbTest2 = TbTest.builder() .age(23L).name("Dennisit").createTime(now).updateTime(now).build(); tbTestMapper.batchUpset(Lists.newArrayList(tbTest1, tbTest2));&#125; SQL执行123DEBUG:2017-09-15 10:54:03.623[debug] ==&gt; Preparing: insert into tb_test (name, age, create_time, update_time) values ( ?, ?, ?, ? ) , ( ?, ?, ?, ? ) on duplicate key update age = values(age), update_time = values(update_time)DEBUG:2017-09-15 10:54:03.680[debug] ==&gt; Parameters: Elonsu(String), 22(Long), 2017-09-15 10:54:03.212(Timestamp), 2017-09-15 10:54:03.212(Timestamp), Dennisit(String), 23(Long), 2017-09-15 10:54:03.212(Timestamp), 2017-09-15 10:54:03.212(Timestamp)DEBUG:2017-09-15 10:54:03.721[debug] &lt;== Updates: 3 执行结果 批量更新(根据主键更新)接口定义1public void batchUpdate(@Param("list") List&lt;TbTest&gt; list); Xml编写1234567891011121314151617181920&lt;update id="batchUpdate" parameterType="java.util.Collection"&gt; &lt;foreach collection="list" item="li" index="index" open="" close="" separator=";"&gt; update tb_test &lt;set &gt; &lt;if test="li.name != null" &gt; name = #&#123;li.name&#125;, &lt;/if&gt; &lt;if test="li.age != null" &gt; age = #&#123;li.age&#125;, &lt;/if&gt; &lt;if test="li.createTime != null" &gt; create_time = #&#123;li.createTime&#125;, &lt;/if&gt; &lt;if test="li.updateTime != null" &gt; update_time = #&#123;li.updateTime&#125;, &lt;/if&gt; &lt;/set&gt; where id = #&#123;li.id&#125; &lt;/foreach&gt; &lt;/update&gt; 用例测试1234567@Testpublic void batchUpdate()&#123; Date now = new Date(); TbTest tbTest1 = new TbTest(3L, "苏若年", 88L, null, now); TbTest tbTest2 = new TbTest(4L, "墨少白", 99L, null, now); tbTestMapper.batchUpdate(Lists.newArrayList(tbTest1, tbTest2));&#125; SQL执行123DEBUG:2017-09-15 11:00:46.741[debug] ==&gt; Preparing: update tb_test SET name = ?, age = ?, update_time = ? where id = ? ; update tb_test SET name = ?, age = ?, update_time = ? where id = ?DEBUG:2017-09-15 11:00:46.776[debug] ==&gt; Parameters: 苏若年(String), 88(Long), 2017-09-15 11:00:46.675(Timestamp), 3(Long), 墨少白(String), 99(Long), 2017-09-15 11:00:46.675(Timestamp), 4(Long)DEBUG:2017-09-15 11:00:46.797[debug] &lt;== Updates: 1 执行结果 批量插入接口定义1public void batchInsert(@Param("list") List&lt;TbTest&gt; list); Xml编写12345678910&lt;insert id="batchInsert" useGeneratedKeys="true" keyProperty="id" parameterType="java.util.Collection"&gt; insert into tb_test (name, age, create_time, update_time) values &lt;foreach collection="list" item="li" index="index" separator=","&gt; &lt;trim prefix="(" suffix=")" suffixOverrides=","&gt; #&#123;li.name&#125;, #&#123;li.age&#125;, #&#123;li.createTime&#125;, #&#123;li.updateTime&#125; &lt;/trim&gt; &lt;/foreach&gt;&lt;/insert&gt; 用例测试123456789101112131415@Testpublic void batchInsert()&#123; Date now = new Date(); TbTest tbTest1 = TbTest.builder() .age(66L).name("mock1").createTime(now).updateTime(now).build(); TbTest tbTest2 = TbTest.builder() .age(77L).name("mock2").createTime(now).updateTime(now).build(); tbTestMapper.batchInsert(Lists.newArrayList(tbTest1, tbTest2)); // tbTest1: &#123;"age":66,"createTime":1505445581768,"id":8,"name":"mock1","updateTime":1505445581768&#125; System.out.println("tbTest1: " + JSON.toJSONString(tbTest1)); // tbTest2: &#123;"age":77,"createTime":1505445581768,"id":9,"name":"mock2","updateTime":1505445581768&#125; System.out.println("tbTest2: " + JSON.toJSONString(tbTest2));&#125; SQL执行123DEBUG:2017-09-15 11:19:41.823[debug] ==&gt; Preparing: insert into tb_test (name, age, create_time, update_time) values ( ?, ?, ?, ? ) , ( ?, ?, ?, ? ) DEBUG:2017-09-15 11:19:41.862[debug] ==&gt; Parameters: mock1(String), 66(Long), 2017-09-15 11:19:41.768(Timestamp), 2017-09-15 11:19:41.768(Timestamp), mock2(String), 77(Long), 2017-09-15 11:19:41.768(Timestamp), 2017-09-15 11:19:41.768(Timestamp) DEBUG:2017-09-15 11:19:41.882[debug] &lt;== Updates: 2 执行结果 动态表/字段传递接口定义1public TbTest columnDynamic(@Param("column") String column, @Param("val") String s); Xml编写123&lt;select id="columnDynamic" resultType="com.boot.stream.domain.mint.TbTest"&gt; select $&#123;column&#125; from tb_test where $&#123;column&#125; = #&#123;val&#125; &lt;/select&gt; 用例测试12345678@Testpublic void dynamicColumn()&#123; // &#123;"name":"苏若年"&#125; System.out.println(JSON.toJSONString(tbTestMapper.columnDynamic("name", "苏若年"))); // &#123;"age":99&#125; System.out.println(JSON.toJSONString(tbTestMapper.columnDynamic("age", "99")));&#125; SQL执行1234567DEBUG:2017-09-15 11:39:10.585[debug] ==&gt; Preparing: select name from tb_test where name = ? DEBUG:2017-09-15 11:39:10.619[debug] ==&gt; Parameters: 苏若年(String) DEBUG:2017-09-15 11:39:10.656[debug] &lt;== Total: 1 DEBUG:2017-09-15 11:39:10.675[debug] ==&gt; Preparing: select age from tb_test where age = ? DEBUG:2017-09-15 11:39:10.675[debug] ==&gt; Parameters: 99(String) DEBUG:2017-09-15 11:39:10.686[debug] &lt;== Total: 1 MyBatis中使用#和$书写占位符有什么区别说明: 在动态sql解析过程，#{}与 ${}的效果是不一样的, #将传入的数据都当成一个字符串，会对传入的数据自动加上引号；$将传入的数据直接显示生成在SQL中. #{}将传入的参数当成一个字符串，会给传入的参数加一个双引号 ${}将传入的参数直接显示生成在sql中，不会添加引号 #{}能够很大程度上防止sql注入，${}无法防止sql注入 ${}在预编译之前已经被变量替换了，这会存在sql注入的风险 写order by子句的时候应该用${}而不是#{} 接下来展示一个sql注入的示例 接口定义1public List&lt;TbTest&gt; tableDynamic(@Param("tableName") String tableName); Xml编写123&lt;select id="tableDynamic" resultType="com.boot.stream.domain.mint.TbTest" statementType="STATEMENT"&gt; select * from $&#123;tableName&#125; where 1=1&lt;/select&gt; 用例测试1234567@Testpublic void sqlInject()&#123; // [&#123;"age":88,"id":3,"name":"苏若年"&#125;,&#123;"age":99,"id":4,"name":"墨少白"&#125;,&#123;"age":66,"id":8,"name":"mock1"&#125;,&#123;"age":77,"id":9,"name":"mock2"&#125;] System.out.println(JSON.toJSONString(tbTestMapper.tableDynamic("tb_test; delete from tb_test; --"))); // [] System.out.println(JSON.toJSONString(tbTestMapper.tableDynamic("tb_test")));&#125; SQL执行12345678DEBUG:2017-09-15 12:06:49.867[debug] ==&gt; Preparing: select * from tb_test; delete from tb_test; -- where 1=1DEBUG:2017-09-15 12:06:49.868[debug] ==&gt; Parameters:DEBUG:2017-09-15 12:06:49.876[debug] &lt;== Total: 4DEBUG:2017-09-15 12:06:49.876[debug] &lt;== Updates: 4DEBUG:2017-09-15 12:06:49.878[debug] ==&gt; Preparing: select * from tb_test where 1=1DEBUG:2017-09-15 12:06:49.879[debug] ==&gt; Parameters:DEBUG:2017-09-15 12:06:49.882[debug] &lt;== Total: 0 可以看到,因为SQL注入,导致表中所有数据被清除. 解释一下MyBatis中命名空间（namespace）的作用 在大型项目中, 可能存在大量的SQL语句，这时候为每个SQL语句起一个唯一的标识（ID）就变得并不容易了。为了解决这个问题，在MyBatis中，可以为每个映射文件起一个唯一的命名空间，这样定义在这个映射文件中的每个SQL语句就成了定义在这个命名空间中的一个ID。只要我们能够保证每个命名空间中这个ID是唯一的，即使在不同映射文件中的语句ID相同，也不会再产生冲突了。 批量主键查询接口定义1public List&lt;TbTest&gt; selectByIds(@Param("list") List&lt;Long&gt; ids); Xml编写12345678910&lt;select id="selectByIds" resultType="com.boot.stream.domain.mint.TbTest"&gt; SELECT &lt;include refid="sqlColumnList"/&gt; FROM tb_test where id IN &lt;foreach collection="list" item="li" open="(" separator="," close=")"&gt; #&#123;li&#125; &lt;/foreach&gt;&lt;/select&gt; 用例测试12345@Testpublic void selectByIds()&#123; // [&#123;"age":88,"createTime":1505455272000,"id":10,"name":"苏若年","updateTime":1505455274000&#125;,&#123;"age":99,"createTime":1505455272000,"id":11,"name":"墨少白","updateTime":1505455274000&#125;,&#123;"age":99,"createTime":1505455272000,"id":12,"name":"林允儿","updateTime":1505455274000&#125;] System.out.println(JSON.toJSONString(tbTestMapper.selectByIds(Lists.newArrayList(10L, 11L, 12L))));&#125; SQL执行123EBUG:2017-09-15 14:15:32.061[debug] ==&gt; Preparing: SELECT id AS id, name AS name, age AS age, create_time AS createTime, update_time AS updateTime FROM tb_test where id IN ( ? , ? , ? ) DEBUG:2017-09-15 14:15:32.089[debug] ==&gt; Parameters: 10(Long), 11(Long), 12(Long) DEBUG:2017-09-15 14:15:32.123[debug] &lt;== Total: 3 动态条件查询接口定义1public List&lt;TbTest&gt; selectByCondition(@Param("object") TbTest tbTest); Xml编写123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;!-- 根据条件查询 --&gt;&lt;select id="selectByCondition" resultType="com.boot.stream.domain.mint.TbTest"&gt;SELECT &lt;include refid="sqlColumnList"/&gt;FROM tb_test&lt;include refid="sqlQueryCondition"/&gt;&lt;include refid="sqlSortByList"/&gt;&lt;/select&gt;&lt;!-- 表字段集 --&gt;&lt;sql id="sqlColumnList"&gt;&lt;trim suffixOverrides=","&gt; id AS id, name AS name, age AS age, create_time AS createTime, update_time AS updateTime,&lt;/trim&gt;&lt;/sql&gt;&lt;!-- 查询条件 --&gt;&lt;sql id="sqlQueryCondition"&gt;&lt;trim suffixOverrides="," prefixOverrides="AND|OR"&gt; &lt;where&gt; &lt;choose&gt; &lt;when test="object.id &gt; 0"&gt; id = #&#123;object.id&#125; &lt;/when&gt; &lt;otherwise&gt; &lt;if test="object.name != null"&gt; AND name = #&#123;object.name&#125; &lt;/if&gt; &lt;if test="object.age != null"&gt; AND age = #&#123;object.age&#125; &lt;/if&gt; &lt;if test="object.createTime"&gt; AND create_time = #&#123;object.createTime&#125; &lt;/if&gt; &lt;if test="object.updateTime"&gt; AND update_time = #&#123;object.updateTime&#125; &lt;/if&gt; &lt;/otherwise&gt; &lt;/choose&gt; &lt;/where&gt;&lt;/trim&gt;&lt;/sql&gt;&lt;!-- 结果集排序集合 --&gt;&lt;sql id="sqlSortByList"&gt;ORDER BY&lt;trim suffixOverrides=","&gt; id DESC, update_time DESC, create_time DESC, age DESC,&lt;/trim&gt;&lt;/sql&gt; 用例测试12345678@Testpublic void selectByCondition()&#123; // 条件含主键查询:[&#123;"age":88,"createTime":1505455272000,"id":10,"name":"苏若年","updateTime":1505455274000&#125;] System.out.println("条件含主键查询:" + JSON.toJSONString(tbTestMapper.selectByCondition(TbTest.builder().id(10L).build()))); // 条件非主键查询:[&#123;"age":99,"createTime":1505455272000,"id":12,"name":"林允儿","updateTime":1505455274000&#125;,&#123;"age":99,"createTime":1505455272000,"id":11,"name":"墨少白","updateTime":1505455274000&#125;] System.out.println("条件非主键查询:" + JSON.toJSONString(tbTestMapper.selectByCondition(TbTest.builder().age(99L).build())));&#125; SQL执行1234567DEBUG:2017-09-15 14:18:26.630[debug] ==&gt; Preparing: SELECT id AS id, name AS name, age AS age, create_time AS createTime, update_time AS updateTime FROM tb_test WHERE id = ? ORDER BY id DESC, update_time DESC, create_time DESC, age DESC DEBUG:2017-09-15 14:18:26.659[debug] ==&gt; Parameters: 10(Long) DEBUG:2017-09-15 14:18:26.700[debug] &lt;== Total: 1 DEBUG:2017-09-15 14:18:26.723[debug] ==&gt; Preparing: SELECT id AS id, name AS name, age AS age, create_time AS createTime, update_time AS updateTime FROM tb_test WHERE age = ? ORDER BY id DESC, update_time DESC, create_time DESC, age DESC DEBUG:2017-09-15 14:18:26.723[debug] ==&gt; Parameters: 99(Long) DEBUG:2017-09-15 14:18:26.727[debug] &lt;== Total: 2]]></content>
      <categories>
        <category>应用实践</category>
      </categories>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据组件-Databus]]></title>
    <url>%2F2015%2F06%2F19%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2F%E6%95%B0%E6%8D%AE%E7%BB%84%E4%BB%B6-Databus%2F</url>
    <content type="text"><![CDATA[Databus概述Databus是一个低延迟、可靠的、支持事务的、保持一致性的数据变更抓取系统, 使用Java语言实现, 由LinkedIn于2013年开源。Databus通过挖掘数据库日志的方式，将数据库变更实时、可靠的从数据库拉取出来，业务可以通过定制化client实时获取变更并进行其他业务逻辑. 官网地址: https://github.com/linkedin/databus Databus特点 来源独立：Databus支持多种数据来源的变更抓取，包括Oracle和MySQL。 可扩展、高度可用：Databus能扩展到支持数千消费者和事务数据来源，同时保持高度可用性。 事务按序提交：Databus能保持来源数据库中的事务完整性，并按照事务分组和来源的提交顺寻交付变更事件。 低延迟、支持多种订阅机制：数据源变更完成后，Databus能在微秒级内将事务提交给消费者。同时，消费者使用Databus中的服务器端过滤功能，可以只获取自己需要的特定数据。 无限回溯：这是Databus最具创新性的组件之一，对消费者支持无限回溯能力。当消费者需要产生数据的完整拷贝时(比如新的搜索索引)它不会对数据库产生任何额外负担，就可以达成目的, 当消费者的数据大大落后于来源数据库时，也可以使用该功能. Databus结构 上图中介绍了Databus系统的构成，包括中继Relay、bootstrap服务和客户端库。Bootstrap服务中包括Bootstrap Producer和Bootstrap Server。快速变化的消费者直接从Relay中取事件。如果一个消费者的数据更新大幅落后，它要的数据就不在Relay的日志中，而是需要请求 Bootstrap服务，提交给它的，将会是自消费者上次处理变更之后的所有数据变更快照. 模块介绍 Source Databases: MySQL以及Oracle数据源 Relays: 负责抓取和存储数据库变更，全内存存储，也可配置使用mmap内存映射文件方式 Schema Registry: 数据库数据类型到Databus数据类型的一个转换表 Bootstrap Service: 一个特殊的客户端，功能和Relays类似，负责存储数据库变更，主要是磁盘存储 Application: 数据库变更消费逻辑，从Relay中拉取变更，并消费变更 Client Lib: 提供挑选关注变更的API给消费逻辑 Consumer Code: 变更消费逻辑，可以是自身消费或者再将变更发送至下游服务 主要组件Databus的主要由以下四个组件构成. Databus Relay 数据抓取端 从Databus来源读取变更的数据行，并将读取到的数据序列化为Databus变更事件保存到内存缓冲区中. 监听来自Databus客户端(包括Bootstrap Producer)的请求，并传输新的Databus数据变更事件. Databus 数据客户端 检查Relay上新的数据变更事件，并执行特定业务逻辑的回调 如果落后Relay太多，向Bootstrap Server发起查询 新客户端会向Bootstrap Server发起bootstrap启动查询，然后切换到向中继发起查询，以完成最新的数据变更事件 单个客户端可以处理整个Databus数据流，它们还可以作为集群的一部分，处理一小部分流。 Databus Bootstrap Producer 辅助引导程序生产端 它只是一个特殊的客户端 检查Relay上的新数据变更事件 将数据变更事件保存到Mysql数据库 Mysql数据库用于Bootstrap和Clients追溯数据 Databus Bootstrap Server 引导程序服务 监听来自Databus客户端的请求，并为bootstrap和追溯返回一个超长的回溯数据变更事件 参考文档 http://www.importnew.com/22294.html https://www.jianshu.com/p/9df54eb1ec35 http://tech.lede.com/2017/05/23/rd/server/databus/]]></content>
      <categories>
        <category>组件框架</category>
        <category>数据组件</category>
      </categories>
      <tags>
        <tag>Databus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[容器组件-Tomcat]]></title>
    <url>%2F2015%2F05%2F17%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2F%E5%AE%B9%E5%99%A8%E7%BB%84%E4%BB%B6-Tomcat%2F</url>
    <content type="text"><![CDATA[Tomcat简介Tomcat是全世界最著名的基于Java的轻量级应用服务器, 是一款完全开源免费的Servlet容器实现. 同时, 它支持Html、JS等静态资源的处理, 因此又可以作为轻量级Web服务器使用. Tomcat启动脚本配置修改 可以通过编辑$CATALINA_HOME/bin/catalina.sh, 修改Tomcat启动配置: 1JAVA_OPS="-server -Xms1024m -Xmx2048m -XX:PermSize=256m -XX:MaxPermSize=512m" 启动容器 1[unzone@localhost ~ ]$ $CATALINA_HOME/bin/startup.sh Tomcat目录说明 Tomcat新特性 自8.0版本开始, Tomcat支持Servlet3.1、JSP2.3、EL3.0、WebSocket1.1; 并且9.0版本开始支持Servlet4.0 自8.0版本开始, 默认的HTTP、AJP链接器采用NIO、而非Tomcat7以前版本的BIO；并且从8.5开始, 移除了对BIO的支持 自8.0版本开始, 链接器新增支持JDK7的NIO2 自8.0版本开始, 链接器新增支持HTTP/2协议. 在自8.0版本中, Tomcat提供了一套全新的资源实现, 采用单独、一致的方法配置Web应用的附件资源. Tomcat架构Tomcat组成模块 组件说明 Server: 表示整个Servlet容器, 因此Tomcat运行环境中只有唯一一个Servlet实例 Service: Service表示一个或者多个Connector的集合, 这些Connector共享同一个Container来处理其请求, 在同一个Tomcat实例内可以包含任意多个Service实例, 他们彼此独立 Connector: 即Tomcat链接器, 用于监听并转换Socket请求, 同时将读取的Socket请求交由Container处理, 支持不同协议以及不同的I/O方式 Container: Container表示能够执行客户端请求并返回响应的一类对象. 在Tomcat中存在不同级别的容器: Engine、Host、Context、Wrapper Engine: Engine表示整个Servlet引擎. 在Tomcat中, Engine为最高层级的容器对象, 尽管Engine不是直接处理请求的容器, 却是获取目标容器的入口. Host: Host作为一类容器, 表示Servlet引擎(即Engine)中的虚拟机, 与一个服务器的网络名有关, 如域名等. 客户端可以使用这个网络名连接服务器, 这个名称必须要在DNS服务器上注册. Context: Context作为一类容器, 用于表示ServletContext, 在Servlet规范中, 一个ServletContext即表示一个独立的Web应用. Wrapper: Wrapper作为一类容器, 用于表示Web应用中定义的Servlet Executor: 表示Tomcat组件间可以共享的线程池 Tomcat类加载器Java类加载器JVM默认提供3个类加载器, 他们以一种父子树的方式创建, 同时使用委派模式确保应用程序可以通过自身的类加载器加载所有可见的Java类. Bootstrap: 用于加载JVM提供的基础运行类,即位于%JAVA_HOME%/jre/lib目录下的核心类库 Extension: Java提供的一个标准的扩展机制用于加载除核心类库外的Jar包, 即只要复制到指定扩展目录(可以多个)下的Jar, JVM会自动加载(不需要通过-class指定), 默认的扩展目录是%JAVA_HOME%/jre/lib/ext. System: 用于加载环境变量CLASSPATH(不推荐使用)指定目录下的或者-classpath运行参数指定的JAR包, System类加载器通常用于加载应用程序Jar包以及其启动入口类(Tomcat的Bootstrap类即由System类加载器加载) 双亲委派模式Java默认的类加载机制是委派模式, 委派过程如下: 从缓存中加载 如果缓存中没有, 则从父亲加载器中加载 如果父亲加载器中没有, 则从当前类加载器加载 如果没有,则抛出异常 Tomcat类加载器 Tomcat除了每个Web应用的类加载器外, 也提供了3个基础的类加载器和Web应用类加载器. 而且这3个类加载器指向的路径和包列表均可以由catalina.propertis配置 Common: 以System为父类加载器, 是位于Tomcat应用服务器顶层的公用类加载器, 其路径为common.loader, 默认指向$CATALINA_HOME/lib下的包. Catalina: 以Common为父加载器, 是用于加载Tomcat应用服务器的类加载器, 其路径为server.loader, 默认为空, 此时Tomcat使用Common类加载器加载应用服务器. Shared: 以Common为父加载器, 是所有Web应用的父加载器, 其路径为shared.loader, 默认为空, 此时Tomcat使用ommon类加载器作为Web应用的父加载器. Web应用: 以Shared为父加载器, 加载/WEB-INF/classes目录下的未压缩的Class和资源文件以及/WEB-INF/lib目录下的包, 该类加载器只对当前Web应用可见, 对其它Web应用均不可见. Web应用类加载顺序Tomcat提供了delegate属性用于控制是否启用Java委派模式, 默认为false(不启用), 当配置为true时, Tomcat将使用Java默认的委派顺序. delegate=false(默认) Tomcat提供的Web应用类加载器与默认的委派模式稍有不同, 当进行类加载时, 除JVM基础类库外, 它会首先尝试通过当前类加载器加载, 然后才进行委派. 过程如下: 从缓存中加载 如果缓存中没有, 则从JVM的Bootstrap类加载器加载 如果没有, 则从当前加载器加载(按照WEB-INF/classes、WEB-INF/lib的顺序) 如果没有, 则从父类加载器加载, 由于父类加载器采用默认的委派模式, 所以加载顺序为System、Common、Shared。 delegate=true 从缓存中加载 如果没有, 则从JVM的Bootstrap类加载器加载 如果没有, 则从父类加载器加载, 加载顺序为System、Common、Shared。 如果没有, 则从当前加载器加载(按照WEB-INF/classes、WEB-INF/lib的顺序) Tomcat请求处理 过程描述 1、用户点击网页内容，请求被发送到本机端口8080(默认)，被在那里监听的Coyote HTTP/1.1 Connector获得. 2、Connector把该请求交给它所在的Service的Engine来处理，并等待Engine的回应. 3、Engine获得请求localhost/test/index.jsp，匹配所有的虚拟主机Host。 4、Engine匹配到名为localhost的Host(即使匹配不到也把请求交给该Host处理，因为该Host被定义为该Engine的默认主机)名为localhost的Host获得请求/test/index.jsp，匹配它所拥有的所有的Context。Host匹配到路径为/test的Context（如果匹配不到就把该请求交给路径名为“ ”的Context去处理). 5、path=“/test”的Context获得请求/index.jsp，在它的mapping table中寻找出对应的Servlet。Context匹配到URL PATTERN为*.jsp的Servlet,对应于JspServlet类. 6、构造HttpServletRequest对象和HttpServletResponse对象，作为参数调用JspServlet的doGet()或doPost().执行业务逻辑、数据存储等程序。 7、Context把执行完之后的HttpServletResponse对象返回给Host. 8、Host把HttpServletResponse对象返回给Engine. 9、Engine把HttpServletResponse对象返回Connector. 10、Connector把HttpServletResponse对象返回给客户Browser Tomcat集群集群增量会话管理器 DeltaManagerDeltaManager会话管理器是tomcat默认的集群会话管理器，它主要用于集群中各个节点之间会话状态的同步维护. 集群增量会话管理器的职责是将某节点的会话该变同步到集群内其他成员节点上，它属于全节点复制模式，所谓全节点复制是指集群中某个节点的状态变化后需要同步到集群中剩余的节点，非全节点方式可能只是同步到其中某个或若干节点. 集群备份会话管理器 BackupManage全节点复制的网络流量随节点数量增加呈平方趋势增长，也正是因为这个因素导致无法构建较大规模的集群，为了使集群节点能更加大，首要解决的就是数据复制时流量增长的问题，于是tomcat提出了另外一种会话管理方式，每个会话只会有一个备份，它使会话备份的网络流量随节点数量的增加呈线性趋势增长，大大减少了网络流量和逻辑操作，可构建较大的集群. BackupManager并不是基于SessionMessage实现的, 它的本地会话存储采用的是一个有状态的,可复制的Map-LazyReplicatedMap(它采用主从的备份策略), 通过这个Map, BackupManager仅会将会话的增量数据复制到一个备份节点, 集群中的所有节点均知晓该备份节点的位置. 集群替代方案Tomcat集群组件多用于实现会话同步, 但是这种方案不适用于较大规模的集群. 除了通信开销外, 部署架构也显的比较复杂, 实际上, 在搭建负载均衡时, 完全可以将会话集中管理. 当规模较大时, 可以采用数据库(PersistentManager+JDBCStore); 当规模较大时, 可以采用高速缓存替代, 如Redis、Memcached. 此时需要自己实现Tomcat的会话管理器. 当前网上已经有非常多的实现方案,如redis-session-manager. 性能调优性能检测工具ApacheBench、JMeter JVM调优概述Tomcat是一款Java应用, 所以JVM的配置与其运行性能密切相关. JVM优化的重点则集中在内存分配以及GC策略调整上, 因为JVM垃圾回收机制会不同程度地导致程序运行中断. 垃圾回收性能度量 吞吐量: 工作时间(排除GC时间)占总时间的百分比, 工作时间并不仅是程序运行的时间, 还包括内存分配的时间. 暂停: 测量时间段内, 由垃圾回收导致的应用程序停止响应测次数. 垃圾区收集器选择 串行收集器: (Serial Collector), 采用单线程执行所有垃圾回收, 适用于单核的服务器. 并行收集器: (Parallel Collector), 又称吞吐量收集器, 以并行的方式执行年轻代垃圾回收, 它适用于在多处理器或者多线程硬件上运行的数据集为中大型的应用 并发收集器: (Concurrent Collector), 以并发的方式执行大部分垃圾回收工作,以缩短垃圾回收的暂停时间, 它适用于那些数据集为中大型、响应时间优先于吞吐量的应用. CMS收集器: (Concurrent Mark Sweep Collector, 并发标记扫描收集器) 适用于那些更愿意缩短垃圾回收暂停时间并且负担得起与垃圾回收共享处理资源的应用. G1收集器: (Garbage-First Garbage Collector) 适用于大容量内存的多核服务器, 它在满足垃圾回收暂停时间目标的同时, 以最大可能性实现高吞吐量. 并行与并发 并发是指一个处理器同时处理多个任务。 并行是指多个处理器或者是多核的处理器同时处理多个不同的任务。 并发是逻辑上的同时发生，而并行是物理上的同时发生。来个比喻：并发是一个人同时吃三个馒头，而并行是三个人同时吃三个馒头。 JVM相关参数说明JVM通用选项说明 并行收集器性能相关选项 CMS收集器相关选项 G1收集器相关选项 打印输出相关选项 Tomcat配置Tomcat容器相关的配置均在$CATALINA_BASE/conf/server.xml中. 修改链接器的maxConnections属性, 该属性决定了服务器在同一时间接收并处理的最大连接数. 当达到该值后, 服务器接收但不会处理更多的请求, 额外的请求将会被阻塞直到连接数低于maxConnections, 此时,服务器将再次接收并处理新连接. 将tcpNoDelay属性设置为true, 会开启Socket的TCP_NO_DELAY选项, 它会禁用Nagle算法. 该算法永不链接小的缓冲消息, 它会降低通过网络发送数据包的数量, 提升网络传输效率, 但是对于交互式应用(如Web)会增加影响时间. 调整maxKeepAliveRequest属性值, 该属性用于控制HTTP请求的keep-alive行为, 指定了链接被服务器关闭之前可以接受的请求最大数目 修改socketbuffer属性, 调整Socket缓冲区大小, 通过合理调整Socket缓冲器有助于提升服务器性能. 将enableLookups属性设置为false, 禁用request.getRemoteHost的DNS查找功能, 减少查找时间. 关闭自动部署功能(线上), 修改Host元素,将autoDeploy属性设置为false. 上面只是简单列出常见的一些调优参数, 更多的可以根据应用场景来选择调整. 应用性能优化 尽量减少浏览器与服务器通信次数, 对于浏览器触发的远程操作, 尽量由一次调用完成. 尽量减少请求响应数据量. 去除无用数据, 降低网络开销. 尽量推迟创建会话的时机, 对于不需要会话的则尽量不要创建. 不要再会话中存储大对象, 这会导致占用内存过多, 降低服务器性能. 尽量缩短会话的有效期, 能够及时移除无效会话, 降低会话管理成本. 合理定义对象作用域, 以便对象可以及时回收. 采用连接池提升访问性能. 如数据库连接池. 对于极少变更的数据, 可以采用考虑缓存提升查询性能. 最小化应用日志, 或者尽量采用简单的日志格式. 生成环境避免生成大量非重要日志,建议只输出INFO及以上的日志. 参考文档 http://www.importnew.com/27724.html https://blog.csdn.net/wangyangzhizhou/article/details/52125704 《Tomcat架构解析》]]></content>
      <categories>
        <category>组件框架</category>
        <category>容器组件</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
        <tag>理论基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发工具-Intellij IDEA]]></title>
    <url>%2F2015%2F05%2F10%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2F%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7-Intellij%20IDEA%2F</url>
    <content type="text"><![CDATA[本篇主要介绍Java开发工具 - Intellij IDEA的基本操作应用 Java常用IDE介绍IDE(Integrated Development Environment)集成开发环境,即用来辅助编写代码提升效率的开发工具: IntelliJ IDEA(首推) MyEclipse Eclipse IDEA破解: https://juejin.im/entry/5afecda5518825429d1f8cf5 IntelliJ IDEA使用界面展示 安装插件步骤:【IntelliJ IDEA】-【Preferences】-【Plugins】, 界面如图 点击搜索插件,然后选择右侧install即可, 插件安装完需要重启IDE. 推荐插件 Lombok Plugin: 可以简化代码中臃肿的模板代码 Alibaba Java Coding Guidelines : 阿里巴巴代码规范检测插件 Bootstrap 4 &amp; Font awesome : activate-power-mode : 一款特效插件, 在书写代码时会有鼠标特效 CodeGlance: 在编辑代码最右侧，显示一块代码小地图 Gsonformat: 可以根据json代码生成java bean. JsonOnlineViewer: json在线查看 Python: python开发支持插件 Background Image Plus: 编辑器背景设置插件, 安装完后会在【View】菜单中新增子选项【Set Background Image】 Markdown Navigator: Markdown支持插件 Free Mybatis: Mybatis支持插件 Freemarker Support: Freemarker支持插件 IDEA 基础配置工具菜单 Maven 面板: 【View】-【Tool Windows】-【Maven Projects】 Database 面板: 【View】-【Tool Windows】- 【Database】 Terminal 面板: 【View】-【Tool Windows】- 【Terminal】 快捷键设置步骤:【IntelliJ IDEA】-【Preferences】-【keymap】, 笔者选用的快捷键Mac OS X 10.5+ 查找最近访问快捷键: Command + E 查找最近编辑快捷键: Command + Shift + E 跳到指定文件夹操作: 双击Shift键 快速补全行末分号操作: 待补行任意位置使用快捷键Command + Shift + Enter 唤醒内置 RestClient操作: 快捷键 Command + Shift + A 唤起工具项搜索, 键入 Load into Rest Client 访问历史粘贴板操作: 快捷键Command + Shift + V Smart Step Into操作: Debug阶段,使用快捷键Shift + F7,可以选择到底要Debug进入哪一个方法 主题设置步骤: 【IntelliJ IDEA】-【Preferences】-【Appearance &amp; Behavior】-【Appearance】 选择Theme可以进行开发工具的主题设定. 配置Maven步骤:【IntelliJ IDEA】-【Preferences】-【Build,Execution,Deployment】-【Build Tools】-【Maven】, 如下图: 说明: 笔者Maven包路径为/Users/Elonsu/Desktop/apache-maven-3.5.0, 默认的仓库路径在当前用户目录下有个.m2目录下,笔者的为’/Users/Elonsu/.m2/Repository’, 选择自定义路径时记得勾选右侧的Override. 调整开发面板显示Maven辅助面板 步骤: 【View】-【Tool Windowss】-【Maven Projects】勾选 显示工具栏面板 步骤: 【View】-【Toolbar】&amp;【Tool Buttons】&amp;【Navigation Bar】勾选 保存设定为默认 步骤: 【Window】-【Store Current Layout as Default】 集成GIT配置步骤:【IntelliJ IDEA】-【Preferences】-【Build,Execution,Deployment】-【Version Control】-【Git】, 如图: 说明: 选择git可执行路径, 笔者为/usr/bin/git, Windows系统为git安装目录下git.exe文件. Checkout git代码步骤: 【VCS】-【Checkout from Version Control】-【Git】, 如图: 点击Git,会弹出如下提示框,输入代码git仓库地址, 如下图: 这里check的代码为https://github.com/spring-projects/spring-data-redis.git. 点击clone 后, IDE会自动进行代码checkout. 集成JDK配置步骤: 选中工程右键 - 【Open Modules Settings】会弹出如下设置框. 步骤: 【Project Settings】- 【Project】, 如下图: 图中是笔者已经配置好的JDK配置, 对于没有配置过得可以点击Edit进行添加, 如下图: 点击图中+按钮,进行自己安装的JDK路径的设置.]]></content>
      <categories>
        <category>组件框架</category>
        <category>开发工具</category>
      </categories>
      <tags>
        <tag>IDE</tag>
        <tag>Intellij IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载组件-Nginx]]></title>
    <url>%2F2015%2F04%2F17%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2F%E8%B4%9F%E8%BD%BD%E7%BB%84%E4%BB%B6-Nginx%2F</url>
    <content type="text"><![CDATA[正向和反向代理正向代理代理的对象是客户端,反向代理代理的对象是服务端 正向代理 我们常说的代理也就是只正向代理,正向代理是一个位于客户端和最终目标服务器之间的服务器,为了从最终目标服务器取得内容,客户端向代理发送一个请求并指定目标,然后代理向原始服务器转交请求并将获得的内容返回给客户端.客户端必须要进行一些特别的设置才能使用正向代理.他的工作原理就像一个跳板,简单的说,我是一个用户,我访问不了某网站,但是我能访问一个代理服务器这个代理服务器呢,他能访问那个我不能访问的网站,于是我先连上代理服务器,告诉他我需要那个无法访问网站的内容代理服务器去取回来,然后返回给我 代理软件推荐: Cisco AnyConnect 反向代理 反向代理正好相反,反向代理隐藏了真实的服务端,客户端向反向代理服务器发送请求,反向代理服务器会帮我们把请求转发到真实的服务器那里去,再有反向代理服务器将结果转交给客户端.比如我们拨打10086客服电话,可能一个地区的10086客服有几个或者几十个,你永远都不需要关心在电话那头的是哪一个,你关心的是你的问题能不能得到专业的解答,你只需要拨通了10086的总机号码,电话那头总会有人会回答你,只是有时慢有时快而已.那么这里的10086总机号码就是我们说的反向代理.客户不知道真正提供服务人的是谁. 代理软件推荐: Nginx(轻量级的Web服务器、反向代理服务器) Nginx进程模式Nginx支持Single模式和Master + Worker模式.Nginx通常工作在Master + Worker模式下, 启动后会有一个master进程和多个worker进程. 查看CPU总核数12[root@icloud-store ~]# cat /proc/cpuinfo | grep processor | wc -l1 查看CPU个数12[root@icloud-store ~]# cat /proc/cpuinfo | grep "physical id" | sort | uniq | wc -l1 配置Work进程数通过nginx.conf文件中worker_process设置. 12[root@icloud-store ~]# cat /usr/local/nginx/conf/nginx.conf | grep worker_processworker_processes 1; 建议设置为CPU核数,高并发场合可以考虑设置成核数*2 Master&amp;Worker进程作用Master进程作用读取并验证配置文件nginx.conf; 管理worker进程； Worker进程的作用每一个Worker进程都维护一个线程(避免线程切换)处理连接和请求;Worker进程的个数由配置文件决定,一般和CPU个数相关(有利于进程切换),配置几个就有几个Worker进程. Master进程主要用来管理worker进程,包含接收来自外界的信号,向各worker进程发送信号,监控worker进程的运行状态,当worker进程退出后(异常情况下)会自动重新启动新的worker进程.而基本的网络事件,则是放在worker进程中来处理了,多个worker进程之间是对等的,他们同等竞争来自客户端的请求,各进程互相之间是独立的,一个请求,只可能在一个worker进程中处理,一个worker进程,不可能处理其它进程的请求. 示例图: Nginx热部署热部署就是配置文件nginx.conf修改后,不需要停用Nginx,不需要中断请求,就能让配置文件生效.Nginx热部署在修改配置文件nginx.conf后,重新生成新的worker进程,当然会以新的配置进行处理请求,而且新的请求必须都交给新的worker进程,至于老的worker进程,等把那些以前的请求处理完毕后,kill掉即可. 启停参数 -c：使用指定的配置文件而不是conf目录下的nginx.conf. -t：测试配置文件是否有语法错误. -s reload: 重载 -s stop: 停止 1234567891011[root@icloud-store ~]# ps aux | grep nginxroot 86982 0.0 0.0 112664 972 pts/0 S+ 03:56 0:00 grep --color=auto nginxroot 125221 0.0 0.0 74808 1228 ? Ss 2017 0:00 nginx: master process /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.confroot 125222 0.0 0.1 75580 2376 ? S 2017 0:00 nginx: worker process[root@icloud-store ~]# vim /usr/local/nginx/conf/nginx.conf[root@icloud-store ~]# service nginx reloadReloading nginx configuration (via systemctl): [ OK ][root@icloud-store ~]# ps aux | grep nginxroot 15324 0.0 0.1 75624 2356 ? S 05:45 0:00 nginx: worker processroot 15345 0.0 0.0 112664 972 pts/0 R+ 05:46 0:00 grep --color=auto nginxroot 125221 0.0 0.1 75624 2532 ? Ss 2017 0:00 nginx: master process /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf 可以看到我们修改配置文件触发热部署,时master进程的id前后都是15221、而worker进程的id从125222变成了15324. Nginx 负载均衡轮询轮询即Round Robin,根据Nginx配置文件中的顺序,依次把客户端的Web请求分发到不同的后端服务器. 1234567891011121314http&#123; upstream sampleapp &#123; server 192.168.0.107:8080; server 192.168.0.109:8080; &#125; .... server&#123; listen 80; ... server sampleapp; location / &#123; proxy_pass http://sampleapp; &#125; &#125; 最少连接Web请求会被转发到连接数最少的服务器上. 1234567891011121314http&#123; upstream sampleapp &#123; least_conn; server 192.168.0.107:8080; server 192.168.0.109:8080; &#125; .... server&#123; listen 80; ... location / &#123; proxy_pass http://sampleapp; &#125; &#125; IP地址哈希前述的两种负载均衡方案中,同一客户端连续的Web请求可能会被分发到不同的后端服务器进行处理,因此如果涉及到会话Session,那么会话会比较复杂.常见的是基于数据库的会话持久化.要克服上面的难题,可以使用基于IP地址哈希的负载均衡方案.这样的话,同一客户端连续的Web请求都会被分发到同一服务器进行处理. 1234567891011121314http&#123; upstream sampleapp &#123; ip_hash; server 192.168.0.107:8080; server 192.168.0.109:8080; &#125; .... server&#123; listen 80; ... location / &#123; proxy_pass http://sampleapp; &#125; &#125; 基于权重基于权重的负载均衡即Weighted Load Balancing,这种方式下,我们可以配置Nginx把请求更多地分发到高配置的后端服务器上,把相对较少的请求分发到低配服务器 1234567891011121314http&#123; upstream sampleapp &#123; server 192.168.0.107:8080 weight=2; server 192.168.0.108:8080; server 192.168.0.109:8080 weight=1 max_fails=2 fail_timeout=30s; &#125; .... server&#123; listen 80; ... location / &#123; proxy_pass http://sampleapp; &#125; &#125; 上面的例子在服务器地址和端口后weight=2的配置,这意味着,每接收到3个请求,前2个请求会被分发到第一个服务器,第3个请求会分发到第二个服务器,[权重越大命中越高]其它的配置同轮询配置. Nginx事件模型Nginx的worker进程个数与CPU绑定、worker进程内部包含一个线程高效回环处理请求, 这的确有助于效率,但这是不够的, 在高并发场景下，要同时处理那么多的请求, 要知道, 有的请求需要发生IO,可能需要很长时间, 如果等着它,就会拖慢worker的处理速度. Nginx采用了Linux的epoll模型,epoll模型基于事件驱动机制,它可以监控多个事件是否准备完毕,如果OK,那么放入epoll队列中,这个过程是异步的.worker只需要从epoll队列循环处理即可. 相关文档 https://www.zhihu.com/question/22062795 事件模型介绍Nginx的连接处理机制在不同的操作系统会采用不同的I/O模型 在Linux下,Nginx使用epoll的I/O多路复用模型, 在FreeBSD中使用kqueue的I/O多路复用模型, 在solaris中使用/dev/poll方式的I/O多路复用模型, 在Windows中使用的是icop. 时间模型配置12345[root@icloud-store ~]# cat /usr/local/nginx/conf/nginx.conf | grep epoll -2events &#123; use epoll; worker_connections 1024;&#125; use 是一个事件模块指令,用来指定Nginx的工作模式. worker_connections定义每个进程(process)的最大连接数,这个连接包括了所有连接,如代理服务器连接、客户端的连接、实际的并发连接. 1Nginx总并发连接 = worker * worker_connections Nginx应用根据特定IP来实现分流将IP地址的最后一段最后一位为0或2或6的转发至www.y-zone.top来执行, 否则转发至www.y-zone2.top来执行. 12345678910111213141516171819upstream www.y-zone.top &#123; server 192.168.1.100: 8080;&#125;upstream www.y-zone2.top &#123; server 192.168.1.200: 8080;&#125;server &#123; listen 80; server_name www.y-zone.top; location / &#123; if ($remote_addr~ * ^ (.*)\.(.*)\.(.*)\.*[026] $) &#123; proxy_pass http: //www.y-zone.top; break; &#125; proxy_pass http: //www.y-zone2.top; &#125;&#125; 将IP地址前3段为112.18.96.*转发至hi-linux-01.com来执行，否则转发至hi-linux-02.com来执行. 12345678910111213141516171819upstream www.y-zone.top &#123; server 192.168.1.100: 8080;&#125;upstream www.y-zone2.top &#123; server 192.168.1.200: 8080;&#125;server &#123; listen 80; server_name www.y-zone.top; location / &#123; if ($remote_addr ~* ^(112)\.(18)\.(96)\.(.*)$) &#123; proxy_pass http: //www.y-zone.top; break; &#125; proxy_pass http: //www.y-zone2.top; &#125;&#125; 根据指定范围IP来实现分流将IP地址的最后一段为1-100的转发至www.y-zone.top来执行，否则转发至www.y-zone2.top执行 12345678910111213141516171819upstream www.y-zone.top &#123; server 192.168.1.100: 8080;&#125;upstream www.y-zone2.top &#123; server 192.168.1.200: 8080;&#125;server &#123; listen 80; server_name www.y-zone.top; location / &#123; if ($remote_addr ~* ^(.*)\.(.*)\.(.*)\.[1, 100]$) &#123; proxy_pass http: //www.y-zone.top; break; &#125; proxy_pass http: //www.y-zone2.top; &#125;&#125; 根据forwarded地址分流将IP地址的第1段为212开头的访问转发至www.y-zone.top来执行，否则转发至www.y-zone2.top执行. 123456789101112131415161718192021server 192.168.1.100: 8080;&#125;upstream www.y-zone2.top &#123; server 192.168.1.200: 8080;&#125;server &#123; listen 80; server_name www.y-zone.top; location / &#123; if ($http_x_forwarded_for ~* ^(212)\.(.*)\.(.*)\.(.*)$) &#123; proxy_pass http: //www.y-zone.top; break; &#125; proxy_pass http: //www.y-zone2.top; &#125;&#125; if指令的作用 if指令会就检查后面表达式的值是否为真(true)。如果为真则执行后面大括号中的内容; 以下是一些条件表达式的常用比较方法： 变量的完整比较可以使用=或!=操作符 部分匹配可以使用~或~*的正则表达式来表示 ~表示区分大小写 ~*表示不区分大小写(nginx与Nginx是一样的) !~与!~*是取反操作，也就是不匹配的意思 检查文件是否存在使用-f或!-f操作符 检查目录是否存在使用-d或!-d操作符 检查文件、目录或符号连接是否存在使用-e或!-e操作符 检查文件是否可执行使用-x或!-x操作符 正则表达式的部分匹配可以使用括号，匹配的部分在后面可以用$1~$9变量代替 Nginx 301跳转到带www域名前提: 在域名解析中添加 domain.com 和 www.domain.com 指向你的主机IP地址. 方法一: 打开 nginx.conf 文件找到你的server配置段 12345server &#123; listen 80; server_name www.domain.com domain.com; if ($host != 'www.domain.com' ) &#123; rewrite ^/(.*)$ http://www.domain.com/$1 permanent ;&#125; 方法二: 在配置文件里面写两个server，domain.com指向www.domain.com 123456789server &#123; listen 80; server_name www.domain.com;server &#123; server_name domain.com; rewrite ^(.*) http://www.domain.com/$1 permanent;&#125; Nginx启用压缩在nginx.conf文件的http模块新增以下内容 12345gzip on;gzip_min_length 10k; # 设置允许压缩的页面最小字节数gzip_comp_level 6;gzip_vary on;gzip_types text/plain text/css application/javascript application/json application/xml text/xml image/png image/gif image/jpeg; 说明：如果不指定类型，Nginx仍然不会压缩 Nginx开启Http2 https://blog.fazero.me/2017/01/06/upgrate-nginx-and-use-http2/ Nginx 调优配置 https://www.jianshu.com/p/dc8a1f7eabaf Nginx高可用可以使用Keepalived+Nginx实现高可用, Keepalived是一个高可用解决方案,主要是用来防止服务器单点发生故障,可以通过和Nginx配合来实现Web服务的高可用.(其实Keepalived不仅仅可以和Nginx配合,还可以和很多其他服务配合) Keepalived + Nginx实现高可用的思路: 请求不要直接打到Nginx上,应该先通过Keepalived(这就是所谓虚拟IP,VIP) Keepalived应该能监控Nginx的生命状态(提供一个用户自定义的脚本,定期检查Nginx进程状态,进行权重变化,从而实现Nginx故障切换) Keepalived + Nginx高可用搭建 http://seanlook.com/2015/05/18/nginx-keepalived-ha/ https://github.com/souyunku/Linux-Tutorial/blob/master/Nginx-Keepalived-Install-And-Settings.md 参考文档 https://github.com/18965050/NginxHttpServer_3rd/wiki https://juejin.im/post/58846fceb123db7389d2b70e]]></content>
      <categories>
        <category>组件框架</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文档工具-Markdown]]></title>
    <url>%2F2015%2F04%2F17%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2F%E6%96%87%E6%A1%A3%E5%B7%A5%E5%85%B7-Markdown%2F</url>
    <content type="text"><![CDATA[Markdown 的设计哲学 Markdown 的目標是實現「易讀易寫」。不過最需要強調的便是它的可讀性。一份使用 Markdown 格式撰寫的文件應該可以直接以純文字發佈，並且看起來不會像是由許多標籤或是格式指令所構成。Markdown 的語法有個主要的目的：用來作為一種網路內容的寫作用語言。 Markdown 工具个人用过的感觉最好用的Markdown编辑器: Mweb 本文约定如果有写 效果如下：， 在 MWeb 编辑状态下只有用 CMD + R 预览才可以看效果。 标题Markdown 语法： 123# 第一级标题 `&lt;h1&gt;`## 第二级标题 `&lt;h2&gt;`###### 第六级标题 `&lt;h6&gt;` 强调Markdown 语法： 12345*这些文字会生成`&lt;em&gt;`*_这些文字会生成`&lt;u&gt;`_**这些文字会生成`&lt;strong&gt;`**__这些文字会生成`&lt;strong&gt;`__ 在 MWeb 中的快捷键为： CMD + U、CMD + I、CMD + B效果如下： 这些文字会生成&lt;em&gt;这些文字会生成&lt;u&gt; 这些文字会生成&lt;strong&gt;这些文字会生成&lt;strong&gt; 换行四个及以上空格加回车。如果不想打这么多空格，只要回车就为换行，请勾选：Preferences - Themes - Translate newlines to &lt;br&gt; tags 列表无序列表Markdown 语法： 1234* 项目一 无序列表 `* + 空格键`* 项目二 * 项目二的子项目一 无序列表 `TAB + * + 空格键` * 项目二的子项目二 在 MWeb 中的快捷键为： Option + U效果如下： 项目一 无序列表 * + 空格键 项目二 项目二的子项目一 无序列表 TAB + * + 空格键 项目二的子项目二 有序列表Markdown 语法： 123451. 项目一 有序列表 `数字 + . + 空格键`2. 项目二3. 项目三 1. 项目三的子项目一 有序列表 `TAB + 数字 + . + 空格键` 2. 项目三的子项目二 效果如下： 项目一 有序列表 数字 + . + 空格键 项目二 项目三 项目三的子项目一 有序列表 TAB + 数字 + . + 空格键 项目三的子项目二 任务列表（Task lists）Markdown 语法： 12- [ ] 任务一 未做任务 `- + 空格 + [ ]`- [x] 任务二 已做任务 `- + 空格 + [x]` 效果如下： [ ] 任务一 未做任务 - + 空格 + [ ] [x] 任务二 已做任务 - + 空格 + [x] 图片Markdown 语法： 12![GitHub set up](http://zh.mweb.im/asset/img/set-up-git.gif)格式: ![Alt Text](url) Control + Shift + I 可插入Markdown语法。如果是 MWeb 的文档库中的文档，还可以用拖放图片、CMD + V 粘贴、CMD + Option + I 导入这三种方式来增加图片。效果如下： 链接Markdown 语法： 123email &lt;example@example.com&gt;[GitHub](http://github.com)自动生成连接 &lt;http://www.github.com/&gt; Control + Shift + L 可插入Markdown语法。如果是 MWeb 的文档库中的文档，拖放或CMD + Option + I 导入非图片时，会生成连接。效果如下： Email 连接： &#x65;&#x78;&#x61;&#x6d;&#112;&#108;&#x65;&#64;&#101;&#x78;&#97;&#109;&#112;&#108;&#x65;&#x2e;&#99;&#x6f;&#x6d;连接标题Github网站自动生成连接像： http://www.github.com/ 这样 区块引用Markdown 语法： 123某某说:&gt; 第一行引用&gt; 第二行费用文字 CMD + Shift + B 可插入Markdown语法。效果如下： 某某说: 第一行引用第二行费用文字 行内代码Markdown 语法： 1像这样即可：`&lt;addr&gt;` `code` CMD + K 可插入Markdown语法。效果如下： 像这样即可：&lt;addr&gt; code 多行或者一段代码Markdown 语法： 123456function fancyAlert(arg) &#123; if(arg) &#123; $.facebox(&#123;div:'#foo'&#125;) &#125;&#125; CMD + Shift + K 可插入Markdown语法。效果如下： 123456function fancyAlert(arg) &#123; if(arg) &#123; $.facebox(&#123;div:'#foo'&#125;) &#125;&#125; 顺序图或流程图Markdown 语法： 123张三-&gt;李四: 嘿，小四儿, 写博客了没?Note right of 李四: 李四愣了一下，说：李四--&gt;张三: 忙得吐血，哪有时间写。 12345678st=&gt;start: 开始e=&gt;end: 结束op=&gt;operation: 我的操作cond=&gt;condition: 确认？st-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op 效果如下（ Preferences - Themes - Enable sequence &amp; flow chart 才会看到效果 ）： 123张三-&gt;李四: 嘿，小四儿, 写博客了没?Note right of 李四: 李四愣了一下，说：李四--&gt;张三: 忙得吐血，哪有时间写。 12345678st=&gt;start: 开始e=&gt;end: 结束op=&gt;operation: 我的操作cond=&gt;condition: 确认？st-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op 更多请参考：http://bramp.github.io/js-sequence-diagrams/, http://adrai.github.io/flowchart.js/ 表格Markdown 语法： 1234第一格表头 | 第二格表头--------- | -------------内容单元格 第一列第一格 | 内容单元格第二列第一格内容单元格 第一列第二格 多加文字 | 内容单元格第二列第二格 效果如下： 第一格表头 第二格表头 内容单元格 第一列第一格 内容单元格第二列第一格 内容单元格 第一列第二格 多加文字 内容单元格第二列第二格 删除线Markdown 语法： 加删除线像这样用： ~~删除这些~~ 效果如下： 加删除线像这样用： 删除这些 分隔线以下三种方式都可以生成分隔线： *** ***** - - - 效果如下： MathJaxMarkdown 语法： 12345678块级公式：$$ x = \dfrac&#123;-b \pm \sqrt&#123;b^2 - 4ac&#125;&#125;&#123;2a&#125; $$\\[ \frac&#123;1&#125;&#123;\Bigl(\sqrt&#123;\phi \sqrt&#123;5&#125;&#125;-\phi\Bigr) e^&#123;\frac25 \pi&#125;&#125; =1+\frac&#123;e^&#123;-2\pi&#125;&#125; &#123;1+\frac&#123;e^&#123;-4\pi&#125;&#125; &#123;1+\frac&#123;e^&#123;-6\pi&#125;&#125;&#123;1+\frac&#123;e^&#123;-8\pi&#125;&#125; &#123;1+\ldots&#125; &#125; &#125; &#125; \\]行内公式： $\Gamma(n) = (n-1)!\quad\forall n\in\mathbb N$ 效果如下（Preferences - Themes - Enable MathJax 才会看到效果）： 块级公式：$$ x = \dfrac{-b \pm \sqrt{b^2 - 4ac}}{2a} $$ \[ \frac{1}{\Bigl(\sqrt{\phi \sqrt{5}}-\phi\Bigr) e^{\frac25 \pi}} =1+\frac{e^{-2\pi}} {1+\frac{e^{-4\pi}} {1+\frac{e^{-6\pi}}{1+\frac{e^{-8\pi}} {1+\ldots} } } } \] 行内公式： $\Gamma(n) = (n-1)!\quad\forall n\in\mathbb N$ 脚注（Footnote）Markdown 语法： 1这是一个脚注：[^sample_footnote] 效果如下： 这是一个脚注：^sample_footnote 注释和阅读更多 Actions-&gt;Insert Read More Comment 或者 Command + . 注 阅读更多的功能只用在生成网站或博客时。 TOCMarkdown 语法： 1[TOC] 效果如下： [TOC]]]></content>
      <categories>
        <category>组件框架</category>
        <category>文档工具</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通用中间件运维部署]]></title>
    <url>%2F2015%2F04%2F10%2F%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5%2F%E9%80%9A%E7%94%A8%E4%B8%AD%E9%97%B4%E4%BB%B6%E8%BF%90%E7%BB%B4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[本篇主要讲述开发过程中常见的组件的部署与配置 Node安装 Node.js是一个基于Chrome V8引擎的JavaScript运行环境; 是运行在服务端的 JavaScript. 1234567[root@icloud-store export]# curl -sL https://rpm.nodesource.com/setup_9.x | bash -[root@icloud-store export]# yum install -y gcc-c++ make[root@icloud-store export]# yum install -y nodejs[root@icloud-store export]# node -vv9.3.0[root@icloud-store export]# npm -v5.5.1 Hexo安装 Hexo是一个简单地、轻量地、基于Node的一个静态博客框架,可以方便的生成静态网页托管(Hexo依赖NodeJs) Hexo安装实例123localhost:~ elson$ sudo npm install -g npmlocalhost:~ elson$ sudo npm install -g hexo --no-optionallocalhost:~ elson$ hexo -V Hexo插件安装123456789101112localhost:~ elson$ npm install hexo-generator-index --savelocalhost:~ elson$ npm install hexo-generator-archive --savelocalhost:~ elson$ npm install hexo-generator-category --savelocalhost:~ elson$ npm install hexo-generator-tag --savelocalhost:~ elson$ npm install hexo-server --savelocalhost:~ elson$ npm install hexo-deployer-git --savelocalhost:~ elson$ npm install hexo-renderer-marked@0.2 --savelocalhost:~ elson$ npm install hexo-renderer-stylus@0.2 --savelocalhost:~ elson$ npm install hexo-generator-feed@1 --savelocalhost:~ elson$ npm install hexo-generator-sitemap@1 --savelocalhost:~ elson$ npm install hexo-renderer-jade --savelocalhost:~ elson$ npm install hexo-renderer-sass --save Hexo创建项目在工程目录下(手动创建)执行hexo init 即可完成工程初始化 12345localhost:wuyu-platform elson$ mkdir -p /Users/elson/wuyu-platform/wuyu-platform-hexolocalhost:wuyu-platform elson$ cd /Users/elson/wuyu-platform/wuyu-platform-hexolocalhost:wuyu-platform elson$ hexo initlocalhost:wuyu-platform-hexo elson$ ls_config.yml db.json node_modules package.json public scaffolds source themes Hexo启动项目Hexo启动使用命令hexo s, 也可以使用hexo s -o启动并直接打开应用页面123localhost:wuyu-platform-hexo elson$ hexo s -oINFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. Hexo静态化项目Hexo静态化使用命令hexo g, 静态化后的文件在工程的public目录下 1234567891011localhost:wuyu-platform-hexo elson$ hexo gINFO Start processingINFO Files loaded in 178 msINFO Generated: sitemap.xmlINFO Generated: atom.xmlINFO Generated: 2016/11/27/hello-world/index.htmlINFO Generated: archives/index.htmlINFO Generated: archives/2016/index.htmlINFO Generated: index.htmlINFO Generated: archives/2016/11/index.htmlINFO 7 files generated in 166 ms Hexo主题扩展推荐主题 https://github.com/iissnan/hexo-theme-next https://github.com/tufu9441/maupassant-hexo https://github.com/Haojen/hexo-theme-Anisina 主题扩展 在工程目录下执行git clone 主题地址 或者下载到工程目录下的themes下 修改工程目录下的_config.yml文件, 配置节点theme: landscape 为要扩展的主题 1localhost:wuyu-platform-hexo elson$ git clone https://github.com/iissnan/hexo-theme-next themes/next Hexo推送GitGit推送配置(修改工程目录下的_config.yml文件,配置节点deploy: ) 1234deploy: type: git repo: https://git.oschina.net/elson/HexoDoc.git branch: master GIT推送命令部署 1localhost:wuyu-platform-hexo elson$ hexo deploy Hexo安装问题安装过程遇到的问题解决,问题列表. JDK 安装 JDK(Java Development Kit)是Java语言的软件开发工具包.它包含了JAVA的运行环境(JVM + Java系统类库)和JAVA工具. 移除自带的OpenJDKCentOs默认可能安装了openjdk,这里我们采用oracle官网的jdk,所以需要移除openjdk. 1234567891011[root@localhost ~]# rpm -qa | grep jdkjava-1.7.0-openjdk-headless-1.7.0.121-2.6.8.0.el7_3.x86_64java-1.8.0-openjdk-headless-1.8.0.121-0.b13.el7_3.x86_64java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64java-1.7.0-openjdk-1.7.0.121-2.6.8.0.el7_3.x86_64copy-jdk-configs-1.2-1.el7.noarch[root@localhost ~]# rpm -e --nodeps java-1.7.0-openjdk-headless-1.7.0.121-2.6.8.0.el7_3.x86_64[root@localhost ~]# rpm -e --nodeps java-1.8.0-openjdk-headless-1.8.0.121-0.b13.el7_3.x86_64[root@localhost ~]# rpm -e --nodeps java-1.8.0-openjdk-1.8.0.121-0.b13.el7_3.x86_64[root@localhost ~]# rpm -e --nodeps java-1.7.0-openjdk-1.7.0.121-2.6.8.0.el7_3.x86_64[root@localhost ~]# rpm -e --nodeps copy-jdk-configs-1.2-1.el7.noarch 安装Oracle官方JDK下载最新版本的JDK,解压到特定目录,笔者解压到/usr/local/jdk1.8.0_121, 然后配置环境变量(修改/etc/profile文件,在文件末尾追加java环境变量设置信息) 1234[root@localhost jdk1.8.0_121]# java -versionjava version "1.8.0_121"Java(TM) SE Runtime Environment (build 1.8.0_121-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode) /etc/profile配置文件末尾追加内容如下(追加完source /etc/profile使配置生效) 123456# set java environmentJAVA_HOME=/usr/local/jdk1.8.0_121JRE_HOME=/usr/local/jdk1.8.0_121/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport JAVA_HOME JRE_HOME CLASS_PATH PATH Nginx安装 Nginx是一个高性能的HTTP和反向代理服务器,也是一个IMAP/POP3/SMTP服务器; 官网地址:http://nginx.org/ Nginx安装示例12345678910111213141516171819202122[root@localhost data]# yum install zlib zlib-devel openssl openssl-devel pcre-devel gcc gcc-c++ autoconf automake pcre-devel gd-devel GeoI GeoIP-data GeoIP-devel[root@localhost data]# wget wget https://nginx.org/download/nginx-1.12.2.tar.gz[root@localhost data]# tar -zxvf nginx-1.12.2.tar.gz[root@localhost data]# cd nginx-1.12.2[root@localhost nginx-1.12.2]# ./configure --prefix=/usr/local/nginx --sbin-path=/usr/local/nginx/sbin/nginx --conf-path=/usr/local/nginx/conf/nginx.conf --http-log-path=/usr/local/nginx/logs/access.log --error-log-path=/usr/local/nginx/logs/error.log --pid-path=/usr/local/nginx/logs/nginx.pid --lock-path=/usr/local/nginx/lock/nginx.lock \--http-client-body-temp-path=/usr/local/nginx/client_body_temp \--http-proxy-temp-path=/usr/local/nginx/proxy_temp \--http-fastcgi-temp-path=/usr/local/nginx/fastcgi-temp \--http-uwsgi-temp-path=/usr/local/nginx/uwsgi-temp \--http-scgi-temp-path=/usr/local/nginx/scgi-temp \--user=root --group=root \--with-http_v2_module \--with-http_gzip_static_module \--with-http_stub_status_module \--with-http_auth_request_module \--with-http_realip_module \--with-http_geoip_module \--with-http_ssl_module \--with-http_flv_module \--with-http_mp4_module \--with-http_image_filter_module[root@localhost nginx-1.9.12]# make &amp; make install Nginx安装验证123456[root@localhost data]# /usr/local/nginx/sbin/nginx -Vnginx version: nginx/1.12.2built by gcc 4.4.7 20120313 (Red Hat 4.4.7-16) (GCC)built with OpenSSL 1.0.1e-fips 11 Feb 2013TLS SNI support enabledconfigure arguments: --prefix=/usr/local/nginx --sbin-path=/usr/local/nginx/sbin/nginx --conf-path=/usr/local/nginx/conf/nginx.conf --http-log-path=/usr/local/nginx/logs/access.log --error-log-path=/usr/local/nginx/logs/error.log --pid-path=/usr/local/nginx/logs/nginx.pid --lock-path=/usr/local/nginx/lock/nginx.lock --http-client-body-temp-path=/usr/local/nginx/client_body_temp --http-proxy-temp-path=/usr/local/nginx/proxy_temp --http-fastcgi-temp-path=/usr/local/nginx/fastcgi-temp --http-uwsgi-temp-path=/usr/local/nginx/uwsgi-temp --http-scgi-temp-path=/usr/local/nginx/scgi-temp --user=root --group=root --with-http_v2_module --with-http_gzip_static_module --with-http_stub_status_module --with-http_auth_request_module --with-http_realip_module --with-http_geoip_module --with-http_ssl_module --with-http_flv_module --with-http_mp4_module --with-http_image_filter_module Nginx启停脚本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879[root@localhost init.d]# service nginxUsage: /etc/init.d/nginx &#123;start|stop|reload|restart|configtest&#125;[root@localhost init.d]# cat nginx#!/bin/bash## chkconfig: - 85 15# description: nginx is a World Wide Web server. It is used to serve# Source Function Library. /etc/init.d/functions# Nginx SettingsNGINX_SBIN="/usr/local/nginx/sbin/nginx"NGINX_CONF="/usr/local/nginx/conf/nginx.conf"NGINX_PID="/usr/local/nginx/logs/nginx.pid"RETVAL=0prog="Nginx"start() &#123; echo -n $"Starting $prog: " mkdir -p /dev/shm/nginx_temp daemon $NGINX_SBIN -c $NGINX_CONF RETVAL=$? echo return $RETVAL&#125;stop() &#123; echo -n $"Stopping $prog: " killproc -p $NGINX_PID $NGINX_SBIN -TERM rm -rf /dev/shm/nginx_temp RETVAL=$? echo return $RETVAL&#125;reload()&#123; echo -n $"Reloading $prog: " killproc -p $NGINX_PID $NGINX_SBIN -HUP RETVAL=$? echo return $RETVAL&#125;restart()&#123; stop start&#125;configtest()&#123; $NGINX_SBIN -c $NGINX_CONF -t return 0&#125;case "$1" in start) start ;; stop) stop ;; reload) reload ;; restart) restart ;; configtest) configtest ;; *) echo $"Usage: $0 &#123;start|stop|reload|restart|configtest&#125;" RETVAL=1esacexit $RETVAL[root@localhost sbin]# service nginx restart[root@localhost sbin]# ps -ef | grep nginx Nginx配置说明相关文档: https://blog.52itstyle.com/archives/557/ Nginx平滑升级相关文档: https://www.centos.bz/2017/07/nginx-upgrade-latest-version/ Mysql安装 MySQL是最流行的关系型数据库管理系统. Yum安装MysSQL12345678910[root@localhost local]# yum list | grep mysql[root@localhost local]# yum install mysql-server mysql-devel mysql[root@localhost local]# service mysqld start[root@localhost local]# vim /etc/my.cnf＃创建root用户[root@localhost local]# mysqladmin -u root password 123456[root@localhost local]# service mysqld restart停止 mysqld： [确定]正在启动 mysqld： [确定][root@localhost local]# mysql -uroot -p123root 使用官方仓库安装Yum安装的版本可能比较低,要安装最新版的话,可以采用MySQL官方仓库,仓库地址: http://repo.mysql.com/安装启动后,会随机生成一个默认密码,该密码在首次登录后,会强制提示用户更新;默认密码可以在mysql的日志文件中查看. 1234567891011121314151617181920212223242526[[root@localhost ~]# wget http://repo.mysql.com/mysql80-community-release-el7.rpm [root@localhost ~]# rpm -ivh mysql80-community-release-el7.rpm [root@localhost ~]# yum install mysql-server [root@localhost ~]# service mysqld start Redirecting to /bin/systemctl start mysqld.service [root@localhost ~]# grep "password" /var/log/mysqld.log 2018-04-21T03:26:58.008783Z 5 [Note] [MY-010454] [Server] A temporary password is generated for root@localhost: Ee3EqGyLOf+s [root@localhost ~]# mysql -uroot -pEe3EqGyLOf+s mysql&gt; select version(); ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement. mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY '123root'; Query OK, 0 rows affected (0.06 sec) mysql&gt; flush privileges; Query OK, 0 rows affected (0.01 sec) mysql&gt; select version(); +-----------+ | version() | +-----------+ | 8.0.11 | +-----------+ 1 row in set (0.00 sec) mysql&gt; exit Bye [root@localhost ~]# mysql -uroot -p123root MySQL开机启动通过chkconfig配置MySQL开机启动. 123456[root@localhost local]# chkconfig --add mysqld[root@localhost local]# chkconfig --list | grep mysqldmysqld 0:关闭 1:关闭 2:关闭 3:关闭 4:关闭 5:关闭 6:关闭[root@localhost local]# chkconfig mysqld on[root@localhost local]# chkconfig --list | grep mysqldmysqld 0:关闭 1:关闭 2:启用 3:启用 4:启用 5:启用 6:关闭 MySQL授权访问示例: 授权dennisit以rmsqlpwd从任意远程端连接到mysql,特定ip限制讲％设置为限制的ip地址即可. 12345678910111213mysql&gt; CREATE USER 'user1'@'%' IDENTIFIED BY 'user1pwd';Query OK, 0 rows affected (0.04 sec)-- 授权所有表mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'user1'@'%' WITH GRANT OPTION;Query OK, 0 rows affected (0.06 sec)mysql&gt; CREATE USER 'user2'@'192.168.0.100' IDENTIFIED BY 'user2pwd';Query OK, 0 rows affected (0.04 sec)-- 授权指定表mysql&gt; GRANT ALL PRIVILEGES ON tb_test.* TO 'user2'@'192.168.0.100' WITH GRANT OPTION;Query OK, 0 rows affected (0.06 sec) 授权后,远程端访问 1[root@localhost ~]# mysql -h172.28.224.34 -uuser1 -puser1pwd Elastic安装 Elastic是一个开源的分布式全文索引组件 12345678[root@localhost local]$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.0.tar.gz[root@localhost local]# tar -zxvf elasticsearch-6.3.0.tar.gz [root@localhost local]# elasticsearch-6.3.0/bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.0/elasticsearch-analysis-ik-6.3.0.zip[root@localhost local]# elasticsearch-6.3.0/bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-pinyin/releases/download/v6.3.0/elasticsearch-analysis-pinyin-6.3.0.zip[root@localhost local]# elasticsearch-6.3.0/bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-stconvert/releases/download/v6.3.0/elasticsearch-analysis-stconvert-6.3.0.zip[root@localhost local]# chown wuyu:wuyu elasticsearch-6.3.0 -R[root@localhost local]# su wuyu[wuyu@localhost local]$ elasticsearch-6.3.0/bin/elasticsearch &amp; X-Pack生成授权账号 交互创建 1[root@localhost local]$ elasticsearch-6.3.0/bin/elasticsearch-setup-passwords auto 自动生成 1[root@localhost local]$ elasticsearch-6.3.0/bin/elasticsearch-setup-passwords auto Redis安装 Redis是一个高性能的key-value数据库. Redis安装12345[root@localhost download]# wget http://download.redis.io/releases/redis-stable.tar.gz[root@localhost download]# tar -zxvf redis-stable.tar.gz[root@localhost download]# cd redis-stable[root@localhost redis-stable]# make MALLOC=libc PREFIX=/usr/local/redis install[root@localhost redis-stable]# cp ./redis.conf /usr/local/redis/ Redis配置修改/usr/local/redis/redis.conf配置文件: 1234567891011121314151617# IP绑定模式bind 0.0.0.0# 服务端口设置port 6379# 以守护进程的方式运行daemonize yes# 当客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能timeout 3000# 进程文件编号pidfile /var/run/redis.pid# 访问授权密码requirepass #2pwd4 Redis启停脚本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970[root@localhost document]# vim /etc/init.d/redis[root@localhost document]# cat /etc/init.d/redis#!/bin/sh## redis Startup script for Redis Server## chkconfig: - 80 12# description: Redis is an open source, advanced key-value store.## processname: redis-server# config: /etc/redis.conf# pidfile: /var/run/redis.pidsource /etc/init.d/functionsBIN="/usr/local/redis/bin"CONFIG="/usr/local/redis/redis.conf"PIDFILE="/var/run/redis.pid"### Read configuration[ -r "$SYSCONFIG" ] &amp;&amp; source "$SYSCONFIG"RETVAL=0prog="redis-server"desc="Redis Server"start() &#123; if [ -e $PIDFILE ];then echo "$desc already running...." exit 1 fi echo -n $"Starting $desc: " daemon $BIN/$prog $CONFIG RETVAL=$? echo [ $RETVAL -eq 0 ] &amp;&amp; touch /var/lock/subsys/$prog return $RETVAL&#125;stop() &#123; echo -n $"Stop $desc: " killproc $prog RETVAL=$? echo [ $RETVAL -eq 0 ] &amp;&amp; rm -f /var/lock/subsys/$prog $PIDFILE return $RETVAL&#125;restart() &#123; stop start&#125;case "$1" instart) start ;;stop) stop ;;restart) restart ;;condrestart) [ -e /var/lock/subsys/$prog ] &amp;&amp; restart RETVAL=$? ;;status) status $prog RETVAL=$? ;;*) echo $"Usage: $0 &#123;start|stop|restart|condrestart|status&#125;" RETVAL=1esacexit $RETVAL[root@localhost init.d]# chmod +x redis[root@localhost redis-stable]# service redis start Redis开机启动1234567[root@localhost redis-stable]# chkconfig --list | grep redis[root@localhost redis-stable]# chkconfig --add redis[root@localhost redis-stable]# chkconfig --list | grep redisredis 0:关 1:关 2:关 3:关 4:关 5:关 6:关[root@localhost redis-stable]# chkconfig redis on[root@localhost redis-stable]# chkconfig --list | grep redisredis 0:关 1:关 2:开 3:开 4:开 5:开 6:关 Redis授权访问123456789[root@localhost redis-stable]# /usr/local/redis/bin/redis-cli -a '111'127.0.0.1:6379&gt; set a 1(error) NOAUTH Authentication required.127.0.0.1:6379&gt; exit[root@localhost redis-stable]# /usr/local/redis/bin/redis-cli -a '#2pwd4'127.0.0.1:6379&gt; set a 1OK127.0.0.1:6379&gt; get a"1" Mongo安装 Mongo是一个基于分布式文件存储的数据库. Mongo安装1234567891011121314[root@localhost cloud]# wget 'https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel62-3.4.2.tgz'[root@localhost cloud]# tar -zxvf mongodb-linux-x86_64-rhel62-3.4.2.tgz[root@localhost cloud]# mv mongodb-linux-x86_64-rhel62-3.4.2 mongo[root@localhost cloud]# mongo/bin/mongod --help[root@localhost cloud]# cd mongo[root@localhost mongo]# pwd/export/cloud/mongodb[root@localhost mongo]# vim mongo.conf[root@localhost mongo]# lsbin GNU-AGPL-3.0 mongo.conf MPL-2 README THIRD-PARTY-NOTICES[root@localhost mongo]# bin/mongod --config /export/cloud/mongo/mongo.confabout to fork child process, waiting until server is ready for connections.forked process: 22187child process started successfully, parent exiting Mongo配置123456789101112131415[root@localhost mongo]# cat mongo.conf# specify port number - 27017 by defaultport=27017# directory for datafiles - defaults to /data/dbdbpath=/export/data/mongo# log file to send write to instead of stdout - has to be a file, not directorylogpath=/export/logs/mongo/mongo.log# append to logpath instead of over-writinglogappend=true# fork server processfork=true# each database will be stored in a separate directorydirectoryperdb=true# run with securityauth=false 说明: dbpath和logpath自己创建 Mongo验证12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@localhost mongo]# pwd/export/cloud/mongo[root@localhost mongo]# bin/mongo&gt; use adminswitched to db admin&gt; show dbsadmin 0.000GBlocal 0.000GB&gt; db.mock.insert(&#123;name:"jack",age:22&#125;);&gt; db.collection.insertMany([&#123;name:”elon",age:20&#125;,&#123;name:”dennisit",age:25&#125;])2017-08-26T13:24:23.078+0800 E QUERY [thread1] SyntaxError: illegal character @(shell):1:32&gt; db.mock.insertMany([&#123;name:"elon",age:20&#125;,&#123;name:"dennisit",age:25&#125;]);&#123; "acknowledged" : true, "insertedIds" : [ ObjectId("59a10628dcec5be9803d12fe"), ObjectId("59a10628dcec5be9803d12ff") ]&#125;&gt; db.mock.find();&#123; "_id" : ObjectId("59a10399dcec5be9803d12fd"), "name" : "jack", "age" : 22 &#125;&#123; "_id" : ObjectId("59a10628dcec5be9803d12fe"), "name" : "elon", "age" : 20 &#125;&#123; "_id" : ObjectId("59a10628dcec5be9803d12ff"), "name" : "dennisit", "age" : 25 &#125;&gt; db.mock.findOne();&#123; "_id" : ObjectId("59a10399dcec5be9803d12fd"), "name" : "jack", "age" : 22&#125;&gt; db.mock.find(&#123;age:&#123;$gt:20&#125;&#125;);&#123; "_id" : ObjectId("59a10399dcec5be9803d12fd"), "name" : "jack", "age" : 22 &#125;&#123; "_id" : ObjectId("59a10628dcec5be9803d12ff"), "name" : "dennisit", "age" : 25 &#125;&gt;&gt; db.mock.find(&#123;name: "elon"&#125;);&#123; "_id" : ObjectId("59a10628dcec5be9803d12fe"), "name" : "elon", "age" : 20 &#125;&gt; db.mock.find().skip(1).limit(5).sort(&#123;age: 1&#125;);&#123; "_id" : ObjectId("59a10399dcec5be9803d12fd"), "name" : "jack", "age" : 22 &#125;&#123; "_id" : ObjectId("59a10628dcec5be9803d12ff"), "name" : "dennisit", "age" : 25 &#125;&gt; db.mock.find().skip(1).limit(5).sort(&#123;age: -1&#125;);&#123; "_id" : ObjectId("59a10399dcec5be9803d12fd"), "name" : "jack", "age" : 22 &#125;&#123; "_id" : ObjectId("59a10628dcec5be9803d12fe"), "name" : "elon", "age" : 20 &#125;&gt; db.mock.remove(&#123;age:&#123;$gt:20&#125;&#125;);WriteResult(&#123; "nRemoved" : 2 &#125;)&gt; db.mock.find();&#123; "_id" : ObjectId("59a10628dcec5be9803d12fe"), "name" : "elon", "age" : 20 &#125;&gt; db.mock.insert(&#123;_id:111, name:"suruonian",age:18&#125;);WriteResult(&#123; "nInserted" : 1 &#125;)&gt; db.mock.find();&#123; "_id" : ObjectId("59a10628dcec5be9803d12fe"), "name" : "elon", "age" : 20 &#125;&#123; "_id" : 111, "name" : "suruonian", "age" : 18 &#125;&gt; db.mock.update(&#123;_id: 111&#125; , &#123;$set : &#123;"name":"苏若年"&#125;&#125;, false, true);WriteResult(&#123; "nMatched" : 1, "nUpserted" : 0, "nModified" : 1 &#125;)&gt; db.mock.find();&#123; "_id" : ObjectId("59a10628dcec5be9803d12fe"), "name" : "elon", "age" : 20 &#125;&#123; "_id" : 111, "name" : "苏若年", "age" : 18 &#125;&gt; db.mock.remove(&#123;&#125;);WriteResult(&#123; "nRemoved" : 2 &#125;)&gt; db.mock.find(); 命令行教程 : http://www.runoob.com/mongodb/mongodb-tutorial.html Mongo权限开启权限认证mongo启动配置中设置auth=true 123456789101112131415[root@localhost mongo]# cat mongo.conf# specify port number - 27017 by defaultport=27017# directory for datafiles - defaults to /data/dbdbpath=/export/data/mongo# log file to send write to instead of stdout - has to be a file, not directorylogpath=/export/logs/mongo/mongo.log# append to logpath instead of over-writinglogappend=true# fork server processfork=true# each database will be stored in a separate directorydirectoryperdb=true# run with securityauth=true 添加管理员账号1234567891011121314[root@localhost mongo]# pwd/export/cloud/mongo[root@localhost mongo]# bin/mongod --config /export/cloud/mongo/mongo.confabout to fork child process, waiting until server is ready for connections.forked process: 10930child process started successfully, parent exiting[root@localhost mongo]# bin/mongo&gt; use admin;switched to db admin&gt; db.createUser(&#123;... user:'admin',... pwd:'123456',... roles:[&#123;role:'userAdminAnyDatabase', db:'admin'&#125;]... &#125;); 说明 这里所说的管理员账号不是像linux里面无所不能的root，而是一个能分配账号的账号。 这样就创建了一个账号,user为admin，密码为123456，他能在admin库中管理任何库的用户。 添加数据操作账号一旦开启的权限管理，接下来所有的操作都必须用合适的账号去做,我们创建的admin账号只能管理用户，而我们真正需要的账号是要能用来读写数据的，那就需要创建一个可读写数据的账号。 1234567891011121314&gt; use adminswitched to db admin&gt; db.auth("admin","123456");1&gt; use stream;switched to db stream&gt; db.createUser(&#123;... user:'test',... pwd:'testpwd',... roles:[&#123;role:'readWrite', db:'stream'&#125;]... &#125;);&gt; exitbye[root@localhost mongo]# bin/mongo 127.0.0.1:27017/stream -utest -ptestpwd 示例给stream库添加了一个账号为”test”,密码为”testpwd”, 权限为”readWrite”的用户. DaemonTools安装 Daemontools是一个守护进程工具，用来监视一个进程以免其意外退出; 它包含了很多管理Unix服务的工具的软件包，其中最核心的工具是supervise，它的功能是监控一个指定的服务，当该服务进程消亡，则重新启动该进程。而要添加让supervise监控的服务非常容易，只需要添加一个被监控的服务的目录，在该目录中添加启动服务器的名字为run的脚本文件即可; 官网地址: http://cr.yp.to/daemontools.html DaemonTools安装示例123456789101112[root@localhost daemontools]# pwd/export/backup/daemontools[root@localhost daemontools]# wget http://cr.yp.to/daemontools/daemontools-0.76.tar.gz[root@localhost daemontools]# gunzip daemontools-0.76.tar.gz[root@localhost daemontools]# lsdaemontools-0.76.tar[root@localhost daemontools]# tar -xpf daemontools-0.76.tar[root@localhost daemontools]# lsadmin daemontools-0.76.tar[root@localhost daemontools]# rm -f daemontools-0.76.tar[root@localhost daemontools]# cd admin/daemontools-0.76[root@localhost daemontools]# package/install 说明: 安装完会在根目录/下创建两个目录：/service和/command目录. /service目录： 存放被daemontools管理的进程，注意在/service目录下存放的只能是连接。 /command目录: 存放的是daemontools的一些常用命令。 12345[root@localhost daemontools]# cd ~[root@localhost ~]# ls /command/envdir fghack pgrphack setlock softlimit svc svscan svstat tai64nlocalenvuidgid multilog readproctitle setuidgid supervise svok svscanboot tai64n[root@localhost ~]# ls /service/ DaemonTools开机启动在/etc/init/下增添加svscan.conf文件, 文件内容如下: 123456789101112[root@localhost ~]# cat /etc/init/svscan.conf# svscan - daemontools## This service starts daemontools from the point the system is# started until it is shut down again.limit nofile 1000000 1000000start on runlevel [345]stop on runlevel [06]respawnexec /command/svscanboot 注释掉安装,在/etc/inittab中自动配置的一行(不使用该方法开机启动,会存在不同系统失效的问题) 1234567891011121314151617181920212223242526272829[root@localhost ~]# cat /etc/inittab# inittab is only used by upstart for the default runlevel.## ADDING OTHER CONFIGURATION HERE WILL HAVE NO EFFECT ON YOUR SYSTEM.## System initialization is started by /etc/init/rcS.conf## Individual runlevels are started by /etc/init/rc.conf## Ctrl-Alt-Delete is handled by /etc/init/control-alt-delete.conf## Terminal gettys are handled by /etc/init/tty.conf and /etc/init/serial.conf,# with configuration in /etc/sysconfig/init.## For information on how to write upstart event handlers, or how# upstart works, see init(5), init(8), and initctl(8).## Default runlevel. The runlevels used are:# 0 - halt (Do NOT set initdefault to this)# 1 - Single user mode# 2 - Multiuser, without NFS (The same as 3, if you do not have networking)# 3 - Full multiuser mode# 4 - unused# 5 - X11# 6 - reboot (Do NOT set initdefault to this)#id:3:initdefault:SV:123456:respawn:/command/svscanboot 添加完启动配置后, 执行以下指令是配置重新加载 12[root@localhost ~]# initctl reload-configuration[root@localhost ~]# initctl start svscan DaemonTools状态检测123456789[root@localhost ~]# ps -ef | grep svscanroot 20141 1 0 22:38 ? 00:00:00 /bin/sh /command/svscanbootroot 20143 20141 0 22:38 ? 00:00:00 svscan /serviceroot 20162 19819 0 22:39 pts/1 00:00:00 grep svscansvscan start/running, process 20141[root@localhost ~]# pstree -a -p 20141svscanboot,20141 /command/svscanboot ├─readproctitle,20144 service errors:... └─svscan,20143 /service DaemonTools目录规范在需要被supervise监控的目录下建立可执行的 run 脚本文件将这个目录软链到/service下，svscan检测到这是个新目录，会自动执行 svc -u 关于run脚本：切记，要用 exec 执行最终执行服务的程序，否则运行 run 脚本的shell收到 svc -d 的 TERM 信号退出之后，实际执行服务的那个程序不会跟着退出。 因为supervise是通过监控run退出时产生的SIGCHLD信号来识别服务已经终止，并重启服务的。如果这里没exec，则会导致fork+exec效果，在svc -d终止服务时，只给run脚本发送TERM命令，而run脚本fork出来的子进程不会收到信号，从而变成孤儿进程继续运行，占据文件锁、TCP端口等资源对于不便exec的程序，可以在后面加&amp;符号后台运行，并在调用命令之后用 waitpid %1 命令等待，从而阻止run脚本退出；run脚本开头处应该用trap命令捕获TERM信号，信号处理过程中给%1发送TERM信号，即可实现整体退出的效果 一个受监控的配置示例结构应当如下图: 1234567myservice ├── run └── supervise -- 这个目录是daemontools生成的 ├── control ├── lock ├── ok └── status DaemonTools监控示例创建监控测试进程123456789101112131415161718192021222324[root@localhost ~]# cd /service/[root@localhost service]# mkdir test.daemontools[root@localhost service]# cd test.daemontools/[root@localhost test.daemontools]# vim TestDaemon.java[root@localhost test.daemontools]# pwd/service/test.daemontools[root@localhost test.daemontools]# lssupervise TestDaemon.java[root@localhost test.daemontools]# lssupervise TestDaemon.java[root@localhost test.daemontools]# javac -d . TestDaemon.java[root@localhost test.daemontools]# vim run[root@localhost test.daemontools]# chmod +x run[root@localhost test.daemontools]# cat run#!/bin/shecho -e "daemon tools test with java demo";exec java TestDaemon[root@localhost test.daemontools]# lsrun supervise TestDaemon.class TestDaemon.java[root@localhost test.daemontools]# sh runstart testi=0i=1i=2 说明: 要监控的监控必须对应一个run脚本(名字必须是run而且权限是755) 尝试kill掉测试进程123456789[root@localhost test.daemontools]# jps31783 Jps31654 TestDaemon14696 Bootstrap[root@localhost test.daemontools]# kill -9 31654[root@localhost test.daemontools]# jps31801 TestDaemon31815 Jps14696 Bootstrap DaemonTools常用指令启动被管理的进程1[root@localhost ~]# svc -u /service/test.daemontools/ 查看被管理的进程状态12[root@localhost ~]# svstat /service/test.daemontools//service/test.daemontools/: up (pid 31801) 202 seconds 重启被管理的服务12# 重启服务（向当前进程发一个TERM信号，退出后，svc会自动将其启动）[root@localhost ~]# svc -t /service/gtbot1 关闭被管理的进程12345678910[root@localhost ~]# svc -d /service/test.daemontools/[root@localhost ~]# svstat /service/test.daemontools//service/test.daemontools/: down 3 seconds, normally up[root@localhost ~]# jps5993 Bootstrap31893 Jps1746 Bootstrap15606 Bootstrap27289 Bootstrap14696 Bootstrap 问题处理问题1: could not read symbols: Bad value错误: 1234/usr/bin/ld: errno: TLS definition in /lib64/libc.so.6 section .tbss mismatches non-TLS reference in envdir.o/lib64/libc.so.6: could not read symbols: Bad valuecollect2: ld 返回 1make: *** [envdir] 错误 1 解决: 编辑src/conf-cc, 在gcc开头的一行最后加上-include /usr/include/errno.h 问题2: 应用程序的错误输出需要重定向到STDOUT去应用程序的错误输出需要 重定向 到 STDOUT 去，不然 daemontools 会认为你的程序出错导致一起重启。通常很多 java 程序是直接把错误输出在 标准错误输出的，需要注意。 1exec java xxx.jar 2&gt;&amp;1 CentOs7防火墙1234567891011121314151617181920212223[root@localhost opt]# systemctl start firewalld.service[root@localhost opt]# systemctl status firewalld.service● firewalld.service - firewalld - dynamic firewall daemon Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled) Active: active (running) since 六 2018-04-21 13:16:20 CST; 5s ago Docs: man:firewalld(1) Main PID: 20152 (firewalld) CGroup: /system.slice/firewalld.service └─20152 /usr/bin/python -Es /usr/sbin/firewalld --nofork --nopid4月 21 13:16:19 localhost systemd[1]: Starting firewalld - dynamic firewall daemon...4月 21 13:16:20 localhost systemd[1]: Started firewalld - dynamic firewall daemon.4月 21 13:16:20 localhost firewalld[20152]: WARNING: ICMP type 'beyond-scope' is not supported by the kernel for ipv6.4月 21 13:16:20 localhost firewalld[20152]: WARNING: beyond-scope: INVALID_ICMPTYPE: No supported ICMP type., igno...time.4月 21 13:16:20 localhost firewalld[20152]: WARNING: ICMP type 'failed-policy' is not supported by the kernel for ipv6.4月 21 13:16:20 localhost firewalld[20152]: WARNING: failed-policy: INVALID_ICMPTYPE: No supported ICMP type., ign...time.4月 21 13:16:20 localhost firewalld[20152]: WARNING: ICMP type 'reject-route' is not supported by the kernel for ipv6.4月 21 13:16:20 localhost firewalld[20152]: WARNING: reject-route: INVALID_ICMPTYPE: No supported ICMP type., igno...time.Hint: Some lines were ellipsized, use -l to show in full.[root@localhost opt]# firewall-cmd --zone=public --add-port=80/tcp --permanentsuccess[root@localhost opt]# firewall-cmd --zone=public --add-port=3306/tcp --permanentsuccess firewall命令说明 firewall-cmd –state ##查看防火墙状态，是否是running firewall-cmd –reload ##重新载入配置，比如添加规则之后，需要执行此命令 firewall-cmd –get-zones ##列出支持的zone firewall-cmd –get-services ##列出支持的服务，在列表中的服务是放行的 firewall-cmd –query-service ftp ##查看ftp服务是否支持，返回yes或者no firewall-cmd –add-service=ftp ##临时开放ftp服务 firewall-cmd –add-service=ftp –permanent ##永久开放ftp服务 firewall-cmd –remove-service=ftp –permanent ##永久移除ftp服务 firewall-cmd –add-port=80/tcp –permanent ##永久添加80端口(–zone #作用域, –permanent #永久生效，没有此参数重启后失效) iptables -L -n ##查看规则，这个命令是和iptables的相同的 man firewall-cmd ##查看帮助 参考文章 https://blog.52itstyle.com/archives/1158/ https://blog.csdn.net/achang21/article/details/52538049 (防火墙)]]></content>
      <categories>
        <category>运维部署</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>Install</tag>
        <tag>NodeJs</tag>
        <tag>JDK</tag>
        <tag>MySQL</tag>
        <tag>Redis</tag>
        <tag>Mongo</tag>
        <tag>DaemonTools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[管理工具-Maven]]></title>
    <url>%2F2015%2F01%2F11%2F%E7%BB%84%E4%BB%B6%E6%A1%86%E6%9E%B6%2F%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7-Maven%2F</url>
    <content type="text"><![CDATA[Maven介绍 Maven配置 Maven工程应用 基于命令行工程应用 基于IDE的工程应用 Maven统一依赖管理 Maven依赖包作用域 Maven依赖冲突排除 参考资料 Maven介绍Maven 是一个项目管理和构建自动化工具. Maven使用惯例优与配置的原则.它要求在没有定制之前，所有的项目都有如下的结构: 目的 功能 ${basedir} 存放 pom.xml和所有的子目录(工程根目录) ${basedir}/src/main/java 项目的 java源代码 ${basedir}/src/main/resources 项目的资源文件，比如说 property文件 ${basedir}/src/test/java 项目的测试类，比如说 JUnit代码 ${basedir}/src/test/resources 测试使用的资源 说明: 编译后的classes会放在 ${basedir}/target/classes下面 Maven配置Maven 依赖包下载 Maven官网下载Maven包: https://maven.apache.org/download.cgi 12[root@localhost /local ]$ wget http://mirror.bit.edu.cn/apache/maven/maven-3/3.5.0/binaries/apache-maven-3.5.0-bin.tar.gz[root@localhost /local ]$ tar -zxvf apache-maven-3.5.0-bin.tar.gz Maven 配置环境变量 在/etc/profile文件末尾追加环境变量,如下: 12345678# 设置Meven环境变量M2_HOME=/usr/local/apache-maven-3.5.0# 设置Java环境变量JAVA_HOME=/usr/local/jdk1.8.0_121JRE_HOME=/usr/local/jdk1.8.0_121/jreCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin:$M2_HOME/bin:export M2_HOME JAVA_HOME JRE_HOME CLASS_PATH PATH 使配置生效 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081[root@localhost ~]$ source /etc/profile[root@localhost ~]$ mvn -versionApache Maven 3.5.0 (35c39251d2af99b6624d40d801f6ff02; 2015-11-11T00:41:47+08:00)Maven home: /usr/local/apache-maven-3.5.0[root@localhost wuyu-module ]$ mvn -helpusage: mvn [options] [&lt;goal(s)&gt;] [&lt;phase(s)&gt;]Options: -am,--also-make If project list is specified, also build projects required by the list -amd,--also-make-dependents If project list is specified, also build projects that depend on projects on the list -B,--batch-mode Run in non-interactive (batch) mode -b,--builder &lt;arg&gt; The id of the build strategy to use. -C,--strict-checksums Fail the build if checksums don't match -c,--lax-checksums Warn if checksums don't match -cpu,--check-plugin-updates Ineffective, only kept for backward compatibility -D,--define &lt;arg&gt; Define a system property -e,--errors Produce execution error messages -emp,--encrypt-master-password &lt;arg&gt; Encrypt master security password -ep,--encrypt-password &lt;arg&gt; Encrypt server password -f,--file &lt;arg&gt; Force the use of an alternate POM file (or directory with pom.xml). -fae,--fail-at-end Only fail the build afterwards; allow all non-impacted builds to continue -ff,--fail-fast Stop at first failure in reactorized builds -fn,--fail-never NEVER fail the build, regardless of project result -gs,--global-settings &lt;arg&gt; Alternate path for the global settings file -gt,--global-toolchains &lt;arg&gt; Alternate path for the global toolchains file -h,--help Display help information -l,--log-file &lt;arg&gt; Log file where all build output will go. -llr,--legacy-local-repository Use Maven 2 Legacy Local Repository behaviour, ie no use of _remote.repositories. Can also be activated by using -Dmaven.legacyLocalRepo=true -N,--non-recursive Do not recurse into sub-projects -npr,--no-plugin-registry Ineffective, only kept for backward compatibility -npu,--no-plugin-updates Ineffective, only kept for backward compatibility -nsu,--no-snapshot-updates Suppress SNAPSHOT updates -o,--offline Work offline -P,--activate-profiles &lt;arg&gt; Comma-delimited list of profiles to activate -pl,--projects &lt;arg&gt; Comma-delimited list of specified reactor projects to build instead of all projects. A project can be specified by [groupId]:artifactId or by its relative path. -q,--quiet Quiet output - only show errors -rf,--resume-from &lt;arg&gt; Resume reactor from specified project -s,--settings &lt;arg&gt; Alternate path for the user settings file -T,--threads &lt;arg&gt; Thread count, for instance 2.0C where C is core multiplied -t,--toolchains &lt;arg&gt; Alternate path for the user toolchains file -U,--update-snapshots Forces a check for missing releases and updated snapshots on remote repositories -up,--update-plugins Ineffective, only kept for backward compatibility -V,--show-version Display version information WITHOUT stopping build -v,--version Display version information -X,--debug Produce execution debug output Maven工程应用基于命令行工程应用基于命令行的工程应用,必须确保已经配置了Maven环境变量. 命令行创建工程使用maven命令行创建一个maven工程.示例如下 1234567891011121314151617181920[root@localhost /export ]$ mvn archetype:generate -B -DarchetypeGroupId=org.apache.maven.archetypes -DarchetypeArtifactId=maven-archetype-quickstart -DarchetypeVersion=1.1 -DgroupId=com.wuyu -DartifactId=wuyu-module -Dversion=1.0.0-SNAPSHOT -Dpackage=com.wuyu.module[root@localhost /export ]$ tree wuyu-module wuyu-module├── pom.xml└── src ├── main │ └── java │ └── com │ └── wuyu │ └── module │ └── App.java └── test └── java └── com └── wuyu └── module └── AppTest.java11 directories, 3 files[root@localhost /export ]$ cd wuyu-module 参数说明 -B: 批量模式,使用该参数会跳过一个一个的交互输入 -DgroupId: 项目组织唯一的标识符 -DartifactId: 项目的唯一的标识符，对应项目的名称，即项目根目录的名称. -Dversion: 生成的项目版本号 -Dpackage: 生成的工程包路径 命令行编译工程修改编译使用的jdk版本, pom.xml文件中增加如下配置指定jdk编译的版本和生成的字节码版本: maven.compiler.source maven.compiler.source 12345678910111213141516171819202122232425262728[root@localhost wuyu-module ]$ cat pom.xml &lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.wuyu&lt;/groupId&gt; &lt;artifactId&gt;wuyu-module&lt;/artifactId&gt; &lt;version&gt;1.0.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;wuyu-module&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 编译生成的工程,查看编译后的结构: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[root@localhost wuyu-module ]$ mvn package[root@localhost wuyu-module ]$ tree ..├── pom.xml├── src│ ├── main│ │ └── java│ │ └── com│ │ └── wuyu│ │ └── module│ │ └── App.java│ └── test│ └── java│ └── com│ └── wuyu│ └── module│ └── AppTest.java├── target│ ├── classes│ │ └── com│ │ └── wuyu│ │ └── module│ │ └── App.class│ ├── generated-sources│ │ └── annotations│ ├── generated-test-sources│ │ └── test-annotations│ ├── maven-archiver│ │ └── pom.properties│ ├── maven-status│ │ └── maven-compiler-plugin│ │ ├── compile│ │ │ └── default-compile│ │ │ ├── createdFiles.lst│ │ │ └── inputFiles.lst│ │ └── testCompile│ │ └── default-testCompile│ │ ├── createdFiles.lst│ │ └── inputFiles.lst│ ├── surefire-reports│ │ ├── TEST-com.wuyu.module.AppTest.xml│ │ └── com.wuyu.module.AppTest.txt│ ├── test-classes│ │ └── com│ │ └── wuyu│ │ └── module│ │ └── AppTest.class│ └── wuyu-module-1.0.0-SNAPSHOT.jar└── ~33 directories, 13 files 可以看到编译后的结构入上图,target下有我们编译的字节码还有编译打包好的jar包wuyu-module-1.0.0-SNAPSHOT.jar. 命令行执行工程12[root@localhost wuyu-module ]$ java -cp target/wuyu-module-1.0.0-SNAPSHOT.jar com.wuyu.module.AppHello World! 基于IDE的工程应用这里演示使用IntelliJ IDEA创建Maven项目. Maven创建单模块工程【File】-【New】-【Project】-【Maven】-【勾选 Create from archetype】: 点击【Next】, 配置工程坐标: 点击【Next】,配置Maven信息: 点击【Next】, 配置工程名称, 示例中配置为: wuyu-module, 点击完成(选择新窗口打开刚才创建的示例工程). Maven创建多模块工程上面示例中我们创建了一个工程,改工程为单模块的. 实际开发中我们可能根据工程职责分成多个模块, 接下来展示如何构建多模块工程. 刚才的工程对于多模块中只有pom.xml文件有用,其它的src目录下的都没用,所以可以做如下操作: 删除src以及其子目录 修改pom.xml中packaging类型为pom. 选中工程,右键:【New】-【Module】,然后创建子模块的定义. 创建jar格式的子模块普通的jar模块,选择的maven类型为maven-archetype-quickstart 创建war格式的子模块 创建完后的多模块工程结构如下: Maven统一依赖管理maven版本统一管理采用&lt;dependencyManagement&gt;元素结点, 主要有两个作用，一个是集中管理项目的依赖项，另一个就是控制使用的依赖项的版本. 该节点主要应用于父pom, 里边托管所有要依赖的包依赖, 子包引入的时候, 采用dependencyManagement里边的包的子集的时候不需要指定版本.类似还有pluginManagement.用于统一管理maven的插件版本. 父类模块中dependencyManagement排除功能,同样会应用到到子模块. 说明: 右侧我们看到的Lifecycle模块是IDEA中提供的方便我们进行Maven命令执行的可视化工具, 支持单选和多选. Maven依赖包作用域Maven在引入依赖包的时候可以指定作用域,使用scope标签. scope的取值有compile(默认)、runtime、test、 provided、 system 和 import compile: 默认作用范围，compile范围内的依赖项在所有情况下都是有效的，包括运行、测试和编译时(会被打到包里)。 runtime: 表示该依赖项只有在运行时才是需要的，在编译的时候不需要。这种类型的依赖项将在运行和test的类路径下可以访问(会被打到包里)。 test: 表示该依赖项只对测试时有用，包括测试代码的编译和运行，对于正常的项目运行是没有影响的(不会打到包里)。 provided: 表示该依赖项将由JDK或者运行容器在运行时提供，也就是说由Maven提供的该依赖项我们只有在编译和测试时才会用到，而在运行时将由JDK或者运行容器提供(不会打到包里)。 system: 当scope为system时，表示该依赖项是我们自己提供的，不需要Maven到仓库里面去找。指定scope为system需要与另一个属性元素systemPath一起使用，它表示该依赖项在当前系统的位置，使用的是绝对路径(不会打到包里)。 import: (Maven2.0.9及以上)import范围只适用于pom文件中的部分。表明指定的POM必须使用部分的依赖。因为依赖已经被替换，所以使用import范围的依赖并不影响依赖传递 Maven依赖冲突排除我们可能经常会遇到这样一个问题：我们的项目有两个依赖项：A &amp; B，而且A和B同时依赖了C，但不是同一个版本。那么我们怎么办呢？ 添加检查插件 12345678&lt;reporting&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-project-info-reports-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/reporting&gt; 然后运行：mvn project-info-reports:dependencies，来查看依赖项报告. 去除依赖项 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot&lt;/artifactId&gt; &lt;version&gt;2.0.5.RELEASE&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusiongs&gt;&lt;/dependency&gt; Maven读书笔记 参考资料 Apache Maven入门篇-上 Apache Maven入门篇-下]]></content>
      <categories>
        <category>组件框架</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解Java-位运算]]></title>
    <url>%2F2014%2F01%2F11%2F%E8%AF%AD%E8%A8%80%E8%AF%AD%E6%B3%95%2F%E7%90%86%E8%A7%A3Java-%E4%BD%8D%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[位运算符主要针对二进制,它包括了：“与”、“非”、“或”、“异或”。 数据基本单位在了解运算符前,先了解一下数据的基本单位 BIT(比特): 一个二进制位(最小数据单位), 比如：0 BYTE(字节): 1BYTE ＝ 8BIT, 比如：01010110 KB(千字节): 1KB = 1024BYTE MB(兆字节): 1MB = 1024KB GB(吉字节): 1GB = 1024MB TB(太字节): 1TB=1024GB Java中的基本数据类型不同语言中的数据长度可能不一样, 这里介绍一下Java语言中的基础数据长度 数据类型与对应长度 类型 长度 boolean - char 16bit byte 8bit short 16bit int 32bit long 64bit float 32bit double 64bit 程序示例12345678910111213141516171819202122@Testpublic void test()&#123; // 输出: 1 System.out.println(Byte.BYTES); // 输出: 2 System.out.println(Short.BYTES); // 输出: 4 System.out.println(Integer.BYTES); // 输出: 8 System.out.println(Long.BYTES); // 输出: 4 System.out.println(Float.BYTES); // 输出: 8 System.out.println(Double.BYTES); // 输出: 11111111111111111111111111111110 System.out.println(Integer.toBinaryString(-2)); // 输出: 1111111111111111111111111111111111111111111111111111111111111110 System.out.println(Long.toBinaryString(-2L)); char c = '苏'; // 输出: 苏 System.out.println(c);&#125; ^(异或) 参与运算的两个数, 如果两个相应位相同(二进制),则结果为0,否则为1.即：0^0=0, 1^0=1, 0^1=1, 1^1=0 运算说明 操作 二进制 十进制 - 000000000000000000000000000000001 1 - 000000000000000000000000000000010 2 ^ 000000000000000000000000000000011 3 程序测试12345@Testpublic void test()&#123; // 输出: 3 System.out.println(1^2);&#125; &amp;(与) 参与运算的两个数, 如果两个相应位都为1(二进制),结果才为1,否则结果为0.即：0^0=0, 1^0=1, 0^1=1, 1^1=1 运算说明 操作 二进制 十进制 - 000000000000000000000000000000001 1 - 000000000000000000000000000000010 2 &amp; 000000000000000000000000000000000 0 程序测试12345@Testpublic void test()&#123; // 输出: 0 System.out.println(1&amp;2);&#125; | (或) 参与运算的两个数, 如果两个相应位只要有一个为1(二进制),那么结果就是1,否则就为0.即：0^0=0, 1^0=1, 0^1=1, 1^1=1 运算说明 操作 二进制 十进制 - 000000000000000000000000000000001 1 - 000000000000000000000000000000010 2 或 000000000000000000000000000000011 3 程序测试12345@Testpublic void test()&#123; // 输出: 3 System.out.println(1|2);&#125; ~(非) 参数计算的数,如果位为0(二进制),结果是1,如果位为1,结果是0. 运算说明 二进制 十进制 运算 二进制 十进制 000000000000000000000000000000001 1 ~ 11111111111111111111111111111110 -2 000000000000000000000000000000010 2 ~ 11111111111111111111111111111101 -3 程序测试12345678910111213@Testpublic void test()&#123; // 输出: -2 System.out.println(~1); // 输出: 11111111111111111111111111111110 System.out.println(Integer.toBinaryString(~1)); // 输出: -3 System.out.println(~2); // 输出: 11111111111111111111111111111101 System.out.println(Integer.toBinaryString(~2));&#125; 关于位运算的面试题实现两个数的交换思路: 两个数做异或,会计算出来一个中间数(即便这两个数相同,那么计算结果为0也满足),用这个中间数做交换时的中间引用停留即可. 代码实现 123456789@Testpublic void find()&#123; int a = 1, b=2; System.out.println("交换前: a = " + a + ", b = " + b); a = a ^ b; b = a ^ b; // ((a^b) ^ b) = a a = a ^ b; // ((a^b) ^ b) = a System.out.println("交换后: a = " + a + ", b = " + b);&#125; 实现字符串翻转思路: 使用异或进行高低位转换 代码实现 1234567891011121314151617/** * @param str 待返转的字符串 * @return 翻转后的字符串 */public static String reverse(String str)&#123; char[] chars = str.toCharArray(); int low = 0; int top = chars.length - 1; while (low &lt; top)&#123; chars[low] ^= chars[top]; chars[top] ^= chars[low]; chars[low] ^= chars[top]; low++; top--; &#125; return new String(chars);&#125; 一堆数, 除过一个数只出现了一次, 其它数均出现了2n次(n&gt;=1), 找出该数.分析: 因为相同的两个数做^结果为0, 0^任何数=任何数. 将这堆数从第一个开始,一直到最后一个进行异或运算, 那么最后结果值就是那个只出现了一次的数. 代码实现 123456789101112@Testpublic void findOne()&#123; int[] arr = new int[] &#123;1, 1, 2, 2, 3, 5, 5, 7, 7, 6, 6&#125;; // 从第一个数标开始 int result = arr[0]; // 以此进行后面的数据的异或运算 for(int i = 1; i &lt; arr.length; i++)&#123; result = result ^ arr[i]; &#125; // 程序输出: 3 System.out.println(result);&#125; java中|与||,&amp;与&amp;&amp;有什么区别&amp;(与): 位运算符,也兼有逻辑预算符的功能.&amp;&amp;(短路与): 只是逻辑运算符. &amp;与&amp;&amp;作为逻辑运算符时有如下区别: &amp;：无论&amp;左边是否为false,他都会继续检验右边的boolean值。 &amp;&amp;:只要检测到左边值为false时, 就会直接判断结果,不会在检验右边的值(因为”与”有一个false最后结果就是false了),所以&amp;&amp;的执行效率更高,所以逻辑运算时一般都是使用&amp;&amp;. |(或)与||(短路或)区别同&amp;(与)和&amp;&amp;(短路与)相似.]]></content>
      <categories>
        <category>语言语法</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
</search>
